import os
import json
import chromadb
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import torch

# Google Generative AI
import google.generativeai as genai
# THÃŠM THÆ¯ VIá»†N SENTENCE TRANSFORMER
from sentence_transformers import SentenceTransformer

# Load environment variables
load_dotenv()

# --- Cáº¥u hÃ¬nh Cá»T LÃ•I ---
INTELLIGENT_DB_PATH = './chroma' 
INTELLIGENT_COLLECTION = 'ksa_project' 
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
CHAT_SESSIONS_FILE = 'smart_chat_sessions.json'
MAX_CONTEXT_MESSAGES = 10 

# THÃŠM MODEL EMBEDDING GIá»NG Há»†T FILE INGEST.PY
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

class SmartChatMemory:
    """
    Quáº£n lÃ½ memory vÃ  context cho smart chatbot.
    LÆ°u vÃ  táº£i session tá»« file JSON.
    """
    
    def __init__(self, sessions_file: str = CHAT_SESSIONS_FILE):
        self.sessions_file = sessions_file
        self.sessions = self.load_sessions()
        self.current_session_id = None
        self.current_context = []
    
    def load_sessions(self) -> Dict[str, List[Dict]]:
        """Load chat sessions tá»« file"""
        try:
            if os.path.exists(self.sessions_file):
                with open(self.sessions_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Error loading sessions: {e}")
        return {}
    
    def save_sessions(self):
        """Save chat sessions vÃ o file"""
        try:
            with open(self.sessions_file, 'w', encoding='utf-8') as f:
                json.dump(self.sessions, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving sessions: {e}")
    
    def start_new_session(self) -> str:
        """Báº¯t Ä‘áº§u session má»›i"""
        session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.current_session_id = session_id
        self.sessions[session_id] = []
        self.current_context = []
        return session_id
    
    def add_message(self, user_message: str, ai_response: str):
        """ThÃªm tin nháº¯n vÃ o session hiá»‡n táº¡i (ÄÃ£ bá» sources thá»«a)"""
        if not self.current_session_id:
            self.start_new_session()
        
        message = {
            'timestamp': datetime.now().isoformat(),
            'user': user_message,
            'ai': ai_response,
        }
        
        # ThÃªm vÃ o session Ä‘á»ƒ lÆ°u file
        self.sessions[self.current_session_id].append(message)
        
        # ThÃªm vÃ o context (bá»™ nhá»› táº¡m) cho prompt tiáº¿p theo
        self.current_context.append(message)
        if len(self.current_context) > MAX_CONTEXT_MESSAGES:
            self.current_context = self.current_context[-MAX_CONTEXT_MESSAGES:]
        
        self.save_sessions()
    
    def get_context_for_prompt(self) -> str:
        """Táº¡o context string cho prompt"""
        if not self.current_context:
            return ""
        
        context_parts = ["=== Lá»ŠCH Sá»¬ CUá»˜C TRÃ’ CHUYá»†N Gáº¦N ÄÃ‚Y ==="]
        
        # Chá»‰ láº¥y 5 tin nháº¯n gáº§n nháº¥t cho vÃ o prompt
        for msg in self.current_context[-5:]: 
            context_parts.append(f"NgÆ°á»i dÃ¹ng: {msg['user']}")
            context_parts.append(f"AI: {msg['ai']}")
            context_parts.append("---")
        
        context_parts.append("=== Káº¾T THÃšC Lá»ŠCH Sá»¬ ===")
        return "\n".join(context_parts)

class SmartRAGRetriever:
    """
    Class Ä‘Æ¡n giáº£n Ä‘á»ƒ truy váº¥n (query) ChromaDB
    (ÄÃƒ Sá»¬A: Táº£i model embedding thá»§ cÃ´ng)
    """
    
    def __init__(self, db_path: str, collection_name: str, model_name: str):
        # 1. Táº£i model embedding (Báº®T BUá»˜C PHáº¢I GIá»NG MODEL KHI INGEST)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Äang táº£i Embedding Model: {model_name} (sá»­ dá»¥ng {device.upper()})")
        self.embedding_model = SentenceTransformer(model_name, device=device)
        print("Táº£i Model thÃ nh cÃ´ng.")

        # 2. Káº¿t ná»‘i DB
        print("Äang káº¿t ná»‘i tá»›i Vector Database...")
        self.client = chromadb.PersistentClient(path=db_path)
        try:
            # Báº®T BUá»˜C: Pháº£i chá»‰ Ä‘á»‹nh "cosine" giá»‘ng lÃºc ingest
            self.collection = self.client.get_collection(
                name=collection_name,
                # metadata={"hnsw:space": "cosine"} # KhÃ´ng cáº§n khi get, chá»‰ cáº§n khi create
            )
            print(f"Káº¿t ná»‘i thÃ nh cÃ´ng collection: '{collection_name}'")
        except Exception as e:
            print(f"Lá»–I: KhÃ´ng tÃ¬m tháº¥y collection '{collection_name}' táº¡i '{db_path}'")
            print("Vui lÃ²ng cháº¡y file 'ingest.py' trÆ°á»›c.")
            raise e
    
    def search(self, query_text: str, n_results: int = 3) -> List[str]:
        """
        Search documents vÃ  chá»‰ tráº£ vá» ná»™i dung (content)
        (ÄÃƒ Sá»¬A: MÃ£ hÃ³a query trÆ°á»›c khi tÃ¬m)
        """
        try:
            # 1. MÃ£ hÃ³a cÃ¢u há»i (query) thÃ nh vector
            query_vector = self.embedding_model.encode(query_text).tolist()
            
            # 2. DÃ¹ng vector Ä‘á»ƒ tÃ¬m kiáº¿m (thay vÃ¬ text)
            results = self.collection.query(
                query_embeddings=[query_vector], # Truyá»n vector
                n_results=n_results
            )
            
            return results['documents'][0] if results['documents'] else []
            
        except Exception as e:
            print(f"Lá»—i khi tÃ¬m kiáº¿m: {e}")
            return []

class SmartChatbot:
    """
    Chatbot Cá»T LÃ•I: Káº¿t há»£p Memory vÃ  RAG
    """
    
    def __init__(self):
        self.memory = SmartChatMemory()
        self.retriever = None
        self.gemini_model = None
        self.setup()
    
    def setup(self):
        """Setup chatbot components"""
        print("ğŸ§  Khá»Ÿi táº¡o Smart Chatbot...")
        
        # 1. Setup Gemini
        if not GOOGLE_API_KEY:
            raise ValueError("KhÃ´ng tÃ¬m tháº¥y GOOGLE_API_KEY. HÃ£y set nÃ³ trong file .env")
        
        genai.configure(api_key=GOOGLE_API_KEY)
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        print("   âœ… Model Gemini Ä‘Ã£ sáºµn sÃ ng.")
        
        # 2. Setup RAG retriever (ÄÃƒ Sá»¬A: Truyá»n tÃªn model vÃ o)
        if not os.path.exists(INTELLIGENT_DB_PATH):
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c database: {INTELLIGENT_DB_PATH}")
        
        self.retriever = SmartRAGRetriever(
            INTELLIGENT_DB_PATH, 
            INTELLIGENT_COLLECTION,
            EMBEDDING_MODEL_NAME # Truyá»n tÃªn model vÃ o
        )
        print(f"   âœ… ÄÃ£ káº¿t ná»‘i database: {self.retriever.collection.count()} tÃ i liá»‡u.")
        
        print("ğŸ‰ Chatbot sáºµn sÃ ng!")
    
    def create_prompt(self, user_question: str, rag_results: List[str]) -> str:
        """
        Táº¡o prompt hoÃ n chá»‰nh tá»« Lá»‹ch sá»­ chat vÃ  Dá»¯ liá»‡u RAG
        """
        # Láº¥y context tá»« memory
        conversation_context = self.memory.get_context_for_prompt()
        
        # Táº¡o RAG context (tá»‘i Æ°u hÃ³a)
        rag_context = ""
        if rag_results:
            rag_context = "### ThÃ´ng tin tá»« cÆ¡ sá»Ÿ kiáº¿n thá»©c:\n"
            for i, content in enumerate(rag_results, 1):
                # Giá»›i háº¡n Ä‘á»™ dÃ i ná»™i dung Ä‘á»ƒ trÃ¡nh prompt quÃ¡ dÃ i
                truncated_content = content[:500] + "..." if len(content) > 500 else content
                rag_context += f"{i}. {truncated_content}\n"
        
        # Táº¡o prompt tá»‘i Æ°u hÃ³a - ngáº¯n gá»n vÃ  hiá»‡u quáº£
        prompt = f"""Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n IT Career vá»›i kháº£ nÄƒng tráº£ lá»i chÃ­nh xÃ¡c, ngáº¯n gá»n vÃ  cÃ³ cáº¥u trÃºc.

{conversation_context}

{rag_context}

**CÃ¢u há»i:** {user_question}

**YÃªu cáº§u tráº£ lá»i:**
- Tráº£ lá»i NGáº®N Gá»ŒN, Táº¬P TRUNG vÃ o váº¥n Ä‘á» chÃ­nh
- Chá»‰ cung cáº¥p thÃ´ng tin TRá»°C TIáº¾P LIÃŠN QUAN Ä‘áº¿n cÃ¢u há»i
- Sá»­ dá»¥ng Markdown Ä‘á»ƒ Ä‘á»‹nh dáº¡ng:
  â€¢ TiÃªu Ä‘á»: ## hoáº·c ###
  â€¢ Danh sÃ¡ch: - hoáº·c sá»‘
  â€¢ Nháº¥n máº¡nh: **text**
  â€¢ Code: ```code```
- Cáº¥u trÃºc cÃ¢u tráº£ lá»i:
  1. CÃ¢u tráº£ lá»i trá»±c tiáº¿p (1-2 cÃ¢u)
  2. Chi tiáº¿t quan trá»ng (dáº¡ng danh sÃ¡ch, tá»‘i Ä‘a 3-5 Ä‘iá»ƒm)
  3. Lá»i khuyÃªn/bÆ°á»›c tiáº¿p theo (náº¿u cáº§n, 1-2 cÃ¢u)
- TRÃNH: Láº·p láº¡i cÃ¢u há»i, dÃ i dÃ²ng, thÃ´ng tin khÃ´ng liÃªn quan
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin: NÃ³i tháº³ng vÃ  Ä‘á» xuáº¥t hÆ°á»›ng tÃ¬m hiá»ƒu

Tráº£ lá»i:"""
        return prompt
    
    def get_response(self, user_question: str) -> str:
        """
        Láº¥y response tá»« chatbot (TÃ¬m kiáº¿m -> Táº¡o prompt -> Gá»i AI -> LÆ°u memory)
        """
        try:
            # 1. Search RAG database (TÃ¬m 3 tÃ i liá»‡u liÃªn quan nháº¥t)
            rag_results = self.retriever.search(user_question, n_results=3)
            
            # 2. Create context-aware prompt
            prompt = self.create_prompt(user_question, rag_results)
            
            # 3. Get AI response
            response = self.gemini_model.generate_content(prompt)
            ai_answer = response.text
            
            # 4. Save to memory
            self.memory.add_message(user_question, ai_answer)
            
            return ai_answer
            
        except Exception as e:
            error_msg = f"Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½: {str(e)}"
            self.memory.add_message(user_question, error_msg)
            return error_msg
    
    def process_command(self, command: str) -> str:
        """Xá»­ lÃ½ cÃ¡c lá»‡nh Ä‘áº·c biá»‡t (Chá»‰ giá»¯ láº¡i lá»‡nh cÆ¡ báº£n)"""
        
        if command == "/new":
            session_id = self.memory.start_new_session()
            return f"ğŸ†• Báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n má»›i: {session_id}"
        
        elif command == "/help":
            return """
ğŸ¤– Smart Chatbot Commands:
  /new   - Báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n má»›i
  /help  - Hiá»‡n hÆ°á»›ng dáº«n nÃ y
  quit   - ThoÃ¡t
"""
        
        return f"â“ Lá»‡nh khÃ´ng há»£p lá»‡: {command}. GÃµ /help Ä‘á»ƒ xem cÃ¡c lá»‡nh."

def main():
    """Main chat loop (ÄÃ£ tinh gá»n)"""
    print("=" * 50)
    print("ğŸ§  CHATBOT KSA (cÃ³ Memory + RAG)")
    print("GÃµ /help Ä‘á»ƒ xem lá»‡nh, 'quit' Ä‘á»ƒ thoÃ¡t")
    
    try:
        # 1. Khá»Ÿi táº¡o chatbot
        chatbot = SmartChatbot()
        
        # 2. Báº¯t Ä‘áº§u session má»›i
        session_id = chatbot.memory.start_new_session()
        print(f"\nğŸ’¬ Báº¯t Ä‘áº§u cuá»™c trÃ²GÃµ: {session_id}")
        
        # 3. VÃ²ng láº·p chat
        while True:
            try:
                user_input = input("\nğŸ¤” Báº¡n: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ‘‹ Táº¡m biá»‡t! Cuá»™c trÃ² chuyá»‡n Ä‘Ã£ Ä‘Æ°á»£c lÆ°u.")
                    break
                
                if not user_input:
                    continue
                
                # Xá»­ lÃ½ lá»‡nh
                if user_input.startswith('/'):
                    result = chatbot.process_command(user_input)
                    print(f"ğŸ¤– System: {result}")
                    continue
                
                # Láº¥y cÃ¢u tráº£ lá»i
                print("ğŸ¤– Bot Ä‘ang suy nghÄ©...")
                response = chatbot.get_response(user_input)
                print(f"ğŸ¤– Bot: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i trong vÃ²ng láº·p: {e}")
    
    except Exception as e:
        print(f"âŒ Lá»–I KHá»I Táº O: {e}")

if __name__ == "__main__":
    main()