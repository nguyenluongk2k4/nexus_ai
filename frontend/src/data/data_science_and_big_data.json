[
  {
    "id": "spec_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)",
    "name": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
    "type": "specialization",
    "children": [
      {
        "id": "ability_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1",
        "name": "Lập trình và Phát triển (Programming & Development)",
        "type": "ability",
        "children": [
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1_1",
            "name": "Phát triển và tối ưu mã Python/R cho phân tích dữ liệu",
            "type": "skill",
            "children": [
              {
                "id": "DS.PyR.BasicSyntaxDataStructures",
                "name": "Cú pháp và Cấu trúc dữ liệu cơ bản của Python và R",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào các nguyên tắc cú pháp cốt lõi, kiểu dữ liệu cơ bản (số, chuỗi, boolean) và các cấu trúc dữ liệu thiết yếu như danh sách, tuple, từ điển (Python) và vector, ma trận, data frame (R). Nắm vững chúng là nền tảng không thể thiếu để viết mã nguồn rõ ràng, hiệu quả và thực hiện các tác vụ phân tích dữ liệu phức tạp hơn trong cả hai ngôn ngữ.",
                "keywords": [
                  "Python syntax",
                  "R syntax",
                  "data types",
                  "Python lists",
                  "Python dictionaries",
                  "R vectors",
                  "R data frames",
                  "control flow basics",
                  "variables",
                  "operators"
                ],
                "learningResources": [
                  {
                    "name": "The Python Tutorial",
                    "url": "https://docs.python.org/3/tutorial/"
                  },
                  {
                    "name": "R for Data Science (2nd Edition) - Chapters 2 & 3",
                    "url": "https://r4ds.hadley.nz/data-structures.html"
                  },
                  {
                    "name": "GeeksforGeeks: Python Programming Tutorial",
                    "url": "https://www.geeksforgeeks.org/python-programming-language/"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Viết một chương trình Python nhỏ để quản lý danh sách sinh viên và điểm số của họ bằng cách sử dụng list và dictionary. Cho phép thêm, xóa, cập nhật và hiển thị dữ liệu.",
                  "Tạo một script R để xây dựng một data frame từ các vector chứa thông tin nhân khẩu học (tuổi, giới tính, thu nhập) của 10 cá nhân, sau đó tính toán các thống kê cơ bản như trung bình, độ lệch chuẩn cho tuổi và thu nhập."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Junior Data Scientist",
                  "Business Intelligence Developer",
                  "Research Assistant (Data Focus)",
                  "Entry-Level Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python Interpreter",
                  "RStudio",
                  "Jupyter Notebook",
                  "VS Code",
                  "Google Colab"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "15-25 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "targetSkillId": "DS.PyR.ControlFlowFunctions",
                    "type": "unlocks",
                    "description": "Nắm vững cú pháp và cấu trúc dữ liệu cơ bản là điều kiện tiên quyết để hiểu và sử dụng các cấu trúc điều khiển luồng và viết hàm."
                  },
                  {
                    "targetSkillId": "DS.PyR.DataManipulationPandasDplyr",
                    "type": "unlocks",
                    "description": "Kiến thức về cấu trúc dữ liệu cơ bản cần thiết để bắt đầu thao tác dữ liệu phức tạp hơn với thư viện Pandas (Python) và dplyr (R)."
                  },
                  {
                    "targetSkillId": "DS.PyR.FileIO",
                    "type": "unlocks",
                    "description": "Hiểu cách lưu trữ và tổ chức dữ liệu trong các cấu trúc cơ bản là cần thiết để đọc và ghi dữ liệu từ các tệp."
                  }
                ]
              },
              {
                "id": "ds_bd_core_data_analysis_libraries",
                "name": "Các thư viện/gói chính cho phân tích dữ liệu (ví dụ: Pandas, NumPy, Tidyverse)",
                "type": "knowledge",
                "children": [],
                "category": "Phát triển và tối ưu mã Python/R cho phân tích dữ liệu",
                "domain": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
                "description": "Các thư viện/gói cốt lõi như Pandas và NumPy (cho Python) hoặc Tidyverse (cho R) là những công cụ nền tảng để thao tác, làm sạch, chuyển đổi và thực hiện các phép toán số học trên dữ liệu. Việc thành thạo chúng là yếu tố then chốt để chuẩn bị và khám phá tập dữ liệu một cách hiệu quả, tạo thành xương sống cho hầu hết các dự án phân tích dữ liệu hoặc học máy.",
                "keywords": [
                  "Pandas",
                  "NumPy",
                  "Tidyverse",
                  "Thao tác dữ liệu",
                  "Làm sạch dữ liệu",
                  "Chuyển đổi dữ liệu",
                  "Tính toán số học",
                  "Lập trình Python",
                  "Lập trình R",
                  "Phân tích dữ liệu",
                  "Xử lý dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "10 Minutes to Pandas",
                    "url": "https://pandas.pydata.org/docs/user_guide/10min.html",
                    "type": "Official Documentation"
                  },
                  {
                    "name": "NumPy Quickstart Tutorial",
                    "url": "https://numpy.org/doc/stable/user/quickstart.html",
                    "type": "Official Documentation"
                  },
                  {
                    "name": "R for Data Science (with Tidyverse)",
                    "url": "https://r4ds.had.co.nz/",
                    "type": "Book/Tutorial"
                  }
                ],
                "prerequisites": [
                  "ds_bd_intro_python_programming",
                  "ds_bd_intro_r_programming"
                ],
                "projectIdeas": [
                  "Thực hiện phân tích khám phá dữ liệu (EDA) trên một tập dữ liệu công khai (ví dụ: dữ liệu Titanic, Iris, hoặc Airbnb) sử dụng Pandas/Tidyverse để phát hiện các mẫu, ngoại lai và mối quan hệ.",
                  "Xây dựng một quy trình làm sạch và chuyển đổi dữ liệu cho một tập dữ liệu thô, xử lý các giá trị thiếu, các điểm dữ liệu ngoại lai và định dạng lại dữ liệu để chuẩn bị cho phân tích hoặc mô hình hóa tiếp theo."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Machine Learning Engineer",
                  "Business Intelligence Analyst",
                  "Statistician"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas)",
                  "Python (NumPy)",
                  "R (Tidyverse - dplyr, ggplot2, tidyr, purrr, forcats, stringr, readr)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-3 tuần",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 8
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ds_bd_intro_python_programming",
                    "description": "Kiến thức cơ bản về lập trình Python là cần thiết để sử dụng Pandas và NumPy."
                  },
                  {
                    "type": "prerequisite",
                    "id": "ds_bd_intro_r_programming",
                    "description": "Kiến thức cơ bản về lập trình R là cần thiết để sử dụng Tidyverse."
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_data_visualization_tools",
                    "description": "Các thư viện này thường được sử dụng cùng với các công cụ trực quan hóa dữ liệu (ví dụ: Matplotlib, Seaborn, ggplot2) để khám phá và trình bày dữ liệu."
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_statistical_analysis_foundations",
                    "description": "Hiểu biết về thống kê giúp áp dụng các thư viện này hiệu quả hơn trong phân tích dữ liệu."
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_advanced_data_analysis",
                    "description": "Thành thạo các thư viện này là nền tảng để thực hiện các kỹ thuật phân tích dữ liệu nâng cao và giải quyết các vấn đề phức tạp."
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_machine_learning_model_development",
                    "description": "Khả năng chuẩn bị và xử lý dữ liệu bằng các thư viện này là bước đầu tiên và quan trọng nhất trong việc phát triển các mô hình học máy."
                  }
                ]
              },
              {
                "id": "DS.PYR.OPT.PERF_CODE",
                "name": "Nguyên tắc và Kỹ thuật tối ưu hóa hiệu suất mã (ví dụ: profiling, vectorization, quản lý bộ nhớ)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các phương pháp và công cụ để phân tích, cải thiện tốc độ và hiệu quả bộ nhớ của mã nguồn Python/R. Việc tối ưu hóa hiệu suất là cực kỳ quan trọng trong Khoa học Dữ liệu và Dữ liệu lớn, cho phép xử lý các tập dữ liệu khổng lồ nhanh hơn và triển khai các mô hình phức tạp hiệu quả hơn.",
                "keywords": [
                  "code optimization",
                  "performance tuning",
                  "profiling",
                  "vectorization",
                  "memory management",
                  "cache efficiency",
                  "JIT compilation",
                  "algorithmic complexity",
                  "parallel computing",
                  "computation graph"
                ],
                "learningResources": [
                  {
                    "name": "Optimizing Pandas for Speed: Techniques and Tools",
                    "url": "https://realpython.com/optimizing-pandas/"
                  },
                  {
                    "name": "Advanced R: Performance (by Hadley Wickham)",
                    "url": "https://adv-r.hadley.nz/perf-tips.html"
                  },
                  {
                    "name": "Python Profilers: An Overview (Real Python)",
                    "url": "https://realpython.com/python-profiling/"
                  }
                ],
                "prerequisites": [
                  "DS.PROGRAMMING.PYTHON_R_FUNDAMENTALS",
                  "DS.DATA_MANIPULATION.PANDAS_NUMPY_DATATABLE",
                  "DS.ALGORITHMS.COMPLEXITY_ANALYSIS"
                ],
                "projectIdeas": [
                  "Tối ưu hóa một script phân tích dữ liệu Pandas/data.table hiện có để giảm thời gian chạy ít nhất 30% bằng cách sử dụng các kỹ thuật vectorization hoặc quản lý bộ nhớ hiệu quả.",
                  "Sử dụng công cụ profiling để xác định nút thắt cổ chai trong một thuật toán Machine Learning tùy chỉnh (ví dụ: thuật toán k-NN) và áp dụng các cải tiến như JIT compilation (Numba) hoặc Cython."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Engineer",
                  "Big Data Engineer",
                  "Quantitative Analyst",
                  "Performance Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "cProfile (Python)",
                  "line_profiler (Python)",
                  "memory_profiler (Python)",
                  "Numba (Python)",
                  "Cython (Python)",
                  "profvis (R)",
                  "Rcpp (R)",
                  "Pandas",
                  "NumPy",
                  "data.table (R)"
                ],
                "difficultyLevel": "Advanced",
                "estimatedTimeToComplete": "30-50 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Quantitative Analyst",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS.PROGRAMMING.PYTHON_R_FUNDAMENTALS",
                    "type": "prerequisite",
                    "description": "Cần nắm vững cú pháp và cấu trúc cơ bản của Python/R."
                  },
                  {
                    "id": "DS.DATA_MANIPULATION.PANDAS_NUMPY_DATATABLE",
                    "type": "prerequisite",
                    "description": "Cần biết cách sử dụng các thư viện xử lý dữ liệu để có thể tối ưu hóa chúng."
                  },
                  {
                    "id": "DS.ALGORITHMS.COMPLEXITY_ANALYSIS",
                    "type": "prerequisite",
                    "description": "Hiểu về độ phức tạp thuật toán là nền tảng để xác định nơi cần tối ưu hóa."
                  },
                  {
                    "id": "DS.BIGDATA.DISTRIBUTED_COMPUTING",
                    "type": "complementary",
                    "description": "Kiến thức về tối ưu hóa hiệu suất mã sẽ bổ trợ cho việc tối ưu hóa trong môi trường điện toán phân tán."
                  },
                  {
                    "id": "DS.ML.MODEL_DEPLOYMENT_OPTIMIZATION",
                    "type": "unlocks",
                    "description": "Kỹ năng này giúp xây dựng và triển khai các mô hình Machine Learning hiệu quả và nhanh chóng hơn trong môi trường production."
                  }
                ]
              },
              {
                "id": "ds_bigdata_ds_algos_basic",
                "name": "Cấu trúc dữ liệu và Thuật toán cơ bản (để lựa chọn giải pháp tối ưu)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị nền tảng về cách tổ chức và quản lý dữ liệu hiệu quả, cùng với các phương pháp giải quyết vấn đề bằng thuật toán. Nắm vững các cấu trúc dữ liệu và thuật toán cơ bản là yếu tố then chốt để viết mã hiệu suất cao, có khả năng mở rộng và đưa ra các lựa chọn tối ưu khi xử lý lượng lớn dữ liệu trong phân tích và học máy.",
                "keywords": [
                  "Cấu trúc dữ liệu",
                  "Thuật toán",
                  "Độ phức tạp thời gian",
                  "Độ phức tạp không gian",
                  "Big O notation",
                  "Mảng",
                  "Danh sách liên kết",
                  "Stack",
                  "Queue",
                  "Cây",
                  "Bảng băm",
                  "Thuật toán sắp xếp",
                  "Thuật toán tìm kiếm"
                ],
                "learningResources": [
                  {
                    "name": "GeeksforGeeks: Data Structures & Algorithms",
                    "url": "https://www.geeksforgeeks.org/data-structures/"
                  },
                  {
                    "name": "Coursera: Algorithms, Part I by Princeton University",
                    "url": "https://www.coursera.org/learn/algorithms-part1"
                  },
                  {
                    "name": "LeetCode: Explore (Data Structures & Algorithms)",
                    "url": "https://leetcode.com/explore/"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Xây dựng và so sánh hiệu suất của các cấu trúc dữ liệu cơ bản (ví dụ: Danh sách liên kết, Stack, Queue) bằng Python hoặc R.",
                  "Thực hiện và đánh giá độ phức tạp thời gian của các thuật toán sắp xếp (ví dụ: Bubble Sort, Merge Sort, Quick Sort) trên các tập dữ liệu khác nhau khi xử lý một dataset có kích thước lớn."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Big Data Engineer",
                  "Data Engineer",
                  "Software Engineer (Data Focus)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "R",
                  "Jupyter Notebook",
                  "VS Code",
                  "PyCharm"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-80 giờ học và thực hành cơ bản",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Software Engineer (Backend)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "programming_fundamentals_python_r",
                    "name": "Lập trình cơ bản với Python/R"
                  },
                  {
                    "type": "complementary",
                    "id": "performance_optimization_techniques",
                    "name": "Kỹ thuật tối ưu hiệu suất mã"
                  },
                  {
                    "type": "unlocks",
                    "id": "advanced_algorithms_for_ml",
                    "name": "Thuật toán nâng cao cho Học máy"
                  },
                  {
                    "type": "unlocks",
                    "id": "big_data_processing_frameworks",
                    "name": "Các Framework xử lý dữ liệu lớn (Spark, Hadoop)"
                  }
                ]
              }
            ],
            "skillName": "Phát triển và tối ưu mã Python/R cho phân tích dữ liệu",
            "competency": "Lập trình và Phát triển (Programming & Development)",
            "description": "Kỹ năng này trang bị khả năng viết mã Python và R hiệu quả, có thể mở rộng và tối ưu hóa cao cho các tác vụ phân tích dữ liệu phức tạp. Người học sẽ thành thạo việc áp dụng các cấu trúc dữ liệu và thuật toán phù hợp, cùng với các thư viện chuyên dụng, để giải quyết các vấn đề dữ liệu thực tế một cách tối ưu về hiệu suất và tài nguyên.",
            "learningResources": [
              "Sách: Python for Data Analysis (Wes McKinney) - Các chương về Pandas, NumPy và các kỹ thuật hiệu suất.",
              "Playlist video: R Programming for Data Science - Đại học Johns Hopkins (Coursera/YouTube) - Tập trung vào các gói Tidyverse và tối ưu hóa cơ bản."
            ],
            "projectIdeas": [
              "Xây dựng một hệ thống xử lý và phân tích dữ liệu giao dịch tài chính quy mô lớn. Dự án này bao gồm việc đọc dữ liệu từ nhiều nguồn, làm sạch, biến đổi và tổng hợp dữ liệu sử dụng các thư viện như Pandas/data.table, sau đó tối ưu hóa mã Python/R để đạt hiệu suất cao nhất khi làm việc với hàng triệu bản ghi, sử dụng các kỹ thuật vectorization và quản lý bộ nhớ hiệu quả."
            ],
            "tools": [
              "Python (với Pandas, NumPy, Dask, Numba)",
              "R (với Tidyverse, data.table)",
              "Jupyter Notebook/Lab",
              "RStudio",
              "Các công cụ profiling (ví dụ: cProfile, line_profiler trong Python; profvis trong R)"
            ],
            "difficultyLevel": "Nâng cao trung cấp",
            "estimatedTimeToComplete": "60-80 giờ",
            "importanceScore": 5
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1_2",
            "name": "Triển khai và quản lý mô hình Machine Learning (MLOps)",
            "type": "skill",
            "children": [
              {
                "id": "ds-bigdata-mlops-principles-architecture",
                "name": "Nguyên lý và Kiến trúc MLOps",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này cung cấp nền tảng vững chắc về các nguyên tắc cốt lõi và các mô hình kiến trúc hệ thống để xây dựng, vận hành và quản lý các mô hình Học máy (ML) trong môi trường sản xuất. Nó bao gồm các khái niệm về tự động hóa chu trình ML, quản lý phiên bản, kiểm thử, giám sát và triển khai liên tục, đảm bảo tính bền vững, hiệu quả và đáng tin cậy cho các ứng dụng ML quy mô lớn.",
                "keywords": [
                  "MLOps",
                  "DevOps for ML",
                  "CI/CD for ML",
                  "Machine Learning Pipeline",
                  "Model Governance",
                  "Model Monitoring",
                  "Model Deployment",
                  "Reproducibility",
                  "Scalability",
                  "Experiment Tracking"
                ],
                "learningResources": [
                  {
                    "name": "MLOps: Principles and Best Practices - Google Cloud",
                    "url": "https://cloud.google.com/architecture/mlops-principles"
                  },
                  {
                    "name": "Machine Learning Engineering for Production (MLOps) Specialization - Coursera (DeepLearning.AI)",
                    "url": "https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops"
                  },
                  {
                    "name": "Introduction to MLOps - Microsoft Azure",
                    "url": "https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlops"
                  }
                ],
                "prerequisites": [
                  "ds-ml-foundations-basic-concepts",
                  "software-eng-devops-fundamentals",
                  "cloud-computing-fundamentals",
                  "software-eng-containerization-basics"
                ],
                "projectIdeas": [
                  "Thiết kế kiến trúc MLOps hoàn chỉnh cho một dự án phân loại văn bản: Vẽ sơ đồ kiến trúc, mô tả các giai đoạn (thu thập dữ liệu, huấn luyện, triển khai, giám sát) và lựa chọn công cụ phù hợp cho từng giai đoạn.",
                  "Nghiên cứu và so sánh các nguyên tắc kiến trúc MLOps của hai nền tảng lớn (ví dụ: AWS SageMaker vs. Google Vertex AI) dựa trên các tiêu chí như khả năng mở rộng, quản lý phiên bản và tính tự động hóa."
                ],
                "relatedJobTitles": [
                  "MLOps Engineer",
                  "Machine Learning Engineer",
                  "Data Scientist (MLOps focus)",
                  "AI/ML Solutions Architect",
                  "DevOps Engineer (ML specialization)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Kubernetes",
                  "Docker",
                  "MLflow",
                  "Kubeflow",
                  "Amazon SageMaker",
                  "Google Cloud Vertex AI",
                  "Azure Machine Learning",
                  "CI/CD Tools (GitHub Actions, GitLab CI/CD, Jenkins)",
                  "Prometheus",
                  "Grafana"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "25-35 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "MLOps Engineer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "AI/ML Solutions Architect",
                    "score": 8
                  },
                  {
                    "path": "Cloud Engineer (ML focus)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ds-bigdata-mlops-implementation-kubeflow"
                  },
                  {
                    "type": "prerequisite",
                    "id": "ds-bigdata-mlops-implementation-sagemaker"
                  },
                  {
                    "type": "complementary",
                    "id": "ds-bigdata-model-monitoring-alerting"
                  },
                  {
                    "type": "complementary",
                    "id": "ds-bigdata-data-governance-for-ml"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds-bigdata-mlops-pipeline-building"
                  }
                ]
              },
              {
                "id": "DS_MLOPS_CONTAINER_ORCHESTRATION",
                "name": "Container hóa và Điều phối (Docker, Kubernetes)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc sử dụng các công nghệ container như Docker để đóng gói ứng dụng và mô hình Machine Learning cùng với các phụ thuộc của chúng. Đồng thời, tìm hiểu về các hệ thống điều phối container như Kubernetes để tự động hóa việc triển khai, mở rộng và quản lý các ứng dụng container hóa một cách hiệu quả. Đây là kỹ năng cốt lõi giúp đảm bảo tính nhất quán, khả năng tái tạo và mở rộng cho các hệ thống MLOps.",
                "keywords": [
                  "Docker",
                  "Kubernetes",
                  "Containerization",
                  "Orchestration",
                  "MLOps",
                  "Microservices",
                  "CI/CD",
                  "Pod",
                  "Deployment",
                  "Service"
                ],
                "learningResources": [
                  {
                    "name": "Docker Documentation: Get Started",
                    "url": "https://docs.docker.com/get-started/"
                  },
                  {
                    "name": "Kubernetes Concepts Documentation",
                    "url": "https://kubernetes.io/docs/concepts/"
                  },
                  {
                    "name": "Learn Docker - Full Course for Beginners (freeCodeCamp)",
                    "url": "https://www.freecodecamp.org/news/learn-docker-full-course/"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Container hóa một mô hình Machine Learning đơn giản (ví dụ: Flask/FastAPI với Scikit-learn) bằng Docker và triển khai cục bộ.",
                  "Triển khai một ứng dụng ML đa dịch vụ (ví dụ: API dự đoán, dịch vụ tiền xử lý dữ liệu) lên Kubernetes, sử dụng các khái niệm như Pods, Deployments, và Services."
                ],
                "relatedJobTitles": [
                  "MLOps Engineer",
                  "Data Engineer",
                  "DevOps Engineer",
                  "Machine Learning Engineer",
                  "Cloud Engineer",
                  "Solutions Architect"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Docker Engine",
                  "Docker Compose",
                  "Kubernetes",
                  "kubectl",
                  "Helm"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-6 weeks",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "MLOps Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "complementary",
                    "targetSkillId": "DS_MLOPS_CI_CD_PIPELINES"
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "DS_MLOPS_MONITORING_LOGGING"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS_MLOPS_DISTRIBUTED_ML_SYSTEMS"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS_MLOPS_SCALABLE_MODEL_DEPLOYMENT"
                  }
                ]
              },
              {
                "id": "DS-MLOps-CICD",
                "name": "Tích hợp và Triển khai liên tục (CI/CD) cho Học máy",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc áp dụng các nguyên tắc Tích hợp liên tục (CI) và Triển khai liên tục (CD) vào quy trình phát triển và vận hành mô hình Học máy (MLOps). Nó bao gồm tự động hóa các bước từ kiểm thử, xây dựng, đến triển khai và cập nhật mô hình, nhằm đảm bảo quy trình phát triển ML nhanh hơn, đáng tin cậy hơn và có khả năng lặp lại cao.",
                "keywords": [
                  "MLOps",
                  "CI/CD Pipeline",
                  "Continuous Integration",
                  "Continuous Deployment",
                  "Model Versioning",
                  "Automated Testing",
                  "Model Deployment",
                  "Orchestration",
                  "Infrastructure as Code",
                  "Reproducibility"
                ],
                "learningResources": [
                  {
                    "title": "MLOps: Continuous Delivery and Automation in Machine Learning (Google Cloud)",
                    "url": "https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning"
                  },
                  {
                    "title": "Applying CI/CD to Machine Learning with MLflow and GitHub Actions",
                    "url": "https://www.mlflow.org/docs/latest/llms/llm-evaluate/workflow/github-actions.html"
                  },
                  {
                    "title": "Azure DevOps for MLOps",
                    "url": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-mlops"
                  }
                ],
                "prerequisites": [
                  "DS-MLOps-Fundamentals",
                  "SE-DevOps-CICDFundamentals",
                  "SE-DevOps-VersionControlGit",
                  "DS-ML-BasicConcepts"
                ],
                "projectIdeas": [
                  "Xây dựng một pipeline CI/CD cơ bản cho một mô hình phân loại ảnh đơn giản (ví dụ: MNIST) sử dụng GitHub Actions/GitLab CI để tự động hóa kiểm thử, xây dựng container Docker và triển khai mô hình lên một endpoint API.",
                  "Thiết lập một quy trình CI/CD cho việc tái huấn luyện và cập nhật mô hình dự đoán giá nhà. Pipeline sẽ tự động kích hoạt khi có dữ liệu mới, huấn luyện lại, kiểm thử chất lượng mô hình và triển khai phiên bản mới nếu đạt tiêu chuẩn."
                ],
                "relatedJobTitles": [
                  "MLOps Engineer",
                  "Machine Learning Engineer",
                  "Data Scientist (MLOps Focus)",
                  "DevOps Engineer (ML Specialty)",
                  "AI Platform Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "GitHub Actions",
                  "GitLab CI/CD",
                  "Jenkins",
                  "Azure DevOps",
                  "AWS CodePipeline/CodeBuild",
                  "MLflow",
                  "Kubeflow Pipelines",
                  "DVC (Data Version Control)",
                  "Apache Airflow",
                  "Docker",
                  "Kubernetes"
                ],
                "difficultyLevel": "Advanced",
                "estimatedTimeToComplete": "40-60 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "MLOps Engineer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "DevOps Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS-MLOps-Fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "SE-DevOps-CICDFundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "SE-DevOps-VersionControlGit",
                    "type": "prerequisite"
                  },
                  {
                    "id": "DS-MLOps-ModelMonitoring",
                    "type": "complementary"
                  },
                  {
                    "id": "DS-MLOps-ExperimentTracking",
                    "type": "complementary"
                  },
                  {
                    "id": "DS-MLOps-AdvancedArchitectures",
                    "type": "unlocks"
                  },
                  {
                    "id": "DS-MLOps-ProductionSystemDesign",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "ds-bigdata-mlops-model-monitoring",
                "name": "Giám sát và Quản lý hiệu suất Mô hình",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc theo dõi hiệu suất, độ chính xác và độ lệch của mô hình Machine Learning sau khi chúng được triển khai vào môi trường sản xuất. Mục tiêu là phát hiện sớm các vấn đề như độ lệch dữ liệu (data drift), độ lệch khái niệm (concept drift) hoặc suy giảm hiệu suất để đảm bảo mô hình hoạt động ổn định và hiệu quả theo thời gian, từ đó duy trì giá trị kinh doanh.",
                "keywords": [
                  "Model Monitoring",
                  "Performance Management",
                  "Data Drift",
                  "Concept Drift",
                  "Model Degradation",
                  "MLOps Observability",
                  "Anomaly Detection",
                  "Model Health",
                  "Alerting Systems",
                  "Production ML"
                ],
                "learningResources": [
                  {
                    "name": "Model Monitoring in Vertex AI - Google Cloud",
                    "url": "https://cloud.google.com/vertex-ai/docs/model-monitoring/overview"
                  },
                  {
                    "name": "Monitor Models in Production - AWS SageMaker Model Monitor",
                    "url": "https://aws.amazon.com/sagemaker/model-monitor/"
                  },
                  {
                    "name": "The Ultimate Guide to Model Monitoring in MLOps",
                    "url": "https://neptune.ai/blog/model-monitoring-guide"
                  }
                ],
                "prerequisites": [
                  "ds-bigdata-mlops-model-deployment",
                  "ds-bigdata-data-quality-validation",
                  "ds-bigdata-ml-evaluation-metrics"
                ],
                "projectIdeas": [
                  "Thiết lập một hệ thống giám sát cơ bản cho một mô hình phân loại văn bản đã triển khai, sử dụng Evidently AI để phát hiện độ lệch dữ liệu và hiệu suất giảm.",
                  "Xây dựng một dashboard giám sát (ví dụ: với Grafana) hiển thị các chỉ số hiệu suất quan trọng (accuracy, precision, recall, F1-score) và độ lệch (data/concept drift) cho một mô hình dự đoán giá nhà."
                ],
                "relatedJobTitles": [
                  "MLOps Engineer",
                  "Machine Learning Engineer",
                  "Data Scientist (MLOps focus)",
                  "Applied Scientist",
                  "Cloud AI Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "AWS SageMaker Model Monitor",
                  "Google Cloud Vertex AI Model Monitoring",
                  "Azure Machine Learning Monitor",
                  "Evidently AI",
                  "WhyLabs (Whylogs)",
                  "Prometheus",
                  "Grafana",
                  "MLflow",
                  "Kubeflow",
                  "Datadog"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "MLOps Engineer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Cloud AI/ML Architect",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds-bigdata-mlops-model-deployment",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds-bigdata-data-quality-validation",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds-bigdata-ml-evaluation-metrics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds-bigdata-mlops-model-retraining",
                    "type": "complementary"
                  },
                  {
                    "id": "ds-bigdata-ml-explainability",
                    "type": "complementary"
                  },
                  {
                    "id": "ds-bigdata-mlops-automated-pipelines",
                    "type": "unlocks"
                  },
                  {
                    "id": "ds-bigdata-mlops-model-governance-advanced",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "MLOps_ModelVersioningTesting",
                "name": "Quản lý Phiên bản và Thử nghiệm Mô hình",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các phương pháp và công cụ để theo dõi, quản lý các phiên bản khác nhau của mô hình Machine Learning, cũng như các kỹ thuật thử nghiệm toàn diện để đảm bảo chất lượng và hiệu suất của mô hình trước và sau khi triển khai. Đây là yếu tố then chốt để duy trì tính nhất quán, khả năng tái tạo và độ tin cậy của các hệ thống AI trong môi trường sản xuất.",
                "keywords": [
                  "Model Versioning",
                  "Experiment Tracking",
                  "Model Registry",
                  "MLflow",
                  "DVC",
                  "Model Testing",
                  "A/B Testing",
                  "Canary Deployment",
                  "Model Quality Assurance",
                  "Reproducibility"
                ],
                "learningResources": [
                  {
                    "name": "MLflow Documentation: Tracking, Projects, and Models",
                    "url": "https://mlflow.org/docs/latest/index.html"
                  },
                  {
                    "name": "DVC Documentation: Data Version Control",
                    "url": "https://dvc.org/doc/start"
                  },
                  {
                    "name": "Continuous Testing in MLOps (Google Cloud)",
                    "url": "https://cloud.google.com/architecture/continuous-testing-mlops"
                  }
                ],
                "prerequisites": [
                  "MLOps_Intro",
                  "MLOps_DataVersioning",
                  "MLOps_CICD_ML",
                  "MLOps_Containerization"
                ],
                "projectIdeas": [
                  "Xây dựng một pipeline ML đơn giản sử dụng MLflow để theo dõi các thử nghiệm, đăng ký mô hình và quản lý phiên bản mô hình. Triển khai hai phiên bản mô hình khác nhau và so sánh hiệu suất.",
                  "Thiết kế và triển khai một chiến lược thử nghiệm tự động cho mô hình ML, bao gồm kiểm thử chất lượng dữ liệu đầu vào (data validation), kiểm thử hiệu suất mô hình (model performance testing) và kiểm thử độ lệch (drift detection) sử dụng các công cụ như Great Expectations hoặc TensorFlow Data Validation."
                ],
                "relatedJobTitles": [
                  "MLOps Engineer",
                  "Machine Learning Engineer",
                  "AI/ML Architect",
                  "Data Scientist (MLOps Focus)",
                  "DevOps Engineer (with ML focus)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "MLflow",
                  "DVC",
                  "Git",
                  "Kubeflow Pipelines",
                  "TFX (TensorFlow Extended)",
                  "Great Expectations",
                  "Apache Airflow (for orchestration)",
                  "AWS SageMaker Model Registry",
                  "Azure ML Model Registry",
                  "Google Cloud Vertex AI Model Registry"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "MLOps Engineer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "AI/ML Architect",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "Research Scientist (ML/AI)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "MLOps_CICD_ML"
                  },
                  {
                    "type": "complementary",
                    "id": "MLOps_ModelDeploymentStrategies"
                  },
                  {
                    "type": "complementary",
                    "id": "MLOps_ModelMonitoring"
                  },
                  {
                    "type": "unlocks",
                    "id": "MLOps_AdvancedExperimentation"
                  },
                  {
                    "type": "prerequisite",
                    "id": "MLOps_DataVersioning"
                  }
                ]
              }
            ],
            "skillName": "Triển khai và quản lý mô hình Machine Learning (MLOps)",
            "competency": "Lập trình và Phát triển (Programming & Development)",
            "description": "Kỹ năng này trang bị cho bạn khả năng thiết kế, xây dựng và quản lý các quy trình làm việc tự động cho mô hình Học máy từ giai đoạn phát triển đến triển khai sản phẩm. Bạn sẽ học cách đảm bảo các mô hình hoạt động ổn định, có thể mở rộng và được giám sát liên tục trong môi trường thực tế.",
            "learningResources": [
              {
                "title": "MLOps Specialization",
                "type": "Khóa học trực tuyến",
                "url": "https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops"
              },
              {
                "title": "Sách: Introducing MLOps",
                "type": "Sách điện tử/Vật lý",
                "url": "https://www.oreilly.com/library/view/introducing-mlops/9781492083292/"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng hệ thống MLOps cho mô hình dự đoán giá nhà",
                "description": "Phát triển một quy trình MLOps đầu cuối (end-to-end) cho mô hình dự đoán giá nhà. Dự án sẽ bao gồm các bước: huấn luyện mô hình, container hóa (Docker), triển khai lên một cụm Kubernetes, thiết lập CI/CD để tự động cập nhật mô hình khi có phiên bản mới, và giám sát hiệu suất mô hình đã triển khai (ví dụ: với Prometheus/Grafana) để phát hiện sự suy giảm chất lượng (model drift)."
              }
            ],
            "tools": [
              "Docker",
              "Kubernetes",
              "MLflow",
              "Kubeflow",
              "Airflow",
              "Jenkins",
              "GitHub Actions",
              "GitLab CI/CD",
              "Prometheus",
              "Grafana",
              "TensorFlow Extended (TFX)",
              "DVC (Data Version Control)"
            ],
            "difficultyLevel": "Nâng cao",
            "estimatedTimeToComplete": "80-120 giờ",
            "importanceScore": 9,
            "knowledgeUnits": [
              "Nguyên lý và Kiến trúc MLOps",
              "Container hóa và Điều phối (Docker, Kubernetes)",
              "Tích hợp và Triển khai liên tục (CI/CD) cho Học máy",
              "Giám sát và Quản lý hiệu suất Mô hình",
              "Quản lý Phiên bản và Thử nghiệm Mô hình"
            ]
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1_3",
            "name": "Lập trình và tích hợp hệ thống Big Data (ví dụ: Apache Spark, Hadoop)",
            "type": "skill",
            "children": [
              {
                "id": "hadoop_core_architecture",
                "name": "Kiến trúc và Thành phần cốt lõi của Hệ sinh thái Hadoop (HDFS, YARN)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc hiểu cấu trúc phân tán của Hadoop, với hai thành phần cốt lõi là Hệ thống tệp phân tán Hadoop (HDFS) và Trình điều phối tài nguyên YARN. Nắm vững HDFS và YARN là nền tảng để lưu trữ và xử lý dữ liệu lớn hiệu quả, cung cấp khả năng mở rộng và khả năng chịu lỗi cho các ứng dụng Big Data.",
                "keywords": [
                  "Hadoop",
                  "HDFS",
                  "YARN",
                  "Big Data",
                  "Distributed File System",
                  "Resource Management",
                  "NameNode",
                  "DataNode",
                  "ResourceManager",
                  "NodeManager",
                  "MapReduce",
                  "Fault Tolerance",
                  "Scalability",
                  "Apache Hadoop"
                ],
                "learningResources": [
                  {
                    "name": "Apache Hadoop 3.3.6 Documentation - HDFS Architecture Guide",
                    "url": "https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
                  },
                  {
                    "name": "Apache Hadoop 3.3.6 Documentation - YARN Architecture Guide",
                    "url": "https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html"
                  },
                  {
                    "name": "Hadoop HDFS Tutorial - Tutorialspoint",
                    "url": "https://www.tutorialspoint.com/hadoop/hadoop_hdfs_overview.htm"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Cài đặt và cấu hình một cụm Hadoop đơn lẻ (Pseudo-Distributed Mode) trên máy ảo hoặc Docker, sau đó thực hiện các thao tác cơ bản với HDFS như tạo thư mục, tải tệp lên/xuống, và xem trạng thái HDFS.",
                  "Chạy một ứng dụng MapReduce đơn giản (ví dụ: đếm từ) trên cụm Hadoop đã cài đặt, theo dõi việc sử dụng tài nguyên và tiến trình ứng dụng thông qua YARN Web UI."
                ],
                "relatedJobTitles": [
                  "Big Data Engineer",
                  "Data Engineer",
                  "Hadoop Administrator",
                  "DevOps Engineer (Big Data)",
                  "Cloud Data Architect"
                ],
                "marketDemand": "High",
                "tools": [
                  "Apache Hadoop",
                  "HDFS CLI",
                  "YARN Web UI",
                  "Linux (Command Line)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "12-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Big Data Architect",
                    "score": 10
                  },
                  {
                    "path": "DevOps Engineer",
                    "score": 8
                  },
                  {
                    "path": "Cloud Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "id": "big_data_programming_integration",
                    "type": "complementary",
                    "description": "Là một phần cốt lõi không thể thiếu của kỹ năng 'Lập trình và tích hợp hệ thống Big Data (ví dụ: Apache Spark, Hadoop)'."
                  },
                  {
                    "id": "hadoop_mapreduce_programming",
                    "type": "unlocks",
                    "description": "Việc hiểu sâu về kiến trúc HDFS và cách YARN quản lý tài nguyên là điều kiện tiên quyết để có thể lập trình và tối ưu các ứng dụng MapReduce hiệu quả."
                  },
                  {
                    "id": "hadoop_cluster_administration",
                    "type": "unlocks",
                    "description": "Kiến thức nền tảng để triển khai, quản lý, giám sát và khắc phục sự cố các cụm Hadoop trong môi trường sản xuất."
                  }
                ]
              },
              {
                "id": "ds_bd_spark_arch_prog_models",
                "name": "Kiến trúc và Mô hình lập trình của Apache Spark (RDDs, DataFrames, Datasets)",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào việc hiểu rõ ba mô hình lập trình cốt lõi của Apache Spark: Resilient Distributed Datasets (RDDs), DataFrames, và Datasets. Học viên sẽ nắm vững sự khác biệt, ưu nhược điểm, và các trường hợp sử dụng tối ưu cho từng mô hình, từ đó có thể thiết kế và triển khai các ứng dụng xử lý dữ liệu lớn hiệu quả trên nền tảng Spark.",
                "keywords": [
                  "Apache Spark",
                  "RDDs",
                  "DataFrames",
                  "Datasets",
                  "Distributed Computing",
                  "Big Data Processing",
                  "Lazy Evaluation",
                  "Catalyst Optimizer",
                  "Tungsten Engine",
                  "Spark SQL"
                ],
                "learningResources": [
                  {
                    "name": "Apache Spark Programming Guide - RDDs",
                    "url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
                  },
                  {
                    "name": "Apache Spark Programming Guide - Structured APIs (DataFrames & Datasets)",
                    "url": "https://spark.apache.org/docs/latest/sql-programming-guide.html"
                  },
                  {
                    "name": "Databricks Blog - A Tale of Three APIs: RDDs, DataFrames, and Datasets",
                    "url": "https://databricks.com/blog/2016/07/14/a-tale-of-three-apis-rdds-dataframes-and-datasets.html"
                  }
                ],
                "prerequisites": [
                  "ds_bd_big_data_concepts_intro",
                  "ds_bd_spark_intro",
                  "programming_python_intermediate"
                ],
                "projectIdeas": [
                  "Phân tích dữ liệu bán hàng với DataFrames: Tải một bộ dữ liệu bán hàng (CSV/JSON), thực hiện các phép biến đổi cơ bản (lọc, nhóm, tổng hợp) sử dụng Spark DataFrames, và hiển thị kết quả.",
                  "Chuyển đổi và xử lý dữ liệu hỗn hợp: Phát triển một ứng dụng Spark để đọc dữ liệu từ nhiều nguồn (ví dụ: một phần từ RDD, một phần từ DataFrame), thực hiện chuyển đổi giữa RDD, DataFrame, Dataset, sau đó thực hiện các phép toán phức tạp và lưu trữ kết quả."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "Spark Developer",
                  "Data Scientist",
                  "Machine Learning Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Apache Spark",
                  "PySpark",
                  "Spark-Scala API",
                  "Jupyter Notebook",
                  "Apache Zeppelin",
                  "IntelliJ IDEA"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Cloud Data Architect",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds_bd_spark_intro",
                    "type": "prerequisite",
                    "description": "Kiến thức cơ bản về Apache Spark và vai trò của nó."
                  },
                  {
                    "id": "ds_bd_spark_sql",
                    "type": "complementary",
                    "description": "Spark SQL xây dựng trên DataFrames/Datasets để xử lý dữ liệu có cấu trúc."
                  },
                  {
                    "id": "ds_bd_spark_optimizations",
                    "type": "unlocks",
                    "description": "Nắm vững RDDs/DataFrames/Datasets là tiền đề để hiểu sâu về tối ưu hóa hiệu suất Spark."
                  },
                  {
                    "id": "ds_bd_spark_mllib",
                    "type": "complementary",
                    "description": "MLlib thường sử dụng DataFrames để xây dựng các pipeline học máy."
                  }
                ]
              },
              {
                "id": "data-science_big-data_distributed-parallel-processing-principles",
                "name": "Nguyên lý xử lý dữ liệu phân tán và song song",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này cung cấp nền tảng về cách thức thiết kế và vận hành các hệ thống máy tính hoạt động đồng thời trên nhiều nút để giải quyết các bài toán lớn. Nắm vững các nguyên lý này là cốt lõi để xây dựng các giải pháp Big Data có khả năng mở rộng, chịu lỗi và đạt hiệu suất cao, đáp ứng nhu cầu xử lý lượng dữ liệu khổng lồ và phức tạp trong môi trường hiện đại.",
                "keywords": [
                  "Hệ thống phân tán",
                  "Tính toán song song",
                  "Xử lý dữ liệu lớn",
                  "Tính toán chịu lỗi",
                  "Khả năng mở rộng (Scalability)",
                  "Đồng bộ hóa",
                  "MapReduce",
                  "Apache Hadoop",
                  "Apache Spark",
                  "Độ nhất quán dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "Introduction to Distributed Systems - Cornell University (Coursera)",
                    "url": "https://www.coursera.org/learn/distributed-systems-cornell",
                    "type": "Course"
                  },
                  {
                    "name": "HDFS Architecture Guide - Apache Hadoop",
                    "url": "https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html",
                    "type": "Documentation"
                  },
                  {
                    "name": "RDD Programming Guide - Apache Spark",
                    "url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html",
                    "type": "Documentation"
                  }
                ],
                "prerequisites": [
                  "programming-fundamentals",
                  "data-structures-algorithms",
                  "operating-system-fundamentals",
                  "networking-basics"
                ],
                "projectIdeas": [
                  "Thiết kế và mô phỏng một thuật toán MapReduce đơn giản (ví dụ: đếm từ) sử dụng mô hình đa luồng/đa tiến trình cục bộ để hiểu cơ chế song song hóa.",
                  "Phác thảo kiến trúc logic cho một hệ thống lưu trữ khóa-giá trị phân tán cơ bản, tập trung vào cách phân phối dữ liệu và đảm bảo tính chịu lỗi (ví dụ: nhân bản dữ liệu)."
                ],
                "relatedJobTitles": [
                  "Kỹ sư Big Data (Big Data Engineer)",
                  "Kỹ sư dữ liệu (Data Engineer)",
                  "Kỹ sư hệ thống phân tán (Distributed Systems Engineer)",
                  "Kỹ sư Đám mây (Cloud Engineer)",
                  "Nhà khoa học dữ liệu (Data Scientist - chuyên về Big Data)",
                  "Kỹ sư phần mềm (Backend/Distributed Systems)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Hadoop",
                  "Apache Spark",
                  "Apache Kafka",
                  "Apache Flink",
                  "Kubernetes",
                  "Apache ZooKeeper",
                  "Ray"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-50 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Kỹ sư Big Data",
                    "score": 10
                  },
                  {
                    "path": "Kỹ sư dữ liệu",
                    "score": 10
                  },
                  {
                    "path": "Kỹ sư hệ thống phân tán",
                    "score": 9
                  },
                  {
                    "path": "Kỹ sư Đám mây",
                    "score": 8
                  },
                  {
                    "path": "Nhà khoa học dữ liệu",
                    "score": 7
                  },
                  {
                    "path": "Kỹ sư MLOps",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "programming-fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-structures-algorithms",
                    "type": "prerequisite"
                  },
                  {
                    "id": "operating-system-fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "networking-basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "big-data-system-architecture",
                    "type": "unlocks"
                  },
                  {
                    "id": "fault-tolerance-scalability-design",
                    "type": "complementary"
                  },
                  {
                    "id": "distributed-system-performance-optimization",
                    "type": "unlocks"
                  },
                  {
                    "id": "cloud-big-data-deployment",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "big_data_popular_programming_languages",
                "name": "Các ngôn ngữ lập trình phổ biến cho Big Data (ví dụ: Scala, Python, Java)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc hiểu và sử dụng thành thạo các ngôn ngữ lập trình chủ chốt như Scala, Python và Java trong bối cảnh Dữ liệu lớn. Nắm vững các ngôn ngữ này là điều cần thiết để phát triển, triển khai và quản lý các ứng dụng xử lý dữ liệu quy mô lớn trên các nền tảng như Apache Spark và Hadoop. Điều này cho phép các nhà khoa học dữ liệu và kỹ sư dữ liệu xây dựng các giải pháp Big Data hiệu quả và mạnh mẽ.",
                "keywords": [
                  "Scala",
                  "Python",
                  "Java",
                  "Big Data Programming",
                  "Apache Spark",
                  "Hadoop",
                  "Distributed Computing",
                  "Data Processing",
                  "Data Engineering",
                  "JVM",
                  "PySpark",
                  "Spark SQL"
                ],
                "learningResources": [
                  {
                    "name": "IBM Python for Data Science, AI & Development",
                    "url": "https://www.coursera.org/specializations/python-data-science"
                  },
                  {
                    "name": "Scala Programming for Apache Spark (Databricks Guide)",
                    "url": "https://docs.databricks.com/getting-started/scala-programming.html"
                  },
                  {
                    "name": "Java for Big Data - Why and How (Edureka Blog)",
                    "url": "https://www.edureka.co/blog/java-for-big-data/"
                  }
                ],
                "prerequisites": [
                  {
                    "id": "programming_fundamentals_introduction",
                    "name": "Các khái niệm lập trình cơ bản"
                  },
                  {
                    "id": "data_structures_algorithms_intro",
                    "name": "Cấu trúc dữ liệu và giải thuật"
                  }
                ],
                "projectIdeas": [
                  "Xây dựng một ứng dụng Apache Spark đơn giản bằng Python (PySpark) hoặc Scala để phân tích tập dữ liệu công khai lớn (ví dụ: dữ liệu giao thông, dữ liệu cảm biến) và tạo báo cáo tổng hợp.",
                  "Triển khai một MapReduce job cơ bản bằng Java trên hệ thống Hadoop để thực hiện đếm từ (Word Count) hoặc phân tích tần suất trên một tập văn bản quy mô lớn."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Developer",
                  "Spark Developer",
                  "Data Scientist (với kỹ năng kỹ thuật)",
                  "Software Engineer (Big Data)",
                  "ML Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Spark",
                  "Apache Hadoop",
                  "Jupyter Notebook",
                  "IntelliJ IDEA",
                  "VS Code",
                  "Apache Maven",
                  "Gradle",
                  "PyCharm",
                  "Scala IDE"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-80 hours (để đạt được hiểu biết nền tảng và khả năng ứng dụng thực tế trên các ngôn ngữ chính)",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Big Data Development",
                    "score": 10
                  },
                  {
                    "path": "Data Science",
                    "score": 8
                  },
                  {
                    "path": "Machine Learning Engineering",
                    "score": 8
                  },
                  {
                    "path": "Cloud Data Architect",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "programming_fundamentals_introduction",
                    "name": "Các khái niệm lập trình cơ bản"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data_structures_algorithms_intro",
                    "name": "Cấu trúc dữ liệu và giải thuật"
                  },
                  {
                    "type": "complementary",
                    "id": "apache_spark_fundamentals",
                    "name": "Tổng quan và các khái niệm cơ bản về Apache Spark"
                  },
                  {
                    "type": "complementary",
                    "id": "apache_hadoop_ecosystem",
                    "name": "Hệ sinh thái Apache Hadoop"
                  },
                  {
                    "type": "unlocks",
                    "id": "big_data_pipeline_design",
                    "name": "Thiết kế và triển khai đường ống dữ liệu lớn (Big Data Pipeline)"
                  },
                  {
                    "type": "unlocks",
                    "id": "advanced_spark_ml",
                    "name": "Học máy nâng cao với Apache Spark"
                  }
                ]
              }
            ],
            "skillName": "Lập trình và tích hợp hệ thống Big Data (ví dụ: Apache Spark, Hadoop)",
            "competency": "Lập trình và Phát triển (Programming & Development)",
            "knowledgeUnits": [
              "Kiến trúc và Thành phần cốt lõi của Hệ sinh thái Hadoop (HDFS, YARN)",
              "Kiến trúc và Mô hình lập trình của Apache Spark (RDDs, DataFrames, Datasets)",
              "Nguyên lý xử lý dữ liệu phân tán và song song",
              "Các ngôn ngữ lập trình phổ biến cho Big Data (ví dụ: Scala, Python, Java)"
            ],
            "description": "Kỹ năng này cho phép người học thiết kế, phát triển và tích hợp các ứng dụng có khả năng mở rộng để xử lý và phân tích các tập dữ liệu lớn. Nó bao gồm việc nắm vững các khung công tác điện toán phân tán như Apache Spark và các thành phần cốt lõi của hệ sinh thái Hadoop để giải quyết các thách thức về Big Data.",
            "learningResources": [
              {
                "title": "Chương 'Distributed Data Processing with Apache Spark and Hadoop' trong một sách giáo trình Big Data",
                "type": "Book Chapter",
                "url": "https://example.com/big-data-book-chapter-spark-hadoop"
              },
              {
                "title": "Playlist: 'Introduction to Apache Spark & Hadoop Ecosystem'",
                "type": "Video Playlist",
                "url": "https://example.com/youtube-playlist-spark-hadoop"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng hệ thống ETL phân tích dữ liệu cảm biến",
                "description": "Phát triển một pipeline ETL (Extract-Transform-Load) sử dụng Apache Spark để đọc dữ liệu cảm biến (ví dụ: nhiệt độ, độ ẩm) từ HDFS, thực hiện các phép biến đổi (làm sạch, tổng hợp, phân tích cơ bản) và lưu trữ kết quả đã xử lý trở lại HDFS hoặc một cơ sở dữ liệu khác để phân tích sâu hơn. Dự án này sẽ yêu cầu sử dụng Spark RDDs/DataFrames/Datasets, hiểu biết về HDFS và áp dụng một trong các ngôn ngữ lập trình như Python (PySpark) hoặc Scala.",
                "complexity": "Intermediate"
              }
            ],
            "tools": [
              "Apache Spark",
              "Apache Hadoop (HDFS, YARN)",
              "Python (PySpark)",
              "Scala",
              "Java",
              "Jupyter Notebook (để phát triển và kiểm thử)"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "60 hours",
            "importanceScore": 9
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1_4",
            "name": "Phát triển ETL và Pipelines dữ liệu",
            "type": "skill",
            "children": [
              {
                "id": "relational_databases_sql",
                "name": "Cơ sở dữ liệu quan hệ và SQL",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này bao gồm các nguyên tắc cơ bản của hệ quản trị cơ sở dữ liệu quan hệ (RDBMS) và ngôn ngữ truy vấn cấu trúc (SQL). Nắm vững RDBMS và SQL là nền tảng thiết yếu để lưu trữ, truy xuất, quản lý và phân tích dữ liệu hiệu quả, đóng vai trò trung tâm trong các quy trình Phát triển ETL và Pipelines dữ liệu cũng như trong Khoa học dữ liệu.",
                "keywords": [
                  "SQL",
                  "RDBMS",
                  "Database Query",
                  "Normalization",
                  "ACID Properties",
                  "JOIN",
                  "DDL",
                  "DML",
                  "Data Manipulation",
                  "Database Schema"
                ],
                "learningResources": [
                  {
                    "name": "SQL Tutorial - W3Schools",
                    "url": "https://www.w3schools.com/sql/"
                  },
                  {
                    "name": "SQLBolt - Learn SQL Interactívely",
                    "url": "https://sqlbolt.com/"
                  },
                  {
                    "name": "Databases and SQL for Data Science with Python - Coursera",
                    "url": "https://www.coursera.org/learn/databases-and-sql-for-data-science-with-python"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Thiết kế một lược đồ cơ sở dữ liệu quan hệ đơn giản (ví dụ: quản lý cửa hàng sách, hệ thống đăng ký khóa học) và triển khai nó trong một RDBMS (như PostgreSQL hoặc MySQL). Sau đó, viết các truy vấn SQL để thêm, cập nhật, xóa và truy xuất dữ liệu phức tạp (sử dụng JOIN, GROUP BY, subqueries).",
                  "Sử dụng SQL để phân tích một bộ dữ liệu công khai lớn (ví dụ: từ Kaggle) liên quan đến các giao dịch hoặc sự kiện. Thực hiện các truy vấn để tìm ra xu hướng, tính toán các chỉ số hiệu suất chính (KPIs), và tạo báo cáo tổng hợp để rút ra insight."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Data Scientist",
                  "ETL Developer",
                  "Data Engineer",
                  "Business Intelligence Developer",
                  "Database Administrator (DBA)",
                  "Backend Developer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "PostgreSQL",
                  "MySQL",
                  "SQL Server",
                  "SQLite",
                  "Oracle Database",
                  "pgAdmin",
                  "DBeaver",
                  "MySQL Workbench",
                  "SSMS (SQL Server Management Studio)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "40-60 hours (để đạt được kiến thức thực hành cơ bản và trung cấp)",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "ETL Developer",
                    "score": 10
                  },
                  {
                    "path": "Database Administrator",
                    "score": 10
                  },
                  {
                    "path": "Backend Developer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data_modeling_design",
                    "type": "complementary",
                    "description": "Kỹ năng thiết kế lược đồ cơ sở dữ liệu hiệu quả là vô cùng quan trọng để triển khai cơ sở dữ liệu quan hệ bằng SQL một cách tối ưu."
                  },
                  {
                    "id": "advanced_sql_optimization",
                    "type": "unlocks",
                    "description": "Nắm vững SQL cơ bản là tiền đề để học các kỹ thuật SQL nâng cao, tối ưu hóa truy vấn và xử lý dữ liệu phức tạp hơn."
                  },
                  {
                    "id": "python_for_data_manipulation",
                    "type": "complementary",
                    "description": "Thường xuyên kết hợp Python với các thư viện như `psycopg2` hoặc `SQLAlchemy` để tương tác với cơ sở dữ liệu quan hệ từ các ứng dụng."
                  },
                  {
                    "id": "data_warehousing_concepts",
                    "type": "unlocks",
                    "description": "Kiến thức SQL là nền tảng cốt lõi để xây dựng, quản lý và truy vấn dữ liệu trong các kho dữ liệu (Data Warehouses) và Data Marts."
                  },
                  {
                    "id": "etl_pipeline_development",
                    "type": "unlocks",
                    "description": "SQL là công cụ chính được sử dụng trong mọi giai đoạn của quy trình ETL (Extract, Transform, Load) để làm sạch, biến đổi và tải dữ liệu."
                  },
                  {
                    "id": "nosql_databases",
                    "type": "complementary",
                    "description": "Hiểu biết về cơ sở dữ liệu quan hệ giúp so sánh và lựa chọn giải pháp lưu trữ dữ liệu phù hợp khi làm việc với các hệ thống NoSQL."
                  }
                ]
              },
              {
                "id": "data-warehouse-data-lake-concepts",
                "name": "Khái niệm về Kho dữ liệu (Data Warehouse) và Hồ dữ liệu (Data Lake)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giới thiệu về hai kiến trúc lưu trữ và xử lý dữ liệu quan trọng: Data Warehouse và Data Lake. Học viên sẽ hiểu sự khác biệt cơ bản, ưu nhược điểm, và các trường hợp sử dụng phù hợp cho mỗi loại, là nền tảng thiết yếu để thiết kế các hệ thống dữ liệu hiện đại cho phân tích và báo cáo.",
                "keywords": [
                  "Data Warehouse",
                  "Data Lake",
                  "OLAP",
                  "OLTP",
                  "Schema-on-read",
                  "Schema-on-write",
                  "Raw data",
                  "Structured data",
                  "Unstructured data",
                  "ETL"
                ],
                "learningResources": [
                  {
                    "name": "What is a data lake? What is a data warehouse?",
                    "url": "https://aws.amazon.com/compare/the-difference-between-data-lake-and-data-warehouse/"
                  },
                  {
                    "name": "Data warehouse vs. data lake vs. data lakehouse",
                    "url": "https://www.ibm.com/topics/data-lakehouse"
                  },
                  {
                    "name": "Data warehousing in Azure",
                    "url": "https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/data-warehousing"
                  }
                ],
                "prerequisites": [
                  "database-fundamentals",
                  "introduction-to-data-modeling"
                ],
                "projectIdeas": [
                  "Nghiên cứu và trình bày so sánh chi tiết giữa Data Warehouse và Data Lake, bao gồm các kịch bản sử dụng thực tế và công nghệ hỗ trợ.",
                  "Phác thảo kiến trúc cấp cao cho một hệ thống dữ liệu phân tích, quyết định khi nào nên sử dụng Data Warehouse, Data Lake, hoặc kết hợp cả hai cho một công ty bán lẻ."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Data Architect",
                  "Big Data Engineer",
                  "Business Intelligence Developer",
                  "Cloud Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Amazon Redshift",
                  "Google BigQuery",
                  "Snowflake",
                  "Azure Synapse Analytics",
                  "Amazon S3",
                  "Azure Data Lake Storage",
                  "Hadoop HDFS",
                  "Apache Spark"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Data Architecture",
                    "score": 10
                  },
                  {
                    "path": "Big Data Development",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence",
                    "score": 8
                  },
                  {
                    "path": "Data Science",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "database-fundamentals",
                    "type": "prerequisite",
                    "description": "Cần hiểu về cơ sở dữ liệu quan hệ và phi quan hệ."
                  },
                  {
                    "id": "introduction-to-data-modeling",
                    "type": "prerequisite",
                    "description": "Cần có kiến thức cơ bản về mô hình hóa dữ liệu để hiểu cấu trúc trong Data Warehouse."
                  },
                  {
                    "id": "designing-data-warehouse-schemas",
                    "type": "unlocks",
                    "description": "Kiến thức nền tảng để thiết kế các schema trong Data Warehouse."
                  },
                  {
                    "id": "data-lake-architecture-design",
                    "type": "unlocks",
                    "description": "Nền tảng để thiết kế kiến trúc và quản lý dữ liệu trong Data Lake."
                  },
                  {
                    "id": "etl-pipeline-development-fundamentals",
                    "type": "unlocks",
                    "description": "Quan trọng để hiểu luồng dữ liệu vào DW/DL và các công cụ ETL."
                  },
                  {
                    "id": "cloud-data-platform-fundamentals",
                    "type": "complementary",
                    "description": "Hỗ trợ hiểu cách triển khai DW/DL trên các nền tảng đám mây."
                  }
                ]
              },
              {
                "id": "knowledge_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_1_4_3",
                "name": "Các định dạng dữ liệu phổ biến (CSV, JSON, Parquet, Avro)",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này giới thiệu về các định dạng dữ liệu phổ biến như CSV, JSON, Parquet và Avro, giải thích cách chúng được sử dụng để cấu trúc, lưu trữ và trao đổi dữ liệu. Việc nắm vững các định dạng này là nền tảng để tối ưu hóa hiệu suất I/O, quản lý schema dữ liệu, và đảm bảo khả năng tương thích trong các hệ thống xử lý dữ liệu lớn và các pipeline ETL hiệu quả.",
                "keywords": [
                  "CSV",
                  "JSON",
                  "Parquet",
                  "Avro",
                  "Định dạng dữ liệu",
                  "Serialization",
                  "Schema Evolution",
                  "Lưu trữ cột",
                  "Nén dữ liệu",
                  "ETL",
                  "Data Pipeline"
                ],
                "learningResources": [
                  {
                    "title": "Understanding Common Data Formats: CSV, JSON, XML, Parquet, Avro, and Protobuf",
                    "url": "https://towardsdatascience.com/understanding-common-data-formats-csv-json-xml-parquet-avro-and-protobuf-8f0a3d4f1c1f",
                    "type": "Article"
                  },
                  {
                    "title": "Apache Parquet Official Website",
                    "url": "https://parquet.apache.org/",
                    "type": "Documentation"
                  },
                  {
                    "title": "Apache Avro Official Website",
                    "url": "https://avro.apache.org/",
                    "type": "Documentation"
                  }
                ],
                "prerequisites": [
                  "programming-fundamentals-python",
                  "basic-data-structures"
                ],
                "projectIdeas": [
                  "**Phân tích và Chuyển đổi Hiệu suất Định dạng:** Lấy một tập dữ liệu lớn (ví dụ: từ Kaggle) ở định dạng CSV hoặc JSON. Viết mã để chuyển đổi tập dữ liệu này sang Parquet và Avro. So sánh kích thước tệp, thời gian đọc và ghi dữ liệu cho mỗi định dạng bằng cách sử dụng thư viện như Pandas/PyArrow (hoặc PySpark).",
                  "**Thiết kế Schema và Tương thích Dữ liệu:** Thiết kế một schema Avro cho một loại dữ liệu phức tạp (ví dụ: nhật ký sự kiện, hồ sơ người dùng). Tạo một vài bản ghi theo schema đó và lưu vào tệp Avro. Sau đó, sửa đổi schema (ví dụ: thêm một trường mới, đổi tên một trường) và thực hành đọc dữ liệu cũ bằng schema mới để hiểu về schema evolution."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "ETL Developer",
                  "Data Architect",
                  "Data Scientist",
                  "Cloud Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas, PyArrow, Fastavro)",
                  "Apache Spark",
                  "Apache Hive",
                  "Apache Flink",
                  "Apache Nifi",
                  "JSON.org",
                  "CSVKit"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "16-24 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "ETL Developer",
                    "score": 9
                  },
                  {
                    "path": "Cloud Data Engineer",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "programming-fundamentals-python",
                    "type": "prerequisite"
                  },
                  {
                    "id": "basic-data-structures",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-serialization-concepts",
                    "type": "complementary"
                  },
                  {
                    "id": "data-compression-techniques",
                    "type": "complementary"
                  },
                  {
                    "id": "distributed-file-systems-hdfs-s3",
                    "type": "complementary"
                  },
                  {
                    "id": "building-data-lakes",
                    "type": "unlocks"
                  },
                  {
                    "id": "optimizing-data-pipelines",
                    "type": "unlocks"
                  },
                  {
                    "id": "real-time-data-streaming-with-kafka",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "ds-etl-basic-python-data-processing",
                "name": "Ngôn ngữ lập trình cơ bản (ví dụ: Python) cho xử lý dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giới thiệu những nguyên tắc cơ bản của lập trình Python, tập trung vào các ứng dụng trong xử lý và thao tác dữ liệu. Nắm vững Python là nền tảng thiết yếu để xây dựng các quy trình ETL (Extract, Transform, Load) và pipeline dữ liệu hiệu quả, cho phép trích xuất, làm sạch, chuyển đổi và tải dữ liệu một cách tự động.",
                "keywords": [
                  "Python",
                  "Lập trình cơ bản",
                  "Xử lý dữ liệu",
                  "Thao tác dữ liệu",
                  "Pandas",
                  "NumPy",
                  "Data cleaning",
                  "Scripting",
                  "Kiểu dữ liệu Python",
                  "Cấu trúc dữ liệu Python"
                ],
                "learningResources": [
                  {
                    "name": "The Python Tutorial (Official Documentation)",
                    "url": "https://docs.python.org/3/tutorial/",
                    "type": "Documentation"
                  },
                  {
                    "name": "Learn Python for Data Science (freeCodeCamp)",
                    "url": "https://www.freecodecamp.org/news/learn-python-for-data-science/",
                    "type": "Article/Tutorial"
                  },
                  {
                    "name": "Pandas User Guide (Official Documentation)",
                    "url": "https://pandas.pydata.org/docs/user_guide/index.html",
                    "type": "Documentation"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Xây dựng một script Python để đọc dữ liệu từ một file CSV, thực hiện làm sạch dữ liệu cơ bản (ví dụ: xử lý giá trị thiếu, định dạng lại cột), sau đó lưu kết quả vào một file CSV mới.",
                  "Viết chương trình Python để trích xuất dữ liệu từ một trang web đơn giản (web scraping với các thư viện như Requests và BeautifulSoup), sau đó phân tích và lưu dữ liệu thành DataFrame của Pandas."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Junior Data Scientist",
                  "ETL Developer",
                  "Data Engineer (Entry-level)",
                  "Business Intelligence Developer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "Jupyter Notebook",
                  "VS Code",
                  "Pandas",
                  "NumPy"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "40-80 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds-etl-advanced-python-data-science",
                    "type": "unlocks",
                    "description": "Nắm vững Python cơ bản là tiền đề để học các kỹ thuật Python nâng cao cho khoa học dữ liệu và xây dựng pipeline phức tạp hơn."
                  },
                  {
                    "id": "ds-foundations-sql-basic",
                    "type": "complementary",
                    "description": "SQL thường được sử dụng cùng với Python để trích xuất và quản lý dữ liệu hiệu quả từ các hệ quản trị cơ sở dữ liệu."
                  },
                  {
                    "id": "ds-foundations-statistics-basics",
                    "type": "complementary",
                    "description": "Kiến thức thống kê cơ bản hỗ trợ việc hiểu và diễn giải dữ liệu khi xử lý và phân tích bằng Python."
                  }
                ]
              }
            ],
            "description": "Kỹ năng này trang bị cho bạn khả năng thiết kế, xây dựng và quản lý các quy trình trích xuất, chuyển đổi và tải dữ liệu (ETL). Bạn sẽ học cách tạo ra các pipelines dữ liệu tự động, đảm bảo dữ liệu được thu thập, xử lý và chuẩn bị hiệu quả để phục vụ cho các mục đích phân tích và báo cáo.",
            "learningResources": [
              "Khóa học 'Data Engineering with Python' trên các nền tảng như Coursera/Udemy, tập trung vào module ETL và thiết kế pipeline.",
              "Phần 'Data Ingestion & Transformation' từ sách 'The Fundamentals of Data Engineering' hoặc các tài liệu tương đương."
            ],
            "projectIdeas": [
              "Xây dựng một pipeline dữ liệu đơn giản: Thu thập dữ liệu từ tệp CSV/JSON (ví dụ: dữ liệu giao dịch công khai hoặc dữ liệu từ API) bằng Python, làm sạch và chuyển đổi (transform) dữ liệu theo các quy tắc nhất định, sau đó tải vào một cơ sở dữ liệu quan hệ (như PostgreSQL hoặc SQLite) và tạo các báo cáo/phân tích cơ bản bằng SQL."
            ],
            "tools": [
              "Python (với các thư viện như pandas cho xử lý dữ liệu, psycopg2 hoặc SQLAlchemy để tương tác DB, pyarrow cho các định dạng Parquet/Avro)",
              "SQL (PostgreSQL, MySQL, SQLite)",
              "Công cụ dòng lệnh hoặc IDE cho SQL (ví dụ: DBeaver, pgAdmin)",
              "Hệ thống điều phối pipeline cơ bản (ví dụ: Apache Airflow - để hiểu khái niệm)"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "40-60 giờ học và thực hành",
            "importanceScore": "Cao"
          }
        ],
        "abilityName": "Lập trình và Phát triển (Programming & Development)",
        "specialization": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
        "description": "Năng lực Lập trình và Phát triển là xương sống để biến các ý tưởng khoa học dữ liệu thành các giải pháp thực tế và có thể triển khai. Nó đòi hỏi khả năng viết mã hiệu quả, xây dựng các pipeline dữ liệu mạnh mẽ, và triển khai các mô hình Machine Learning có khả năng mở rộng. Năng lực này đảm bảo rằng các phân tích và mô hình có thể được vận hành trong môi trường sản xuất thực tế, hỗ trợ quá trình ra quyết định dựa trên dữ liệu.",
        "learningResources": [
          {
            "name": "IBM Data Science Professional Certificate",
            "type": "Khóa học trực tuyến (Coursera)",
            "url": "https://www.coursera.org/professional-certificates/ibm-data-science"
          },
          {
            "name": "Designing Data-Intensive Applications",
            "type": "Sách",
            "url": "https://dataintensive.net/"
          }
        ],
        "projectIdeas": [
          {
            "name": "Hệ thống phân tích và dự đoán dữ liệu thời gian thực cho E-commerce",
            "description": "Xây dựng một hệ thống end-to-end thu thập dữ liệu giao dịch và hành vi người dùng (streaming data) từ một trang web thương mại điện tử. Phát triển các pipeline ETL sử dụng Apache Spark để xử lý dữ liệu lớn, sau đó xây dựng và triển khai một mô hình Machine Learning (ví dụ: hệ thống khuyến nghị sản phẩm hoặc dự đoán churn) theo thời gian thực. Áp dụng các nguyên tắc MLOps để tự động hóa việc triển khai, giám sát và cập nhật mô hình."
          }
        ],
        "tools": [
          "Python",
          "R",
          "Apache Spark",
          "Hadoop Ecosystem (HDFS, YARN)",
          "TensorFlow/PyTorch",
          "Scikit-learn",
          "Docker",
          "Kubernetes",
          "MLflow",
          "Apache Airflow"
        ],
        "difficultyLevel": "Nâng cao",
        "estimatedTimeToComplete": "6-12 tháng thực hành liên tục",
        "importanceScore": "5/5"
      },
      {
        "id": "ability_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_2",
        "name": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
        "type": "ability",
        "children": [
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_2_1",
            "name": "Phân tích Hồi quy và Mô hình Dự đoán (Regression Analysis & Predictive Modeling)",
            "type": "skill",
            "children": [
              {
                "id": "regression-models-types-principles",
                "name": "Các Loại Mô hình Hồi quy (Ví dụ: Hồi quy Tuyến tính, Hồi quy Logistic) và Nguyên lý hoạt động cơ bản",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giới thiệu về các mô hình hồi quy cơ bản như Hồi quy Tuyến tính và Hồi quy Logistic, tập trung vào nguyên lý hoạt động và ứng dụng của chúng. Việc hiểu rõ các mô hình này là nền tảng quan trọng để thực hiện phân tích dự đoán và xây dựng các hệ thống học máy hiệu quả trong Khoa học Dữ liệu.",
                "keywords": [
                  "Hồi quy Tuyến tính",
                  "Hồi quy Logistic",
                  "Học có giám sát",
                  "Dự đoán",
                  "Phân loại",
                  "Hàm chi phí",
                  "Gradient Descent",
                  "Phân tích Hồi quy",
                  "Mô hình Dự đoán",
                  "Machine Learning"
                ],
                "learningResources": [
                  {
                    "name": "Machine Learning Course by Andrew Ng (Coursera)",
                    "url": "https://www.coursera.org/learn/machine-learning"
                  },
                  {
                    "name": "Scikit-learn documentation on Linear & Logistic Regression",
                    "url": "https://scikit-learn.org/stable/modules/linear_model.html"
                  },
                  {
                    "name": "StatQuest with Josh Starmer: Linear Regression & Logistic Regression Playlist",
                    "url": "https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGD5T_J_sE3bA_HjS10W1"
                  }
                ],
                "prerequisites": [
                  "introduction-to-machine-learning",
                  "basic-statistics-probability"
                ],
                "projectIdeas": [
                  "Xây dựng mô hình Hồi quy Tuyến tính để dự đoán giá nhà dựa trên các đặc trưng như diện tích, số phòng, vị trí từ một tập dữ liệu công khai.",
                  "Phát triển mô hình Hồi quy Logistic để dự đoán khả năng khách hàng rời bỏ dịch vụ (customer churn prediction) dựa trên hành vi sử dụng và thông tin cá nhân của họ."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "Quantitative Analyst",
                  "Statistician"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Scikit-learn, Pandas, NumPy)",
                  "R (glm, lm)",
                  "Jupyter Notebook",
                  "Google Colab"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Research Scientist",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetId": "introduction-to-machine-learning"
                  },
                  {
                    "type": "prerequisite",
                    "targetId": "basic-statistics-probability"
                  },
                  {
                    "type": "complementary",
                    "targetId": "model-evaluation-feature-selection"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "advanced-regression-models"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "classification-clustering-algorithms"
                  }
                ]
              },
              {
                "id": "DS-RA-005",
                "name": "Giả định của Mô hình Hồi quy và Phương pháp kiểm tra (Ví dụ: Giả định OLS)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc hiểu các giả định cơ bản mà các mô hình hồi quy tuyến tính, đặc biệt là Hồi quy Bình phương nhỏ nhất thông thường (OLS), dựa vào để cho ra các ước lượng không thiên lệch và hiệu quả. Việc kiểm tra và xử lý các vi phạm giả định này là cực kỳ quan trọng để đảm bảo tính hợp lệ của suy luận thống kê và độ tin cậy của mô hình dự đoán.",
                "keywords": [
                  "Giả định OLS",
                  "Gauss-Markov",
                  "Tính tuyến tính",
                  "Hiệp phương sai đồng nhất (Homoscedasticity)",
                  "Không đa cộng tuyến (No Multicollinearity)",
                  "Tính độc lập của sai số",
                  "Phân phối chuẩn của phần dư",
                  "Biểu đồ chẩn đoán (Diagnostic plots)",
                  "Kiểm định giả thuyết hồi quy",
                  "Suy luận thống kê"
                ],
                "learningResources": [
                  {
                    "title": "5 Assumptions of OLS Regression and How to Check Them",
                    "url": "https://towardsdatascience.com/5-assumptions-of-ols-regression-and-how-to-check-them-c923f1a0a544",
                    "type": "Article"
                  },
                  {
                    "title": "STAT 501: Regression Assumptions - Penn State",
                    "url": "https://online.stat.psu.edu/stat501/lesson/2/2.1",
                    "type": "Course Material"
                  },
                  {
                    "title": "Introduction to Statistical Learning with Applications in R (Chapter 3: Linear Regression)",
                    "url": "https://www.statlearning.com/",
                    "type": "Textbook Reference"
                  }
                ],
                "prerequisites": [
                  "DS-STAT-001",
                  "DS-RA-001",
                  "DS-STAT-003"
                ],
                "projectIdeas": [
                  "Chọn một bộ dữ liệu thực tế (ví dụ: dữ liệu giá nhà, dự đoán doanh thu), xây dựng mô hình hồi quy OLS và thực hiện kiểm tra đầy đủ các giả định. Báo cáo các phát hiện và đề xuất các hành động khắc phục nếu giả định bị vi phạm.",
                  "Tạo dữ liệu mô phỏng cố ý vi phạm một hoặc nhiều giả định OLS (ví dụ: đa cộng tuyến, phương sai không đồng nhất). So sánh kết quả ước lượng và suy luận thống kê của mô hình OLS trên dữ liệu này với dữ liệu không vi phạm."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Statistician",
                  "Quantitative Analyst",
                  "Machine Learning Engineer",
                  "Research Scientist",
                  "Econometrician"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (statsmodels, scikit-learn, pandas, matplotlib, seaborn)",
                  "R (lm, car, broom, ggplot2)",
                  "SAS",
                  "STATA"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "12-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  },
                  {
                    "path": "Business Analyst",
                    "score": 7
                  },
                  {
                    "path": "Research Scientist",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS-STAT-001",
                    "type": "prerequisite",
                    "description": "Kiến thức về thống kê mô tả, phân phối xác suất và các khái niệm thống kê cơ bản là nền tảng."
                  },
                  {
                    "id": "DS-RA-001",
                    "type": "prerequisite",
                    "description": "Hiểu rõ về mô hình hồi quy tuyến tính và cách ước lượng các hệ số là cần thiết."
                  },
                  {
                    "id": "DS-STAT-003",
                    "type": "prerequisite",
                    "description": "Khả năng thực hiện và diễn giải kiểm định giả thuyết là yếu tố cốt lõi để đánh giá các giả định."
                  },
                  {
                    "id": "DS-RA-006",
                    "type": "complementary",
                    "description": "Các kỹ thuật hồi quy mạnh mẽ (Robust Regression) giúp xử lý khi giả định bị vi phạm."
                  },
                  {
                    "id": "DS-RA-007",
                    "type": "unlocks",
                    "description": "Hiểu giả định là cơ sở để học các mô hình hồi quy nâng cao hơn như Hồi quy Tuyến tính Tổng quát (GLM) hoặc Hồi quy Chuỗi thời gian (Time Series Regression)."
                  }
                ]
              },
              {
                "id": "DS_PM_RegressionAndPredictionMetrics",
                "name": "Các Chỉ số Đánh giá Hiệu suất Mô hình Hồi quy và Dự đoán (Ví dụ: R-squared, MAE, RMSE, AUC-ROC)",
                "type": "knowledge",
                "children": [],
                "title": "Các Chỉ số Đánh giá Hiệu suất Mô hình Hồi quy và Dự đoán (Ví dụ: R-squared, MAE, RMSE, AUC-ROC)",
                "description": "Kiến thức này tập trung vào các chỉ số định lượng dùng để đánh giá hiệu suất của mô hình hồi quy và dự đoán. Việc hiểu và lựa chọn đúng các chỉ số như R-squared, MAE, RMSE (cho hồi quy) và AUC-ROC (cho phân loại nhị phân) là cực kỳ quan trọng để đánh giá khách quan, so sánh và cải thiện chất lượng của các mô hình học máy. Nó giúp đảm bảo mô hình không chỉ chính xác mà còn phù hợp với mục tiêu kinh doanh cụ thể.",
                "keywords": [
                  "R-squared",
                  "MAE",
                  "RMSE",
                  "MSE",
                  "MAPE",
                  "AUC-ROC",
                  "Model Evaluation",
                  "Performance Metrics",
                  "Regression Metrics",
                  "Classification Metrics",
                  "Predictive Modeling",
                  "Data Science"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn: Regression metrics documentation",
                    "url": "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"
                  },
                  {
                    "name": "Scikit-learn: ROC AUC documentation",
                    "url": "https://scikit-learn.org/stable/modules/model_evaluation.html#roc-auc"
                  },
                  {
                    "name": "Understanding Regression Model Evaluation Metrics",
                    "url": "https://www.vinsight.ai/blog/understanding-regression-model-evaluation-metrics"
                  }
                ],
                "prerequisites": [
                  "DS_BasicStatisticalConcepts",
                  "DS_IntroductionToRegressionModeling",
                  "DS_IntroductionToClassification"
                ],
                "projectIdeas": [
                  "Xây dựng và đánh giá hiệu suất của ít nhất hai mô hình hồi quy (ví dụ: Hồi quy Tuyến tính, Cây Quyết định) trên một tập dữ liệu công khai (ví dụ: California Housing), sử dụng R-squared, MAE và RMSE để so sánh. Trình bày kết quả và giải thích mô hình nào hoạt động tốt hơn dựa trên các chỉ số.",
                  "Phân tích tập dữ liệu phân loại nhị phân (ví dụ: Dự đoán bệnh tim, Phát hiện gian lận) bằng cách sử dụng một mô hình phân loại. Tính toán và giải thích ý nghĩa của chỉ số AUC-ROC, đồng thời so sánh nó với các chỉ số khác như Accuracy, Precision, Recall."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "Quantitative Analyst",
                  "Statistician",
                  "AI Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (scikit-learn, NumPy, Pandas)",
                  "R",
                  "Jupyter Notebooks",
                  "Google Colab"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "12-20 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Quantitative Analyst",
                    "score": 9
                  },
                  {
                    "path": "Research Scientist",
                    "score": 9
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "DS_BasicStatisticalConcepts"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS_IntroductionToRegressionModeling"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS_IntroductionToClassification"
                  },
                  {
                    "type": "complementary",
                    "id": "DS_ModelTuningAndOptimization"
                  },
                  {
                    "type": "complementary",
                    "id": "DS_CrossValidationAndResampling"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS_AdvancedModelComparisonTechniques"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS_ModelDeploymentAndMonitoring"
                  }
                ]
              },
              {
                "id": "DS_ML_007",
                "name": "Khái niệm về Overfitting, Underfitting, và các Kỹ thuật Điều chỉnh (Ví dụ: Regularization, Cross-validation)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giải thích các vấn đề phổ biến Overfitting (mô hình quá khớp) và Underfitting (mô hình chưa khớp) trong học máy, những tình trạng làm giảm hiệu suất tổng quát của mô hình trên dữ liệu mới. Nó cung cấp các kỹ thuật điều chỉnh thiết yếu như Regularization (chính quy hóa) và Cross-validation (kiểm định chéo) để cải thiện khả năng tổng quát hóa, đảm bảo mô hình hoạt động hiệu quả trong thực tế và đưa ra dự đoán đáng tin cậy.",
                "keywords": [
                  "Overfitting",
                  "Underfitting",
                  "Regularization",
                  "Cross-validation",
                  "Bias-Variance Trade-off",
                  "L1 Regularization",
                  "L2 Regularization",
                  "Model Generalization",
                  "Hyperparameter Tuning"
                ],
                "learningResources": [
                  {
                    "name": "Cross-validation: Evaluating estimator performance - scikit-learn",
                    "url": "https://scikit-learn.org/stable/modules/cross_validation.html"
                  },
                  {
                    "name": "1.1.4. Regularized Linear Models - scikit-learn",
                    "url": "https://scikit-learn.org/stable/modules/linear_model.html#regularized-linear-models"
                  },
                  {
                    "name": "Overfitting and Underfitting In Machine Learning: What They Are And How To Avoid Them",
                    "url": "https://towardsdatascience.com/overfitting-and-underfitting-in-machine-learning-what-they-are-and-how-to-avoid-them-7c15e45a20b7"
                  }
                ],
                "prerequisites": [
                  "ML_001",
                  "ML_003",
                  "ML_005"
                ],
                "projectIdeas": [
                  "Xây dựng mô hình hồi quy để dự đoán giá nhà (ví dụ: dataset Boston Housing), so sánh hiệu suất khi áp dụng và không áp dụng các kỹ thuật Regularization (Lasso/Ridge) và phân tích ảnh hưởng của tham số alpha.",
                  "Sử dụng Cross-validation để đánh giá độ bền của các mô hình phân loại khác nhau (ví dụ: Logistic Regression, SVM) trên một tập dữ liệu y tế (ví dụ: phát hiện bệnh tim mạch), phân tích tác động của các chiến lược phân tách dữ liệu khác nhau (ví dụ: K-Fold, Stratified K-Fold)."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Research Scientist",
                  "Quantitative Analyst"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Scikit-learn",
                  "Pandas",
                  "NumPy",
                  "TensorFlow",
                  "PyTorch"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Research Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ML_001"
                  },
                  {
                    "type": "prerequisite",
                    "id": "ML_003"
                  },
                  {
                    "type": "prerequisite",
                    "id": "ML_005"
                  },
                  {
                    "type": "complementary",
                    "id": "DS_ML_008"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS_ML_009"
                  }
                ]
              }
            ],
            "skillName": "Phân tích Hồi quy và Mô hình Dự đoán (Regression Analysis & Predictive Modeling)",
            "parentCapability": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
            "description": "Kỹ năng này trang bị cho bạn khả năng xây dựng và diễn giải các mô hình hồi quy để phân tích mối quan hệ giữa các biến và đưa ra dự đoán. Bạn sẽ học cách lựa chọn mô hình phù hợp, đánh giá hiệu suất của chúng và áp dụng các kỹ thuật để đảm bảo mô hình ổn định và tổng quát, từ đó hỗ trợ quyết định dựa trên dữ liệu.",
            "learningResources": [
              {
                "title": "Chương 'Linear Regression' và 'Logistic Regression' trong sách 'An Introduction to Statistical Learning'",
                "type": "Sách (PDF/Video Lectures)",
                "url": "https://www.statlearning.com/"
              },
              {
                "title": "Playlist về 'Regression Analysis' và 'Machine Learning Fundamentals' trên kênh YouTube của Krish Naik hoặc freeCodeCamp.org",
                "type": "Playlist Video",
                "url": "https://www.youtube.com/c/KrishNaik"
              }
            ],
            "projectIdeas": [
              {
                "title": "Dự đoán giá nhà và phân tích yếu tố ảnh hưởng",
                "description": "Sử dụng bộ dữ liệu giá nhà (ví dụ: Ames Housing Dataset hoặc Boston Housing Dataset) để xây dựng các mô hình hồi quy (tuyến tính, logistic nếu có biến mục tiêu phân loại hoặc mô hình phức tạp hơn) nhằm dự đoán giá trị hoặc xác suất một ngôi nhà được bán với giá cao. Phân tích các giả định của mô hình, đánh giá hiệu suất bằng các chỉ số như R-squared, MAE, RMSE và áp dụng các kỹ thuật điều chỉnh như Regularization (Lasso, Ridge) hoặc Cross-validation để cải thiện độ chính xác và tính tổng quát của mô hình.",
                "deliverables": [
                  "Mô hình hồi quy đã được huấn luyện và đánh giá.",
                  "Báo cáo phân tích về các yếu tố ảnh hưởng đến giá nhà.",
                  "Trực quan hóa kết quả và hiệu suất mô hình."
                ]
              }
            ],
            "tools": [
              "Python (scikit-learn, pandas, numpy, statsmodels, matplotlib, seaborn)",
              "R (lm, glm, caret, dplyr, ggplot2)",
              "Jupyter Notebook / RStudio"
            ],
            "difficultyLevel": "Trung bình đến Khó",
            "estimatedTimeToComplete": "30-40 giờ",
            "importanceScore": 9
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_2_2",
            "name": "Phát triển và Đánh giá Mô hình Học máy (Machine Learning Model Development & Evaluation)",
            "type": "skill",
            "children": [
              {
                "id": "skill.ml_dev_eval.ml_model_types_principles",
                "name": "Các Loại Mô hình Học máy và Nguyên lý Hoạt động cơ bản",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này giới thiệu về các loại mô hình học máy chính (Học có giám sát, Học không giám sát, Học tăng cường) và đi sâu vào nguyên lý hoạt động cơ bản của một số thuật toán phổ biến trong từng loại. Hiểu rõ kiến thức này là nền tảng để lựa chọn, áp dụng và diễn giải các mô hình học máy một cách hiệu quả trong các bài toán thực tế.",
                "keywords": [
                  "Học máy",
                  "Supervised Learning",
                  "Unsupervised Learning",
                  "Reinforcement Learning",
                  "Hồi quy",
                  "Phân loại",
                  "Phân cụm",
                  "Thuật toán ML",
                  "Nguyên lý hoạt động",
                  "Machine Learning Models"
                ],
                "learningResources": [
                  {
                    "name": "Machine Learning by Andrew Ng (Coursera)",
                    "url": "https://www.coursera.org/learn/machine-learning"
                  },
                  {
                    "name": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Book by Aurélien Géron)",
                    "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
                  },
                  {
                    "name": "Scikit-learn User Guide (Official Documentation)",
                    "url": "https://scikit-learn.org/stable/user_guide.html"
                  }
                ],
                "prerequisites": [
                  "skill.ml_dev_eval.intro_to_ml",
                  "skill.data_science_foundations.python_for_ds",
                  "skill.data_science_foundations.linear_algebra_basics",
                  "skill.data_science_foundations.stats_probability_basics"
                ],
                "projectIdeas": [
                  "Xây dựng và so sánh hiệu suất của ít nhất hai mô hình phân loại (ví dụ: Hồi quy Logistic, Cây quyết định) trên tập dữ liệu Iris hoặc Titanic.",
                  "Thực hiện thuật toán K-Means để phân cụm khách hàng dựa trên dữ liệu giao dịch giả định và diễn giải các cụm đã tìm thấy."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Data Analyst",
                  "Research Scientist (ML/AI)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "scikit-learn",
                  "Jupyter Notebook",
                  "Pandas",
                  "NumPy"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "15-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Research Scientist (ML/AI)",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetId": "skill.ml_dev_eval.intro_to_ml"
                  },
                  {
                    "type": "complementary",
                    "targetId": "skill.ml_dev_eval.data_preprocessing_feature_engineering"
                  },
                  {
                    "type": "complementary",
                    "targetId": "skill.ml_dev_eval.model_evaluation_metrics"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "skill.ml_dev_eval.advanced_ml_algorithms"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "skill.ml_dev_eval.deep_learning_intro"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "skill.ml_dev_eval.model_optimization_tuning"
                  }
                ]
              },
              {
                "id": "ds-ml-data-preprocessing-feature-engineering",
                "name": "Tiền xử lý Dữ liệu và Kỹ thuật Trích chọn Đặc trưng",
                "type": "knowledge",
                "children": [],
                "title": "Tiền xử lý Dữ liệu và Kỹ thuật Trích chọn Đặc trưng",
                "description": "Đơn vị kiến thức này tập trung vào các bước quan trọng để làm sạch, biến đổi và chuẩn bị dữ liệu thô cho các mô hình học máy, đồng thời tạo ra các đặc trưng mới, giàu thông tin hơn. Nắm vững các kỹ thuật này là tối quan trọng vì chất lượng và mức độ phù hợp của dữ liệu đầu vào quyết định trực tiếp hiệu suất, độ tin cậy và khả năng diễn giải của bất kỳ mô hình học máy nào.",
                "keywords": [
                  "Xử lý dữ liệu thiếu",
                  "Phát hiện ngoại lai",
                  "Chuẩn hóa dữ liệu",
                  "Mã hóa biến danh mục",
                  "Giảm chiều dữ liệu",
                  "Trích chọn đặc trưng",
                  "Kỹ thuật trích chọn đặc trưng",
                  "Biến đổi dữ liệu",
                  "PCA",
                  "TF-IDF"
                ],
                "learningResources": [
                  {
                    "name": "Feature Engineering for Machine Learning Course (Coursera by Google Cloud)",
                    "url": "https://www.coursera.org/learn/feature-engineering-for-machine-learning"
                  },
                  {
                    "name": "Scikit-learn Preprocessing and Feature Engineering Documentation",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html"
                  },
                  {
                    "name": "Towards Data Science: A Guide to Feature Engineering",
                    "url": "https://towardsdatascience.com/a-guide-to-feature-engineering-8c9e1262d63"
                  }
                ],
                "prerequisites": [
                  "ds-data-manipulation-pandas-numpy",
                  "ds-exploratory-data-analysis"
                ],
                "projectIdeas": [
                  {
                    "title": "Dự án Tiền xử lý và Trích chọn Đặc trưng cho Dữ liệu Giá Nhà",
                    "description": "Áp dụng các kỹ thuật tiền xử lý (xử lý thiếu giá trị, ngoại lai, mã hóa danh mục) và trích chọn đặc trưng (tạo biến tương tác, biến tổng hợp từ thời gian/địa lý) trên bộ dữ liệu giá nhà (ví dụ: Kaggle House Prices) để cải thiện hiệu suất dự đoán của mô hình hồi quy."
                  },
                  {
                    "title": "Dự án Tiền xử lý Dữ liệu Văn bản cho Phân tích Cảm xúc",
                    "description": "Thu thập một bộ dữ liệu bình luận/tweet, thực hiện các bước tiền xử lý văn bản (xóa nhiễu, chuẩn hóa, stemming/lemmatization) và áp dụng các kỹ thuật trích chọn đặc trưng (TF-IDF, Word Embeddings) để chuẩn bị dữ liệu cho mô hình phân loại cảm xúc."
                  }
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Applied Scientist",
                  "AI Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Pandas",
                  "NumPy",
                  "Scikit-learn",
                  "Matplotlib",
                  "Seaborn",
                  "Apache Spark (cho dữ liệu lớn)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-50 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Research Scientist",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds-ml-model-training-evaluation",
                    "type": "unlocks"
                  },
                  {
                    "id": "ds-exploratory-data-analysis",
                    "type": "complementary"
                  },
                  {
                    "id": "ds-data-quality-management",
                    "type": "complementary"
                  }
                ]
              },
              {
                "id": "ds-ml-eval-metrics-validation",
                "name": "Các Chỉ số Đánh giá Mô hình và Phương pháp Thẩm định",
                "type": "knowledge",
                "children": [],
                "title": "Các Chỉ số Đánh giá Mô hình và Phương pháp Thẩm định",
                "description": "Đơn vị kiến thức này tập trung vào việc trang bị cho người học những chỉ số đánh giá quan trọng (như Accuracy, Precision, Recall, F1-score, RMSE, AUC) để định lượng hiệu suất của các mô hình học máy. Đồng thời, nó giới thiệu các phương pháp thẩm định mô hình mạnh mẽ (ví dụ: Cross-validation, Holdout validation) nhằm đánh giá độ tin cậy và khả năng tổng quát hóa của mô hình trên dữ liệu mới. Nắm vững các khái niệm này là nền tảng để phát triển các giải pháp ML hiệu quả, công bằng và có khả năng triển khai thực tế, đảm bảo mô hình hoạt động đúng kỳ vọng.",
                "keywords": [
                  "Accuracy",
                  "Precision",
                  "Recall",
                  "F1-score",
                  "ROC AUC",
                  "RMSE",
                  "MAE",
                  "Cross-validation",
                  "K-fold Cross-validation",
                  "Holdout Validation",
                  "Confusion Matrix",
                  "Bias-Variance Tradeoff"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn: Model evaluation and scoring",
                    "url": "https://scikit-learn.org/stable/modules/model_evaluation.html"
                  },
                  {
                    "name": "Google Developers: Machine Learning Crash Course - Classification Evaluation",
                    "url": "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
                  },
                  {
                    "name": "Machine Learning Mastery: K-Fold Cross-Validation for Machine Learning",
                    "url": "https://machinelearningmastery.com/k-fold-cross-validation/"
                  }
                ],
                "prerequisites": [
                  "ds-ml-fundamentals",
                  "ds-data-preprocessing",
                  "ds-stats-descriptive",
                  "ds-ml-model-training"
                ],
                "projectIdeas": [
                  "So sánh các chỉ số đánh giá (Accuracy, Precision, Recall, F1, ROC AUC) cho ít nhất hai mô hình phân loại (ví dụ: Logistic Regression, SVM, Random Forest) trên một tập dữ liệu công khai (ví dụ: Titanic, Iris).",
                  "Thực hiện kỹ thuật K-fold Cross-Validation trên một mô hình hồi quy (ví dụ: Linear Regression, Decision Tree Regressor) và phân tích sự thay đổi của chỉ số RMSE qua các folds."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Research Scientist (Machine Learning)",
                  "Applied Scientist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Scikit-learn",
                  "Pandas",
                  "NumPy",
                  "Matplotlib",
                  "Seaborn",
                  "TensorFlow/Keras (cho các metric tùy chỉnh)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-25 hours (lý thuyết & thực hành)",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Research Scientist (ML/AI)",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst (Advanced)",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds-ml-model-training",
                    "type": "prerequisite",
                    "description": "Cần hiểu cách huấn luyện mô hình để có thể đánh giá chúng."
                  },
                  {
                    "id": "ds-ml-hyperparameter-tuning",
                    "type": "complementary",
                    "description": "Việc tinh chỉnh siêu tham số dựa trên các chỉ số đánh giá và phương pháp thẩm định để tìm ra mô hình tối ưu."
                  },
                  {
                    "id": "ds-ml-model-monitoring",
                    "type": "unlocks",
                    "description": "Kiến thức về các chỉ số đánh giá là nền tảng để giám sát hiệu suất mô hình đã triển khai."
                  },
                  {
                    "id": "ds-ml-bias-fairness",
                    "type": "unlocks",
                    "description": "Việc đánh giá tính công bằng của mô hình yêu cầu hiểu biết sâu sắc về các chỉ số đánh giá, đặc biệt là theo các phân khúc dữ liệu khác nhau."
                  }
                ]
              },
              {
                "id": "ML_OUTF_UNDF_BIASVAR_TRADEOFF",
                "name": "Hiện tượng Overfitting, Underfitting và Phân tích Sai lệch-Phương sai (Bias-Variance Tradeoff)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giải thích các vấn đề phổ biến trong huấn luyện mô hình học máy: overfitting (mô hình quá phức tạp, học thuộc dữ liệu huấn luyện) và underfitting (mô hình quá đơn giản, không thể nắm bắt được quy luật). Nắm vững Bias-Variance Tradeoff là yếu tố cốt lõi để tối ưu hóa hiệu suất mô hình, đảm bảo khả năng tổng quát hóa tốt trên dữ liệu mới và cân bằng giữa độ phức tạp của mô hình với khả năng phù hợp trên các tập dữ liệu khác nhau.",
                "keywords": [
                  "Overfitting",
                  "Underfitting",
                  "Bias-Variance Tradeoff",
                  "Generalization Error",
                  "Model Complexity",
                  "High Bias",
                  "High Variance",
                  "Model Evaluation",
                  "Cross-validation",
                  "Regularization"
                ],
                "learningResources": [
                  {
                    "title": "Understanding the Bias-Variance Tradeoff",
                    "url": "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-11a2f42f7035",
                    "type": "Article"
                  },
                  {
                    "title": "Underfitting vs. Overfitting Example (Scikit-learn)",
                    "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html",
                    "type": "Code Example/Tutorial"
                  },
                  {
                    "title": "The Peril of Overfitting (Google Machine Learning Crash Course)",
                    "url": "https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting",
                    "type": "Online Lesson"
                  }
                ],
                "prerequisites": [
                  "ML_BASICS_FUNDAMENTALS",
                  "STAT_PROB_BASICS",
                  "ML_MODEL_EVALUATION_METRICS"
                ],
                "projectIdeas": [
                  "Xây dựng một mô hình hồi quy đa thức trên tập dữ liệu tổng hợp và điều chỉnh bậc đa thức để minh họa rõ ràng các trường hợp overfitting, underfitting và sự đánh đổi bias-variance. Sau đó, trực quan hóa kết quả.",
                  "Sử dụng kỹ thuật kiểm định chéo (cross-validation) để đánh giá hiệu suất của ít nhất hai mô hình học máy khác nhau (ví dụ: Cây quyết định, Hồi quy Logistic) trên một tập dữ liệu thực. Phân tích các chỉ số hiệu suất để xác định và thảo luận về các dấu hiệu của overfitting hoặc underfitting."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Research Scientist (ML)",
                  "ML Ops Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "scikit-learn",
                  "NumPy",
                  "Matplotlib",
                  "Seaborn"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI/ML Researcher",
                    "score": 9
                  },
                  {
                    "path": "ML Ops Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ML_BASICS_FUNDAMENTALS"
                  },
                  {
                    "type": "prerequisite",
                    "id": "STAT_PROB_BASICS"
                  },
                  {
                    "type": "prerequisite",
                    "id": "ML_MODEL_EVALUATION_METRICS"
                  },
                  {
                    "type": "complementary",
                    "id": "ML_CROSS_VALIDATION"
                  },
                  {
                    "type": "unlocks",
                    "id": "ML_REGULARIZATION_TECHNIQUES"
                  },
                  {
                    "type": "unlocks",
                    "id": "ML_HYPERPARAMETER_TUNING"
                  }
                ]
              }
            ],
            "skillName": "Phát triển và Đánh giá Mô hình Học máy (Machine Learning Model Development & Evaluation)",
            "competency": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
            "description": "Kỹ năng này trang bị cho người học khả năng xây dựng, huấn luyện và đánh giá một cách toàn diện các mô hình học máy cho nhiều tác vụ khác nhau. Nó bao gồm toàn bộ quy trình từ chuẩn bị dữ liệu, lựa chọn mô hình phù hợp đến điều chỉnh siêu tham số và đánh giá hiệu suất, đảm bảo giải pháp đưa ra là mạnh mẽ và có khả năng tổng quát hóa tốt.",
            "learningResources": [
              {
                "title": "Chương 2: Dự án Học máy Từ đầu đến cuối (End-to-End Machine Learning Project)",
                "type": "Chương sách",
                "source": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition) - Aurélien Géron"
              },
              {
                "title": "Khóa học Machine Learning",
                "type": "Khóa học trực tuyến",
                "source": "Coursera by Andrew Ng"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng và Đánh giá Mô hình Dự đoán Giá nhà",
                "description": "Sử dụng một tập dữ liệu về giá nhà, thực hiện tiền xử lý dữ liệu, lựa chọn và huấn luyện các mô hình học máy khác nhau (ví dụ: Hồi quy Tuyến tính, Cây Quyết định, Rừng Ngẫu nhiên). Sau đó, đánh giá hiệu suất của chúng bằng các chỉ số phù hợp (RMSE, R-squared) và phân tích các hiện tượng như overfitting/underfitting thông qua kỹ thuật thẩm định chéo."
              }
            ],
            "tools": [
              "Python",
              "Scikit-learn",
              "Pandas",
              "NumPy",
              "Matplotlib",
              "Seaborn"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "40-60 giờ",
            "importanceScore": 9
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_2_3",
            "name": "Xử lý Dữ liệu Lớn và Kỹ thuật Đặc trưng (Big Data Processing & Feature Engineering)",
            "type": "skill",
            "children": [
              {
                "id": "data-science-big-data-processing-feature-engineering-big-data-architectures-platforms",
                "name": "Kiến trúc và Nền tảng Xử lý Dữ liệu Lớn",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào việc thiết kế, xây dựng và quản lý các hệ thống xử lý dữ liệu lớn. Nó bao gồm việc hiểu các mô hình kiến trúc phân tán, các nền tảng công nghệ như Hadoop và Spark, cũng như các dịch vụ đám mây chuyên biệt để xử lý dữ liệu quy mô petabyte. Nắm vững kiến thức này là thiết yếu để tạo ra các giải pháp dữ liệu có khả năng mở rộng, hiệu quả và đáng tin cậy.",
                "keywords": [
                  "Big Data Architecture",
                  "Distributed Systems",
                  "Hadoop Ecosystem",
                  "Apache Spark",
                  "Apache Kafka",
                  "Data Pipelines",
                  "Cloud Data Platforms",
                  "Data Lake",
                  "Data Warehousing",
                  "Stream Processing",
                  "Batch Processing",
                  "ETL/ELT"
                ],
                "learningResources": [
                  {
                    "name": "Designing Data-Intensive Applications by Martin Kleppmann",
                    "type": "Book",
                    "url": "https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/"
                  },
                  {
                    "name": "Architecting with Google Cloud Compute Engine (Coursera Specialization)",
                    "type": "Online Course",
                    "url": "https://www.coursera.org/specializations/gcp-architecture"
                  },
                  {
                    "name": "Apache Spark Documentation",
                    "type": "Official Documentation",
                    "url": "https://spark.apache.org/docs/latest/"
                  }
                ],
                "prerequisites": [
                  "data-science-core-programming-python",
                  "data-science-big-data-core-sql-databases",
                  "data-science-big-data-core-distributed-systems-concepts"
                ],
                "projectIdeas": [
                  "Xây dựng một pipeline xử lý dữ liệu batch đơn giản bằng Apache Spark để phân tích dữ liệu log từ web server và lưu kết quả vào một Data Lake (ví dụ: MinIO hoặc HDFS cục bộ).",
                  "Thiết lập một cụm Hadoop nhỏ (sử dụng Docker hoặc môi trường đám mây miễn phí) và triển khai một ứng dụng MapReduce/Spark cơ bản để đếm tần suất từ khóa trong một tập dữ liệu văn bản lớn."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Architect",
                  "Cloud Data Engineer",
                  "Solutions Architect (Data)",
                  "Big Data Developer",
                  "Machine Learning Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Hadoop",
                  "Apache Spark",
                  "Apache Kafka",
                  "Apache Flink",
                  "Apache Cassandra",
                  "Apache Hive",
                  "AWS EMR",
                  "AWS Glue",
                  "Azure Databricks",
                  "Google Cloud Dataflow",
                  "Google Cloud Dataproc",
                  "Apache Airflow"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 weeks",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architect",
                    "score": 10
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data-science-core-programming-python",
                    "type": "prerequisite",
                    "description": "Kỹ năng lập trình cơ bản, đặc biệt là Python hoặc Java/Scala, là cần thiết để tương tác và phát triển các ứng dụng trên các nền tảng dữ liệu lớn."
                  },
                  {
                    "id": "data-science-big-data-core-sql-databases",
                    "type": "prerequisite",
                    "description": "Hiểu biết về SQL và cơ sở dữ liệu giúp trong việc quản lý dữ liệu, thiết kế schema và tích hợp với các hệ thống dữ liệu truyền thống, cũng như sử dụng các công cụ query dữ liệu lớn như Hive."
                  },
                  {
                    "id": "data-science-big-data-core-distributed-systems-concepts",
                    "type": "prerequisite",
                    "description": "Nắm vững các khái niệm về hệ thống phân tán (ví dụ: đồng bộ, bất đồng bộ, tính nhất quán, khả năng chịu lỗi) là nền tảng để hiểu cách thức hoạt động và thiết kế của các kiến trúc dữ liệu lớn."
                  },
                  {
                    "id": "data-science-big-data-processing-feature-engineering-data-pipeline-design",
                    "type": "unlocks",
                    "description": "Kiến thức về kiến trúc và nền tảng là cơ sở để thiết kế, xây dựng và tối ưu hóa các đường ống dữ liệu (data pipelines) phức tạp và hiệu quả để di chuyển và biến đổi dữ liệu."
                  },
                  {
                    "id": "data-science-big-data-processing-feature-engineering-stream-processing",
                    "type": "complementary",
                    "description": "Xử lý dữ liệu luồng (stream processing) là một phần quan trọng của kiến trúc dữ liệu lớn hiện đại, thường sử dụng các nền tảng tương tự (Kafka, Flink, Spark Streaming) hoặc tích hợp với chúng."
                  },
                  {
                    "id": "data-science-big-data-processing-feature-engineering-cloud-data-solutions",
                    "type": "complementary",
                    "description": "Nhiều kiến trúc dữ liệu lớn được triển khai và quản lý trên nền tảng đám mây (AWS, Azure, GCP), do đó kiến thức về các giải pháp và dịch vụ dữ liệu đám mây là bổ sung quan trọng."
                  }
                ]
              },
              {
                "id": "data_preprocessing_cleaning",
                "name": "Kỹ thuật Tiền xử lý và Làm sạch Dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào các phương pháp biến đổi dữ liệu thô thành định dạng sạch sẽ, nhất quán và sẵn sàng cho việc phân tích hoặc xây dựng mô hình. Đây là bước cực kỳ quan trọng vì chất lượng dữ liệu đầu vào ảnh hưởng trực tiếp đến độ chính xác và độ tin cậy của mọi kết quả phân tích hoặc dự đoán, đặc biệt trong Khoa học Dữ liệu và Học máy.",
                "keywords": [
                  "Data Cleaning",
                  "Missing Data Imputation",
                  "Outlier Detection",
                  "Data Normalization",
                  "Data Standardization",
                  "Feature Scaling",
                  "Data Transformation",
                  "Noise Reduction",
                  "Data Consistency",
                  "Data Quality"
                ],
                "learningResources": [
                  {
                    "name": "Data Cleaning - Kaggle Learn",
                    "url": "https://www.kaggle.com/learn/data-cleaning"
                  },
                  {
                    "name": "Data Analysis with Python - IBM (Coursera)",
                    "url": "https://www.coursera.org/learn/data-analysis-python"
                  },
                  {
                    "name": "Scikit-learn Preprocessing Documentation",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html"
                  }
                ],
                "prerequisites": [
                  "python_programming_fundamentals",
                  "statistical_thinking_for_data_science",
                  "exploratory_data_analysis_eda"
                ],
                "projectIdeas": [
                  "Thực hiện làm sạch và tiền xử lý một tập dữ liệu thực tế (ví dụ: tập dữ liệu Titanic, bất động sản hoặc ngân hàng) bao gồm xử lý dữ liệu thiếu, ngoại lai và chuyển đổi biến cho mục đích phân tích.",
                  "So sánh hiệu quả của các kỹ thuật tiền xử lý khác nhau (ví dụ: chuẩn hóa Min-Max vs. tiêu chuẩn hóa Z-score, các phương pháp điền dữ liệu thiếu) trên hiệu suất của một mô hình học máy đơn giản."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Machine Learning Engineer",
                  "Data Engineer",
                  "Business Intelligence Analyst"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas, NumPy, Scikit-learn)",
                  "R (dplyr, tidyr)",
                  "SQL",
                  "Apache Spark"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "25-40 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Science",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineering",
                    "score": 9
                  },
                  {
                    "path": "Data Engineering",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "targetSkillId": "exploratory_data_analysis_eda",
                    "type": "complementary"
                  },
                  {
                    "targetSkillId": "feature_engineering_techniques",
                    "type": "unlocks"
                  },
                  {
                    "targetSkillId": "machine_learning_model_building",
                    "type": "unlocks"
                  },
                  {
                    "targetSkillId": "big_data_processing_frameworks",
                    "type": "complementary"
                  }
                ]
              },
              {
                "id": "feature_engineering_methods",
                "name": "Phương pháp Kỹ thuật Đặc trưng",
                "type": "knowledge",
                "children": [],
                "description": "Kỹ thuật đặc trưng (Feature Engineering) là quá trình sử dụng kiến thức miền để tạo ra các biến mới hoặc biến đổi các biến hiện có từ dữ liệu thô, nhằm giúp thuật toán học máy hoạt động tốt hơn. Nó đóng vai trò then chốt trong việc nâng cao hiệu suất, độ chính xác và khả năng giải thích của mô hình, biến dữ liệu thành một định dạng phù hợp hơn cho việc học.",
                "keywords": [
                  "Kỹ thuật đặc trưng",
                  "Biến đổi đặc trưng",
                  "Mã hóa đặc trưng",
                  "Giảm chiều dữ liệu",
                  "Chọn lọc đặc trưng",
                  "Xử lý thiếu dữ liệu",
                  "Xử lý ngoại lai",
                  "Tạo đặc trưng mới",
                  "Feature scaling",
                  "One-hot encoding"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn User Guide - Preprocessing data",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html"
                  },
                  {
                    "name": "Coursera: Feature Engineering for Machine Learning (Google Cloud)",
                    "url": "https://www.coursera.org/learn/feature-engineering-for-machine-learning"
                  },
                  {
                    "name": "Kaggle Learn - Feature Engineering",
                    "url": "https://www.kaggle.com/learn/feature-engineering"
                  }
                ],
                "prerequisites": [
                  "data_manipulation_pandas",
                  "statistical_fundamentals",
                  "machine_learning_fundamentals",
                  "data_types_and_structures"
                ],
                "projectIdeas": [
                  "Áp dụng các phương pháp kỹ thuật đặc trưng khác nhau (như scaling, encoding, polynomial features) lên bộ dữ liệu Titanic và so sánh tác động của chúng đến hiệu suất của mô hình phân loại (ví dụ: Logistic Regression, Random Forest).",
                  "Xây dựng một mô hình dự đoán giá nhà sử dụng bộ dữ liệu Ames Housing, tập trung vào việc tạo ra các đặc trưng mới từ các cột hiện có (ví dụ: tổng diện tích, tuổi của ngôi nhà, tỷ lệ phòng tắm/phòng ngủ) và tối ưu hóa các đặc trưng cho mô hình."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Data Analyst",
                  "Big Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Scikit-learn",
                  "Pandas",
                  "NumPy",
                  "TensorFlow",
                  "Keras",
                  "Matplotlib",
                  "Seaborn"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetSkillId": "data_manipulation_pandas",
                    "description": "Cần thành thạo xử lý dữ liệu để thực hiện kỹ thuật đặc trưng."
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "machine_learning_fundamentals",
                    "description": "Hiểu biết về ML giúp định hướng kỹ thuật đặc trưng phù hợp cho mô hình."
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "model_training_evaluation",
                    "description": "Kỹ thuật đặc trưng cải thiện đáng kể quá trình huấn luyện và kết quả đánh giá mô hình."
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "advanced_ml_techniques",
                    "description": "Kỹ thuật đặc trưng hiệu quả là nền tảng để áp dụng và tối ưu các kỹ thuật học máy nâng cao."
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "exploratory_data_analysis",
                    "description": "EDA cung cấp cái nhìn sâu sắc cần thiết để thiết kế các đặc trưng hiệu quả."
                  }
                ]
              },
              {
                "id": "DS.BD.FE.DimRed",
                "name": "Kỹ thuật Giảm chiều dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Kỹ thuật giảm chiều dữ liệu là tập hợp các phương pháp biến đổi dữ liệu từ không gian có số chiều cao sang không gian có số chiều thấp hơn trong khi vẫn giữ lại các thông tin quan trọng nhất. Điều này giúp giảm thiểu độ phức tạp tính toán, chống lại hiện tượng overfitting, cải thiện hiệu suất của các mô hình học máy và tăng cường khả năng trực quan hóa dữ liệu, đặc biệt quan trọng khi làm việc với các tập dữ liệu lớn và phức tạp.",
                "keywords": [
                  "Principal Component Analysis (PCA)",
                  "Linear Discriminant Analysis (LDA)",
                  "t-Distributed Stochastic Neighbor Embedding (t-SNE)",
                  "Uniform Manifold Approximation and Projection (UMAP)",
                  "Feature Extraction",
                  "Curse of Dimensionality",
                  "Manifold Learning",
                  "Singular Value Decomposition (SVD)",
                  "Autoencoders",
                  "Feature Compression"
                ],
                "learningResources": [
                  {
                    "name": "Dimensionality Reduction - scikit-learn Documentation",
                    "url": "https://scikit-learn.org/stable/modules/decomposition.html"
                  },
                  {
                    "name": "Machine Learning by Andrew Ng (Coursera) - Week 8: Dimensionality Reduction",
                    "url": "https://www.coursera.org/learn/machine-learning"
                  },
                  {
                    "name": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Chapter 8: Dimensionality Reduction)",
                    "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
                  }
                ],
                "prerequisites": [
                  "MATH.LinAlg",
                  "MATH.Stats",
                  "DS.ML.Supervised",
                  "DS.ML.Unsupervised",
                  "PROG.Python.DataManipulation"
                ],
                "projectIdeas": [
                  "Sử dụng PCA để giảm số chiều của tập dữ liệu ảnh MNIST hoặc Fashion MNIST, sau đó huấn luyện một mô hình phân loại (ví dụ: SVM hoặc Random Forest) và so sánh hiệu suất, tốc độ huấn luyện với việc sử dụng dữ liệu gốc.",
                  "Áp dụng t-SNE hoặc UMAP để trực quan hóa một tập dữ liệu y tế (ví dụ: dữ liệu gen, protein) hoặc tài chính (ví dụ: giao dịch ngân hàng), phân tích và diễn giải các cụm dữ liệu tự nhiên được hình thành."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Research Scientist (AI/ML)",
                  "Big Data Engineer (with ML focus)"
                ],
                "marketDemand": "High",
                "tools": [
                  "scikit-learn",
                  "NumPy",
                  "SciPy",
                  "Matplotlib",
                  "Seaborn",
                  "UMAP-learn",
                  "TensorFlow (for Autoencoders)",
                  "PyTorch (for Autoencoders)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-40 giờ (lý thuyết và thực hành cơ bản)",
                "importanceScore": 8,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "AI Research Scientist",
                    "score": 8
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "MATH.LinAlg"
                  },
                  {
                    "type": "prerequisite",
                    "id": "MATH.Stats"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS.ML.Supervised"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS.ML.Unsupervised"
                  },
                  {
                    "type": "complementary",
                    "id": "DS.BD.FE.FeatureSelection"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS.ML.DeepLearning.Autoencoders"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS.BD.Vis.Advanced"
                  }
                ]
              }
            ],
            "skillName": "Xử lý Dữ liệu Lớn và Kỹ thuật Đặc trưng (Big Data Processing & Feature Engineering)",
            "parentCompetency": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
            "description": "Kỹ năng này trang bị khả năng xử lý, phân tích các tập dữ liệu cực lớn, từ việc xây dựng kiến trúc nền tảng đến việc làm sạch và biến đổi dữ liệu thô thành các đặc trưng có giá trị. Người học sẽ biết cách chuẩn bị dữ liệu hiệu quả để tối ưu hóa hiệu suất của các mô hình học máy.",
            "learningResources": [
              {
                "title": "Chương 'Xử lý Dữ liệu Lớn' trong sách 'Data Science for Business'",
                "type": "Chương sách",
                "url": "https://www.oreilly.com/library/view/data-science-for/9781449361327/"
              },
              {
                "title": "Khóa học trực tuyến 'Feature Engineering for Machine Learning'",
                "type": "Khóa học",
                "url": "https://www.coursera.org/learn/feature-engineering"
              }
            ],
            "projectIdeas": [
              {
                "title": "Phát triển hệ thống phân tích và dự đoán xu hướng thị trường chứng khoán",
                "description": "Sử dụng Apache Spark để thu thập và xử lý dữ liệu giao dịch chứng khoán (historical data) từ nhiều nguồn khác nhau. Áp dụng kỹ thuật tiền xử lý để làm sạch dữ liệu, sau đó sử dụng các phương pháp kỹ thuật đặc trưng (ví dụ: tạo biến động, chỉ số kỹ thuật) và giảm chiều dữ liệu (PCA) để chuẩn bị tập dữ liệu cho các mô hình học máy dự đoán biến động giá trong tương lai."
              }
            ],
            "tools": [
              "Apache Spark",
              "Apache Hadoop",
              "Python (Pandas, Scikit-learn)",
              "Kafka",
              "SQL (cho các nền tảng Big Data như Hive, Impala)"
            ],
            "difficultyLevel": "Nâng cao",
            "estimatedTimeToComplete": "80-120 giờ",
            "importanceScore": 5
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_2_4",
            "name": "Kiểm định Giả thuyết và Suy luận Thống kê (Hypothesis Testing & Statistical Inference)",
            "type": "skill",
            "children": [
              {
                "id": "hypothesis_testing_concepts_process",
                "name": "Khái niệm và Quy trình Kiểm định Giả thuyết (Giả thuyết Null/Đối, P-value, Mức ý nghĩa)",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này giới thiệu các khái niệm cốt lõi của kiểm định giả thuyết thống kê, bao gồm việc hình thành giả thuyết Null (H₀) và giả thuyết Đối (H₁), hiểu P-value, và xác định mức ý nghĩa (alpha). Nắm vững các khái niệm này là nền tảng để đưa ra các quyết định dựa trên dữ liệu, giúp phân biệt giữa sự khác biệt ngẫu nhiên và sự khác biệt có ý nghĩa thống kê trong Khoa học dữ liệu và Dữ liệu lớn.",
                "keywords": [
                  "Giả thuyết Null",
                  "Giả thuyết Đối",
                  "P-value",
                  "Mức ý nghĩa",
                  "Kiểm định thống kê",
                  "Sai lầm loại I",
                  "Sai lầm loại II",
                  "Vùng bác bỏ",
                  "Giá trị tới hạn",
                  "Quy trình kiểm định giả thuyết"
                ],
                "learningResources": [
                  {
                    "name": "Khan Academy: Significance tests (hypothesis testing)",
                    "url": "https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals"
                  },
                  {
                    "name": "StatQuest with Josh Starmer: P-values explained",
                    "url": "https://www.youtube.com/watch?v=0zZYBALOIEU"
                  },
                  {
                    "name": "Towards Data Science: Hypothesis Testing Explained",
                    "url": "https://towardsdatascience.com/hypothesis-testing-explained-with-examples-and-formulas-bb64549ce23c"
                  }
                ],
                "prerequisites": [
                  "statistical_inference_descriptive_statistics",
                  "statistical_inference_basic_probability",
                  "statistical_inference_central_limit_theorem",
                  "sampling_techniques_and_bias"
                ],
                "projectIdeas": [
                  "Sử dụng một tập dữ liệu giả định (ví dụ: dữ liệu A/B test đơn giản) để đặt ra giả thuyết Null và Đối, tính toán P-value, và đưa ra kết luận dựa trên mức ý nghĩa đã chọn.",
                  "Viết một bài giải thích ngắn gọn về mối quan hệ giữa P-value, mức ý nghĩa và quyết định bác bỏ/không bác bỏ giả thuyết Null, kèm theo một ví dụ minh họa dễ hiểu."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Statistician",
                  "Business Intelligence Analyst",
                  "Research Scientist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (SciPy, NumPy, Pandas, Statsmodels)",
                  "R (Base R, Tidyverse)",
                  "Excel (cho các ví dụ đơn giản)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 9
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  }
                ],
                "skillRelations": [
                  {
                    "id": "statistical_inference_descriptive_statistics",
                    "type": "prerequisite",
                    "description": "Cần hiểu các số đo thống kê cơ bản để phân tích dữ liệu mẫu."
                  },
                  {
                    "id": "statistical_inference_basic_probability",
                    "type": "prerequisite",
                    "description": "Cần nắm vững các khái niệm xác suất để hiểu P-value và phân phối."
                  },
                  {
                    "id": "statistical_inference_central_limit_theorem",
                    "type": "prerequisite",
                    "description": "Nền tảng cho việc sử dụng các kiểm định dựa trên phân phối chuẩn."
                  },
                  {
                    "id": "sampling_techniques_and_bias",
                    "type": "prerequisite",
                    "description": "Hiểu cách lấy mẫu ảnh hưởng đến khả năng tổng quát hóa kết quả."
                  },
                  {
                    "id": "hypothesis_testing_specific_tests",
                    "type": "unlocks",
                    "description": "Kiến thức này là nền tảng để học các loại kiểm định cụ thể như T-test, Chi-square, ANOVA."
                  },
                  {
                    "id": "statistical_inference_confidence_intervals",
                    "type": "complementary",
                    "description": "Khoảng tin cậy cung cấp một cách khác để suy luận thống kê, bổ sung cho kiểm định giả thuyết."
                  },
                  {
                    "id": "regression_linear_regression",
                    "type": "unlocks",
                    "description": "Các kiểm định giả thuyết được sử dụng để đánh giá ý nghĩa của các hệ số trong mô hình hồi quy."
                  }
                ]
              },
              {
                "id": "STAT_INFERENCE_HT_02_TYPE_ERRORS_POWER",
                "name": "Sai lầm Loại I, Loại II và Lực kiểm định (Power of a Test)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này đi sâu vào việc hiểu các loại lỗi có thể xảy ra trong kiểm định giả thuyết: Sai lầm Loại I (bác bỏ giả thuyết null đúng) và Sai lầm Loại II (chấp nhận giả thuyết null sai). Nó cũng bao gồm khái niệm Lực kiểm định (Power of a Test), là xác suất bác bỏ giả thuyết null khi nó thực sự sai. Việc nắm vững các khái niệm này là cực kỳ quan trọng để đánh giá độ tin cậy của kết quả kiểm định thống kê và thiết kế các thí nghiệm hiệu quả trong Khoa học Dữ liệu và Dữ liệu lớn.",
                "keywords": [
                  "Sai lầm Loại I",
                  "Sai lầm Loại II",
                  "Lực kiểm định",
                  "Alpha (α)",
                  "Beta (β)",
                  "Mức ý nghĩa",
                  "Giả thuyết Null",
                  "Giả thuyết Thay thế",
                  "False Positive",
                  "False Negative"
                ],
                "learningResources": [
                  {
                    "name": "StatQuest: P-value, Type I and Type II Errors",
                    "url": "https://www.youtube.com/watch?v=5koM01s4J88",
                    "type": "Video Lecture"
                  },
                  {
                    "name": "Inferential Statistics Course (Duke University) - Coursera",
                    "url": "https://www.coursera.org/learn/inferential-statistics-intro",
                    "type": "Online Course Module"
                  },
                  {
                    "name": "NIST Engineering Statistics Handbook: Type I and Type II Errors",
                    "url": "https://www.itl.nist.gov/div898/handbook/prc/section1/prc14.htm",
                    "type": "Reference Guide"
                  }
                ],
                "prerequisites": [
                  "STAT_INFERENCE_HT_01_INTRO_HT",
                  "PROB_STAT_02_DISTRIBUTIONS_AND_PARAMETERS"
                ],
                "projectIdeas": [
                  {
                    "name": "Mô phỏng và Phân tích Sai lầm Loại I/II",
                    "description": "Viết mã (Python/R) để mô phỏng một kiểm định giả thuyết (ví dụ: kiểm định t) dưới các điều kiện khác nhau (kích thước mẫu, hiệu ứng thực tế, mức ý nghĩa alpha). Phân tích tần suất xảy ra Sai lầm Loại I và Loại II, đồng thời trực quan hóa mối quan hệ giữa chúng và lực kiểm định."
                  },
                  {
                    "name": "Tính toán Lực kiểm định cho Thiết kế Thí nghiệm A/B",
                    "description": "Chọn một kịch bản A/B testing thực tế (ví dụ: tăng tỷ lệ chuyển đổi trên website). Tính toán kích thước mẫu cần thiết để phát hiện một hiệu ứng mong muốn với một mức lực kiểm định (ví dụ 80%) và mức ý nghĩa cho trước. Phân tích ảnh hưởng của các yếu tố đầu vào (effect size, alpha) lên kết quả."
                  }
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Statistician",
                  "Machine Learning Engineer",
                  "Quantitative Analyst",
                  "Research Scientist",
                  "Business Intelligence Analyst"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (SciPy, StatsModels, Pingouin)",
                  "R (base stats, pwr package)",
                  "Jupyter Notebooks",
                  "RStudio"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Quantitative Analyst",
                    "score": 10
                  },
                  {
                    "path": "Research Scientist",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "STAT_INFERENCE_HT_01_INTRO_HT",
                    "type": "prerequisite",
                    "description": "Cần hiểu các khái niệm cơ bản về kiểm định giả thuyết, giả thuyết null và giả thuyết thay thế."
                  },
                  {
                    "id": "PROB_STAT_02_DISTRIBUTIONS_AND_PARAMETERS",
                    "type": "prerequisite",
                    "description": "Cần hiểu về các phân phối xác suất và các tham số thống kê."
                  },
                  {
                    "id": "STAT_INFERENCE_HT_03_P_VALUE_INTERPRETATION",
                    "type": "complementary",
                    "description": "Các khái niệm này bổ trợ cho việc giải thích p-value và đưa ra quyết định thống kê chính xác."
                  },
                  {
                    "id": "STAT_INFERENCE_HT_04_SAMPLE_SIZE_CALCULATION",
                    "type": "unlocks",
                    "description": "Hiểu về lực kiểm định là nền tảng để tính toán kích thước mẫu cần thiết cho các nghiên cứu và thí nghiệm."
                  },
                  {
                    "id": "AB_TESTING_01_DESIGN_AND_ANALYSIS",
                    "type": "unlocks",
                    "description": "Cung cấp kiến thức để thiết kế và phân tích A/B test một cách có ý nghĩa, tránh các kết luận sai lầm."
                  }
                ]
              },
              {
                "id": "DS-HTSI-SAMP_DIST_CLT",
                "name": "Phân phối Mẫu và Định lý Giới hạn Trung tâm (Central Limit Theorem)",
                "type": "knowledge",
                "children": [],
                "description": "Phân phối mẫu mô tả cách một thống kê (ví dụ: trung bình mẫu) phân bố qua nhiều mẫu ngẫu nhiên từ một quần thể. Định lý Giới hạn Trung tâm (CLT) là một định lý nền tảng, khẳng định rằng phân phối của các trung bình mẫu sẽ xấp xỉ phân phối chuẩn khi kích thước mẫu đủ lớn, bất kể hình dạng phân phối ban đầu của quần thể. Kiến thức này cực kỳ quan trọng vì nó là nền tảng cho suy luận thống kê, cho phép chúng ta rút ra kết luận về quần thể dựa trên dữ liệu từ các mẫu.",
                "keywords": [
                  "Central Limit Theorem",
                  "Sampling Distribution",
                  "Sample Mean",
                  "Standard Error",
                  "Inferential Statistics",
                  "Hypothesis Testing",
                  "Confidence Interval",
                  "Population Parameter",
                  "Normal Distribution",
                  "Law of Large Numbers"
                ],
                "learningResources": [
                  {
                    "name": "Khan Academy: Central Limit Theorem",
                    "url": "https://www.khanacademy.org/math/ap-statistics/sampling-distributions-ap/sampling-distribution-sample-mean/v/central-limit-theorem"
                  },
                  {
                    "name": "StatQuest: The Central Limit Theorem",
                    "url": "https://www.youtube.com/watch?v=JNmDCy6y00A"
                  },
                  {
                    "name": "Investopedia: Central Limit Theorem (CLT)",
                    "url": "https://www.investopedia.com/terms/c/central_limit_theorem.asp"
                  }
                ],
                "prerequisites": [
                  "DS-PROB-RANDOM_VARIABLES",
                  "DS-STATS-DESCRIPTIVE_STATS",
                  "DS-PROB-NORMAL_DISTRIBUTION"
                ],
                "projectIdeas": [
                  "Viết mã Python/R để mô phỏng Định lý Giới hạn Trung tâm: Lấy mẫu lặp đi lặp lại từ các phân phối phi chuẩn (ví dụ: phân phối đều, phân phối mũ) và trực quan hóa phân phối của các trung bình mẫu, cho thấy nó hội tụ về phân phối chuẩn.",
                  "Sử dụng một tập dữ liệu thực (ví dụ: chiều cao người, thời gian phản hồi máy chủ) để tính toán và giải thích khoảng tin cậy cho trung bình quần thể, giải thích cách CLT hỗ trợ việc này."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Statistician",
                  "Quantitative Analyst",
                  "Business Intelligence Analyst",
                  "Machine Learning Engineer",
                  "Research Scientist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (NumPy, SciPy, Pandas, Matplotlib, Seaborn)",
                  "R (base R, dplyr, ggplot2)",
                  "Jupyter Notebook / RStudio"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  },
                  {
                    "path": "Quantitative Analyst",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "DS-PROB-RANDOM_VARIABLES"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS-STATS-DESCRIPTIVE_STATS"
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS-PROB-NORMAL_DISTRIBUTION"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS-HTSI-HYPOTHESIS_TESTING_FUNDAMENTALS"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS-HTSI-CONFIDENCE_INTERVALS"
                  },
                  {
                    "type": "complementary",
                    "id": "DS-SAMPLING-SAMPLING_TECHNIQUES"
                  }
                ]
              },
              {
                "id": "DS_HTSI_ConfidenceIntervals",
                "name": "Ước lượng Khoảng và Khoảng Tin cậy (Confidence Intervals)",
                "type": "knowledge",
                "children": [],
                "description": "Ước lượng Khoảng và Khoảng Tin cậy (Confidence Intervals - CI) cung cấp một phạm vi giá trị mà tham số thực của tổng thể (population parameter) có khả năng nằm trong đó, dựa trên dữ liệu mẫu. Kiến thức này rất quan trọng để định lượng sự không chắc chắn liên quan đến các ước lượng điểm, từ đó đưa ra các suy luận thống kê mạnh mẽ và quyết định sáng suốt hơn trong phân tích dữ liệu.",
                "keywords": [
                  "Confidence Interval",
                  "Ước lượng khoảng",
                  "Khoảng tin cậy",
                  "Sai số chuẩn",
                  "Mức tin cậy",
                  "Mức ý nghĩa",
                  "Ước lượng điểm",
                  "Phân phối Z",
                  "Phân phối t",
                  "Biên độ lỗi"
                ],
                "learningResources": [
                  {
                    "name": "Confidence intervals - Khan Academy",
                    "url": "https://www.khanacademy.org/math/statistics-probability/confidence-intervals-z-scores",
                    "type": "Online Course/Tutorial"
                  },
                  {
                    "name": "Confidence Intervals Explained - Statology",
                    "url": "https://www.statology.org/confidence-intervals/",
                    "type": "Blog/Tutorial"
                  },
                  {
                    "name": "MIT OpenCourseware: Introduction to Probability and Statistics - Lecture Notes on Confidence Intervals",
                    "url": "https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/pages/lecture-notes/",
                    "type": "Academic Course Material"
                  }
                ],
                "prerequisites": [
                  "DS_MATH_BasicProbability",
                  "DS_STAT_DescriptiveStatistics",
                  "DS_STAT_CentralLimitTheorem"
                ],
                "projectIdeas": [
                  "Sử dụng dữ liệu khảo sát khách hàng để tính toán khoảng tin cậy cho tỷ lệ hài lòng của khách hàng đối với một sản phẩm/dịch vụ cụ thể.",
                  "Từ dữ liệu giao dịch mẫu, ước lượng khoảng tin cậy cho mức chi tiêu trung bình của khách hàng trong một khoảng thời gian nhất định và diễn giải ý nghĩa của kết quả."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Statistician",
                  "Business Intelligence Analyst",
                  "Research Scientist",
                  "Quantitative Analyst"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (NumPy, SciPy, Pandas, Statsmodels)",
                  "R (base R, stats package)",
                  "Excel",
                  "Jupyter Notebook",
                  "RStudio"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-16 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 8
                  },
                  {
                    "path": "Research Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS_MATH_BasicProbability",
                    "type": "prerequisite"
                  },
                  {
                    "id": "DS_STAT_DescriptiveStatistics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "DS_STAT_CentralLimitTheorem",
                    "type": "prerequisite"
                  },
                  {
                    "id": "DS_HTSI_HypothesisTesting",
                    "type": "complementary"
                  },
                  {
                    "id": "DS_STAT_SamplingMethods",
                    "type": "complementary"
                  },
                  {
                    "id": "DS_HTSI_ABTesting",
                    "type": "unlocks"
                  },
                  {
                    "id": "DS_ML_ModelEvaluationMetrics",
                    "type": "unlocks"
                  }
                ]
              }
            ],
            "skillName": "Kiểm định Giả thuyết và Suy luận Thống kê (Hypothesis Testing & Statistical Inference)",
            "competency": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
            "description": "Kỹ năng này trang bị cho người học khả năng đưa ra các kết luận và quyết định có căn cứ về một quần thể dựa trên dữ liệu mẫu. Người học sẽ hiểu cách thiết lập và kiểm định các giả thuyết thống kê, đánh giá mức độ tin cậy của các ước lượng và diễn giải các kết quả một cách chính xác. Đây là nền tảng thiết yếu để suy luận từ dữ liệu một cách khoa học và vững chắc.",
            "learningResources": [
              {
                "type": "Chapter",
                "title": "Chương 7-9: Kiểm định Giả thuyết và Ước lượng Khoảng",
                "source": "Sách 'Thống kê ứng dụng cho Khoa học Dữ liệu' của David S. Moore hoặc tương đương",
                "url": "https://www.amazon.com/Statistics-Data-Scientists-Machine-Learning/dp/109810452X"
              },
              {
                "type": "Video Playlist",
                "title": "Series Kiểm định Giả thuyết và Khoảng tin cậy",
                "source": "Kênh YouTube của Khan Academy hoặc 365 Data Science",
                "url": "https://www.youtube.com/playlist?list=PLvyG5wHj8-1p6B5zX5J1S_K-jN2z2G4F1"
              }
            ],
            "projectIdeas": [
              {
                "title": "Phân tích Hiệu quả Chiến dịch Marketing",
                "description": "Bạn được cung cấp dữ liệu doanh số trước và sau khi triển khai một chiến dịch marketing mới. Hãy sử dụng kiểm định giả thuyết để xác định xem chiến dịch này có tạo ra sự gia tăng doanh số trung bình đáng kể hay không. Đồng thời, xây dựng khoảng tin cậy cho mức tăng trưởng doanh số trung bình và phân tích nguy cơ mắc phải sai lầm loại I hoặc loại II trong kết luận của bạn. Trực quan hóa kết quả và trình bày các đề xuất dựa trên suy luận thống kê."
              }
            ],
            "tools": [
              "Python (SciPy, Statsmodels, NumPy, Pandas, Matplotlib, Seaborn)",
              "R (dplyr, ggplot2, base R stats)",
              "Excel (Data Analysis ToolPak)",
              "SPSS / SAS (phần mềm thống kê chuyên dụng)"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "25-35 giờ",
            "importanceScore": 8
          }
        ],
        "abilityName": "Phân tích Thống kê và Học máy (Statistical Analysis & Machine Learning)",
        "domain": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
        "description": "Năng lực này là nền tảng để trích xuất thông tin chuyên sâu, xây dựng các mô hình dự đoán mạnh mẽ và đưa ra quyết định dựa trên dữ liệu. Nó kết hợp nền tảng suy luận thống kê chặt chẽ với sức mạnh của các thuật toán học máy hiện đại, cho phép chuyên gia hiểu các mẫu phức tạp và dự báo xu hướng tương lai. Trong lĩnh vực Khoa học dữ liệu và Dữ liệu lớn, việc thành thạo năng lực này giúp biến dữ liệu thô thành thông tin tình báo có thể hành động.",
        "learningResources": [
          {
            "type": "Khóa học",
            "name": "Machine Learning by Andrew Ng (Coursera)",
            "link": "https://www.coursera.org/learn/machine-learning"
          },
          {
            "type": "Sách",
            "name": "An Introduction to Statistical Learning (ISLR)",
            "link": "http://www-bcf.usc.edu/~gareth/ISL/index.html"
          }
        ],
        "projectIdeas": [
          "Phát triển một mô hình dự đoán toàn diện từ đầu đến cuối (end-to-end) cho các trường hợp như dự đoán khách hàng rời bỏ (churn prediction) hoặc phát hiện gian lận. Dự án sẽ bao gồm các bước xử lý dữ liệu lớn, kỹ thuật đặc trưng, lựa chọn và đánh giá mô hình học máy, đồng thời sử dụng các phương pháp thống kê để xác nhận kết quả và đưa ra suy luận."
        ],
        "tools": [
          "Python (Scikit-learn, Pandas, NumPy, TensorFlow/PyTorch)",
          "R",
          "Apache Spark",
          "Hadoop",
          "SQL",
          "Nền tảng Cloud (AWS Sagemaker, Google Cloud AI Platform, Azure Machine Learning)"
        ],
        "difficultyLevel": "Nâng cao",
        "estimatedTimeToComplete": "6-12 tháng (để đạt trình độ thành thạo cơ bản)",
        "importanceScore": "5/5"
      },
      {
        "id": "ability_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_3",
        "name": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
        "type": "ability",
        "children": [
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_3_1",
            "name": "Thiết kế và Triển khai Kiến trúc Dữ liệu Lớn (Big Data Architecture Design and Implementation)",
            "type": "skill",
            "children": [
              {
                "id": "BIG_DATA_ARCH_MODELS_COMPONENTS",
                "name": "Các Mô hình và Thành phần Kiến trúc Dữ liệu Lớn",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này khám phá các mô hình kiến trúc Big Data phổ biến như Lambda và Kappa, cùng với các thành phần cốt lõi tạo nên một hệ sinh thái dữ liệu lớn hoàn chỉnh. Nó bao gồm các khía cạnh về lưu trữ, xử lý, truyền tải dữ liệu và quản lý tài nguyên. Việc nắm vững các mô hình và thành phần này là thiết yếu để thiết kế các hệ thống dữ liệu lớn có khả năng mở rộng, hiệu quả và đáng tin cậy, đáp ứng các yêu cầu nghiệp vụ phức tạp trong thế giới thực.",
                "keywords": [
                  "Kiến trúc Lambda",
                  "Kiến trúc Kappa",
                  "Hồ dữ liệu (Data Lake)",
                  "Kho dữ liệu (Data Warehouse)",
                  "Xử lý hàng loạt (Batch Processing)",
                  "Xử lý luồng (Stream Processing)",
                  "Hệ sinh thái Hadoop",
                  "Apache Spark",
                  "Apache Kafka",
                  "ETL/ELT Pipeline",
                  "Metadata Management"
                ],
                "learningResources": [
                  {
                    "name": "Big Data Architectures on AWS",
                    "url": "https://aws.amazon.com/big-data/architecture/"
                  },
                  {
                    "name": "Azure Big Data Solutions",
                    "url": "https://azure.microsoft.com/en-us/solutions/big-data/"
                  },
                  {
                    "name": "The Lambda Architecture: Principles for Architecting Realtime Big Data Systems",
                    "url": "https://www.oreilly.com/library/view/the-lambda-architecture/9781491907767/"
                  }
                ],
                "prerequisites": [
                  "K_BIG_DATA_INTRO_CONCEPTS"
                ],
                "projectIdeas": [
                  {
                    "name": "Thiết kế kiến trúc Big Data cho hệ thống khuyến nghị thương mại điện tử",
                    "description": "Xây dựng một thiết kế kiến trúc Big Data chi tiết (sử dụng một trong các mô hình Lambda/Kappa) để thu thập, xử lý và phân tích dữ liệu người dùng (lịch sử duyệt, mua hàng) nhằm cung cấp các khuyến nghị sản phẩm theo thời gian thực và hàng loạt."
                  },
                  {
                    "name": "Phân tích và so sánh kiến trúc Lambda vs. Kappa cho hệ thống phân tích nhật ký",
                    "description": "Nghiên cứu một trường hợp sử dụng phân tích nhật ký hệ thống (log analytics), sau đó phân tích ưu nhược điểm của việc áp dụng kiến trúc Lambda và Kappa. Đề xuất kiến trúc tối ưu nhất và giải thích các thành phần chính được sử dụng."
                  }
                ],
                "relatedJobTitles": [
                  "Big Data Architect",
                  "Data Engineer",
                  "Solutions Architect",
                  "Cloud Architect",
                  "Data Platform Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Hadoop (HDFS, YARN, MapReduce)",
                  "Apache Spark",
                  "Apache Kafka",
                  "Apache Flink",
                  "Apache NiFi",
                  "AWS S3 / Azure Data Lake Storage / Google Cloud Storage",
                  "Snowflake / Google BigQuery / AWS Redshift / Azure Synapse Analytics",
                  "Apache Hive",
                  "Apache Presto/Trino"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architecture",
                    "score": 10
                  },
                  {
                    "path": "Cloud Architecture",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "K_BIG_DATA_INTRO_CONCEPTS",
                    "type": "prerequisite",
                    "name": "Giới thiệu về Dữ liệu lớn và Thách thức"
                  },
                  {
                    "id": "DATA_WAREHOUSE_LAKE_DESIGN_IMPLEMENTATION",
                    "type": "complementary",
                    "name": "Thiết kế và Triển khai Kho dữ liệu/Hồ dữ liệu"
                  },
                  {
                    "id": "CLOUD_BIG_DATA_ARCH_DESIGN",
                    "type": "unlocks",
                    "name": "Thiết kế Kiến trúc Dữ liệu lớn trên Cloud"
                  },
                  {
                    "id": "BIG_DATA_ARCH_OPTIMIZATION_OPERATIONS",
                    "type": "unlocks",
                    "name": "Tối ưu hóa và Vận hành Kiến trúc Dữ liệu lớn"
                  }
                ]
              },
              {
                "id": "ds_bd_bigdata_arch_design_impl_bigdata_ecosystem_tech",
                "name": "Hệ sinh thái và Công nghệ Xử lý Dữ liệu Lớn",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này bao gồm tổng quan về các thành phần, công cụ và quy trình hoạt động trong một hệ sinh thái dữ liệu lớn hiện đại. Nó tập trung vào việc hiểu cách các công nghệ như Hadoop, Spark, Kafka và các cơ sở dữ liệu NoSQL tương tác để thu thập, lưu trữ, xử lý và phân tích dữ liệu ở quy mô lớn. Việc nắm vững hệ sinh thái này là rất quan trọng để xây dựng các giải pháp dữ liệu hiệu quả, có khả năng mở rộng và bền vững.",
                "keywords": [
                  "Hadoop Ecosystem",
                  "Apache Spark",
                  "Apache Kafka",
                  "NoSQL Databases",
                  "Data Lake",
                  "Batch Processing",
                  "Stream Processing",
                  "Distributed Computing",
                  "Cloud Big Data",
                  "ETL/ELT"
                ],
                "learningResources": [
                  {
                    "title": "Apache Hadoop Documentation",
                    "url": "https://hadoop.apache.org/docs/stable/"
                  },
                  {
                    "title": "Apache Spark Documentation",
                    "url": "https://spark.apache.org/docs/latest/"
                  },
                  {
                    "title": "IBM Introduction to Big Data (Coursera)",
                    "url": "https://www.coursera.org/learn/introduction-to-big-data"
                  }
                ],
                "prerequisites": [
                  "ds_bd_bigdata_arch_design_impl_intro_to_bigdata_concepts",
                  "programming_fundamentals_python",
                  "database_fundamentals_sql_nosql_concepts"
                ],
                "projectIdeas": [
                  "Thiết kế và triển khai một pipeline xử lý dữ liệu batch đơn giản bằng Apache Spark để phân tích tập dữ liệu lớn (ví dụ: dữ liệu log, dữ liệu giao dịch công khai) và lưu trữ kết quả vào HDFS hoặc một hệ thống file khác.",
                  "Xây dựng một hệ thống ingestion dữ liệu thời gian thực cơ bản sử dụng Apache Kafka để thu thập dữ liệu từ một nguồn mô phỏng và chuyển tiếp đến một ứng dụng phân tích hoặc lưu trữ."
                ],
                "relatedJobTitles": [
                  "Kỹ sư Dữ liệu (Data Engineer)",
                  "Kỹ sư Dữ liệu lớn (Big Data Engineer)",
                  "Kiến trúc sư Dữ liệu (Data Architect)",
                  "Kỹ sư Điện toán Đám mây (Cloud Engineer - Data Focus)",
                  "Chuyên gia Giải pháp Dữ liệu (Data Solutions Specialist)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Hadoop (HDFS, YARN, MapReduce)",
                  "Apache Spark (Core, SQL, Streaming)",
                  "Apache Kafka",
                  "Apache Flink",
                  "NoSQL Databases (Cassandra, MongoDB, HBase)",
                  "Cloud Big Data Services (AWS EMR, Azure HDInsight, GCP Dataproc, Databricks)",
                  "Apache Hive",
                  "Apache Pig"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-60 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Kỹ sư Dữ liệu",
                    "score": 10
                  },
                  {
                    "path": "Kiến trúc sư Dữ liệu",
                    "score": 9
                  },
                  {
                    "path": "Khoa học Dữ liệu (với vai trò MLOps/DataOps)",
                    "score": 8
                  },
                  {
                    "path": "Kỹ sư Đám mây",
                    "score": 8
                  },
                  {
                    "path": "Kỹ sư phần mềm (phát triển các ứng dụng dữ liệu)",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ds_bd_bigdata_arch_design_impl_intro_to_bigdata_concepts"
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_data_warehousing_etl_elt"
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_cloud_computing_for_big_data"
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_distributed_systems_fundamentals"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_advanced_data_pipeline_design"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_stream_processing_realtime_analytics"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_data_lakehouse_architecture"
                  }
                ]
              },
              {
                "id": "DS_BD_BDADI_NoSQL_SQL_Principles",
                "name": "Nguyên lý Cơ sở Dữ liệu NoSQL và SQL trong Bối cảnh Dữ liệu Lớn",
                "type": "knowledge",
                "children": [],
                "name_en": "NoSQL and SQL Database Principles in Big Data Context",
                "description": "Đơn vị kiến thức này khám phá các nguyên lý cơ bản, kiến trúc và trường hợp sử dụng của cả cơ sở dữ liệu SQL (quan hệ) và NoSQL (phi quan hệ) trong bối cảnh Dữ liệu Lớn. Việc hiểu rõ sự khác biệt cốt lõi, điểm mạnh và điểm yếu của chúng là vô cùng quan trọng để thiết kế các giải pháp lưu trữ dữ liệu hiệu quả, có khả năng mở rộng và bền bỉ cho các ứng dụng chuyên sâu về dữ liệu hiện đại.",
                "keywords": [
                  "SQL",
                  "NoSQL",
                  "RDBMS",
                  "Distributed Databases",
                  "ACID",
                  "BASE",
                  "Scalability",
                  "Consistency Models",
                  "Data Modeling",
                  "Big Data Architecture",
                  "Schema-on-read",
                  "Schema-on-write"
                ],
                "learningResources": [
                  {
                    "name": "An Introduction to Relational Databases (IBM Developer)",
                    "url": "https://developer.ibm.com/articles/intro-to-relational-databases/",
                    "type": "Article"
                  },
                  {
                    "name": "NoSQL Databases Explained: A Brief Introduction (MongoDB)",
                    "url": "https://www.mongodb.com/nosql-explained",
                    "type": "Article"
                  },
                  {
                    "name": "Choosing the right database for your application (AWS)",
                    "url": "https://aws.amazon.com/compare/the-right-database-for-your-application/",
                    "type": "Guide"
                  }
                ],
                "prerequisites": [
                  "Core_CS_DataStructures",
                  "Core_CS_Database_Concepts"
                ],
                "projectIdeas": [
                  "Thiết kế kiến trúc dữ liệu cho một ứng dụng mạng xã hội nhỏ, so sánh việc sử dụng mô hình SQL (ví dụ: PostgreSQL) với mô hình NoSQL (ví dụ: MongoDB hoặc Apache Cassandra) để lưu trữ hồ sơ người dùng, bài đăng và tương tác. Phân tích ưu nhược điểm của mỗi cách tiếp cận về khả năng mở rộng, tính nhất quán và hiệu suất trong ngữ cảnh dữ liệu lớn.",
                  "Xây dựng một hệ thống theo dõi đơn giản cho dữ liệu cảm biến (IoT) hoặc log ứng dụng, sử dụng một cơ sở dữ liệu NoSQL như Apache Cassandra hoặc Apache Hbase để xử lý lượng lớn dữ liệu ghi nhanh, không cấu trúc. Phát triển các API truy vấn cơ bản để lấy dữ liệu theo thời gian hoặc theo thiết bị."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Architect",
                  "Database Administrator (Cloud/Distributed)",
                  "Solutions Architect",
                  "Data Scientist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "PostgreSQL",
                  "MySQL",
                  "Apache Hive",
                  "Apache Presto/Trino",
                  "MongoDB",
                  "Apache Cassandra",
                  "Redis",
                  "Apache Hbase",
                  "Neo4j",
                  "Apache CouchDB",
                  "AWS DynamoDB",
                  "Google Cloud Bigtable",
                  "Azure Cosmos DB"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-50 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architect",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 8
                  },
                  {
                    "path": "Solutions Architect",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "Core_CS_DataStructures",
                    "type": "prerequisite"
                  },
                  {
                    "id": "Core_CS_Database_Concepts",
                    "type": "prerequisite"
                  },
                  {
                    "id": "DS_BD_BDADI_Data_Modeling_BigData",
                    "type": "complementary"
                  },
                  {
                    "id": "DS_BD_BDADI_Distributed_Systems_Fund",
                    "type": "complementary"
                  },
                  {
                    "id": "DS_BD_BDADI_Cloud_Data_Services",
                    "type": "complementary"
                  },
                  {
                    "id": "DS_BD_BDADI_BigData_Storage_Solutions",
                    "type": "unlocks"
                  },
                  {
                    "id": "DS_BD_BDADI_Data_Pipeline_Design",
                    "type": "unlocks"
                  },
                  {
                    "id": "DS_BD_BDADI_Realtime_Data_Arch",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "data-science-big-data_big-data-architecture_data-pipeline-design",
                "name": "Thiết kế và Triển khai Luồng Dữ liệu (Data Pipeline Design and Implementation)",
                "type": "knowledge",
                "children": [],
                "description": "Luồng dữ liệu (Data pipeline) là một chuỗi các bước tự động để di chuyển, biến đổi và tải dữ liệu từ nhiều nguồn khác nhau đến đích cuối cùng, thường là một kho dữ liệu hoặc hồ dữ liệu. Việc thiết kế và triển khai hiệu quả các luồng dữ liệu là cốt lõi để đảm bảo dữ liệu sạch, đáng tin cậy và kịp thời cho phân tích, học máy và các ứng dụng kinh doanh khác, từ đó tối ưu hóa quá trình ra quyết định và vận hành hệ thống dữ liệu lớn.",
                "keywords": [
                  "Data Pipeline",
                  "ETL/ELT",
                  "Data Ingestion",
                  "Data Transformation",
                  "Data Orchestration",
                  "Batch Processing",
                  "Stream Processing",
                  "Data Warehousing",
                  "Data Lake",
                  "Apache Airflow",
                  "Apache Kafka",
                  "Apache Spark"
                ],
                "learningResources": [
                  {
                    "name": "Designing Data-Intensive Applications by Martin Kleppmann",
                    "type": "Book",
                    "url": "https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Scalable/dp/1449373321"
                  },
                  {
                    "name": "Data Engineering with Google Cloud Professional Certificate (Coursera)",
                    "type": "Online Course",
                    "url": "https://www.coursera.org/professional-certificates/google-cloud-data-engineer"
                  },
                  {
                    "name": "Apache Airflow Official Documentation",
                    "type": "Documentation",
                    "url": "https://airflow.apache.org/docs/apache-airflow/stable/index.html"
                  }
                ],
                "prerequisites": [
                  "data-science-big-data_programming_python-programming",
                  "data-science-big-data_database_sql-fundamentals",
                  "data-science-big-data_big-data-concepts_distributed-systems-basics",
                  "data-science-big-data_big-data-architecture_data-storage-solutions"
                ],
                "projectIdeas": [
                  "Xây dựng một luồng dữ liệu ETL đơn giản sử dụng Python (với Pandas) và Apache Airflow để thu thập dữ liệu từ một API công khai (ví dụ: thời tiết, chứng khoán), làm sạch, biến đổi và lưu trữ vào một cơ sở dữ liệu (ví dụ: PostgreSQL) hoặc S3.",
                  "Thiết kế và triển khai một luồng dữ liệu ELT (Extract, Load, Transform) sử dụng Apache Spark để xử lý tập dữ liệu lớn (ví dụ: nhật ký truy cập web, dữ liệu bán hàng) từ một Data Lake (ví dụ: S3), thực hiện các phép biến đổi phức tạp (aggregate, join) và tải vào một Data Warehouse (ví dụ: Snowflake hoặc Redshift)."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "ETL Developer",
                  "Data Architect",
                  "ML Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Airflow",
                  "Apache Kafka",
                  "Apache Spark",
                  "Apache Flink",
                  "AWS Glue",
                  "Azure Data Factory",
                  "Google Cloud Dataflow",
                  "Luigi",
                  "Prefect",
                  "DBT (Data Build Tool)",
                  "Snowflake",
                  "Amazon Redshift",
                  "Google BigQuery",
                  "Python (Pandas, Dask)"
                ],
                "difficultyLevel": "Advanced",
                "estimatedTimeToComplete": "80-120 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architect",
                    "score": 10
                  },
                  {
                    "path": "ML Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "data-science-big-data_programming_python-programming",
                    "name": "Lập trình Python"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data-science-big-data_database_sql-fundamentals",
                    "name": "Cơ bản về SQL"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data-science-big-data_big-data-concepts_distributed-systems-basics",
                    "name": "Cơ bản về Hệ thống Phân tán"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data-science-big-data_big-data-architecture_data-storage-solutions",
                    "name": "Giải pháp Lưu trữ Dữ liệu Lớn"
                  },
                  {
                    "type": "complementary",
                    "id": "data-science-big-data_big-data-architecture_data-governance",
                    "name": "Quản trị Dữ liệu"
                  },
                  {
                    "type": "complementary",
                    "id": "data-science-big-data_big-data-architecture_real-time-data-processing",
                    "name": "Xử lý Dữ liệu Thời gian Thực"
                  },
                  {
                    "type": "complementary",
                    "id": "data-science-big-data_big-data-architecture_cloud-data-platforms",
                    "name": "Nền tảng Dữ liệu Đám mây"
                  },
                  {
                    "type": "unlocks",
                    "id": "data-science-big-data_machine-learning_mlops",
                    "name": "MLOps"
                  },
                  {
                    "type": "unlocks",
                    "id": "data-science-big-data_data-warehousing_data-warehouse-design",
                    "name": "Thiết kế Kho dữ liệu"
                  },
                  {
                    "type": "unlocks",
                    "id": "data-science-big-data_analytics_business-intelligence-dashboarding",
                    "name": "Thiết kế Bảng điều khiển BI"
                  }
                ]
              }
            ],
            "competency": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
            "description": "Kỹ năng này cho phép bạn thiết kế, xây dựng và triển khai các kiến trúc dữ liệu lớn mạnh mẽ và có khả năng mở rộng. Bạn sẽ có khả năng lựa chọn các công nghệ phù hợp, tích hợp các thành phần khác nhau để tạo ra luồng dữ liệu hiệu quả từ thu thập đến phân tích. Từ đó, đáp ứng được các yêu cầu về hiệu suất và quy mô xử lý dữ liệu khổng lồ của doanh nghiệp, giúp trích xuất giá trị kinh doanh từ dữ liệu lớn.",
            "learningResources": [
              {
                "title": "Chương 1-3: Các nguyên lý nền tảng và kiến trúc trong 'Designing Data-Intensive Applications'",
                "type": "Book Chapter",
                "url": "https://www.oreilly.com/library/view/designing-data-intensive-applications/9781449373320/"
              },
              {
                "title": "Khóa học 'Big Data Architect Mastertrack' (ví dụ trên Coursera/Udemy)",
                "type": "Online Course Series",
                "url": "https://www.coursera.org/specializations/big-data-architect"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng hệ thống phân tích dữ liệu bán lẻ theo thời gian thực",
                "description": "Thiết kế và triển khai kiến trúc thu thập dữ liệu giao dịch từ các điểm bán hàng sử dụng Apache Kafka. Xử lý và làm sạch dữ liệu này bằng Apache Spark Streaming, sau đó lưu trữ vào cơ sở dữ liệu NoSQL (như Cassandra) và một data lake (S3/ADLS). Cuối cùng, cung cấp dữ liệu đã xử lý cho các dashboard phân tích thông qua các công cụ truy vấn như Presto/Hive."
              }
            ],
            "tools": [
              "Apache Hadoop (HDFS, YARN)",
              "Apache Spark",
              "Apache Kafka",
              "Apache Flink",
              "Apache Cassandra",
              "MongoDB",
              "Apache HBase",
              "Apache Hive",
              "Presto/Trino",
              "Apache Airflow",
              "AWS EMR",
              "Google Cloud Dataflow",
              "Azure Databricks",
              "AWS S3",
              "Azure Data Lake Storage"
            ],
            "difficultyLevel": "Advanced",
            "estimatedTimeToComplete": "300-500 giờ",
            "importanceScore": "Very High"
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_3_2",
            "name": "Xử lý Dữ liệu Phân tán với Apache Spark (Distributed Data Processing with Apache Spark)",
            "type": "skill",
            "children": [
              {
                "id": "distributed_systems_principles_architecture",
                "name": "Nguyên lý và Kiến trúc Hệ thống Phân tán",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị nền tảng về cách các hệ thống máy tính phối hợp hoạt động trên nhiều máy để giải quyết các vấn đề phức tạp, như xử lý dữ liệu lớn. Việc hiểu các nguyên lý cốt lõi như tính nhất quán, khả năng chịu lỗi và khả năng mở rộng là cực kỳ quan trọng để thiết kế, triển khai và gỡ lỗi hiệu quả các ứng dụng phân tán, đặc biệt là trong môi trường dữ liệu lớn với các công cụ như Apache Spark.",
                "keywords": [
                  "distributed computing",
                  "fault tolerance",
                  "consistency models",
                  "scalability",
                  "CAP theorem",
                  "concurrency",
                  "message passing",
                  "RPC",
                  "consensus algorithms",
                  "distributed transactions"
                ],
                "learningResources": [
                  {
                    "name": "Designing Data-Intensive Applications",
                    "type": "Book",
                    "url": "https://www.oreilly.com/library/view/designing-data-intensive-applications/9781449373320/"
                  },
                  {
                    "name": "MIT 6.824 Distributed Systems (Spring 2020)",
                    "type": "Online Course (Lectures)",
                    "url": "https://ocw.mit.edu/courses/6-824-distributed-systems-spring-2020/video_galleries/lecture-videos/"
                  },
                  {
                    "name": "Distributed Systems: Concepts and Design (5th Edition)",
                    "type": "Book",
                    "url": "https://www.pearson.com/us/higher-education/program/Coulouris-Distributed-Systems-Concepts-and-Design-5th-Edition/PGM224163.html"
                  }
                ],
                "prerequisites": [
                  "cs_fundamentals",
                  "networking_basics",
                  "operating_systems_concepts"
                ],
                "projectIdeas": [
                  "Xây dựng một dịch vụ key-value store phân tán đơn giản với các thuộc tính cơ bản về nhất quán và chịu lỗi.",
                  "Mô phỏng một thuật toán đồng thuận (như Paxos hoặc Raft) để hiểu cách các nút đạt được sự đồng thuận trong hệ thống phân tán."
                ],
                "relatedJobTitles": [
                  "Distributed Systems Engineer",
                  "Data Engineer",
                  "Big Data Engineer",
                  "Solutions Architect",
                  "Cloud Engineer",
                  "Site Reliability Engineer (SRE)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Kafka",
                  "Apache ZooKeeper",
                  "Kubernetes",
                  "gRPC",
                  "Apache Hadoop (HDFS, YARN)",
                  "Apache Cassandra"
                ],
                "difficultyLevel": "Advanced",
                "estimatedTimeToComplete": "60-100 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Science",
                    "score": 8
                  },
                  {
                    "path": "Big Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Software Engineering (Backend/Distributed Systems)",
                    "score": 10
                  },
                  {
                    "path": "Cloud Architecture",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "distributed_data_processing_spark",
                    "type": "unlocks",
                    "description": "Hiểu rõ các nguyên lý này là nền tảng để sử dụng hiệu quả và tối ưu Apache Spark."
                  },
                  {
                    "id": "hadoop_ecosystem_fundamentals",
                    "type": "complementary",
                    "description": "Kiến thức về Hadoop Ecosystem áp dụng trực tiếp các nguyên lý hệ thống phân tán."
                  },
                  {
                    "id": "real_time_data_streaming_kafka",
                    "type": "prerequisite",
                    "description": "Là tiền đề để hiểu sâu các hệ thống xử lý dòng dữ liệu thời gian thực như Kafka."
                  }
                ]
              },
              {
                "id": "K001",
                "name": "Khái niệm cơ bản về Big Data và Hệ sinh thái Hadoop",
                "type": "knowledge",
                "children": [],
                "name_en": "Basic Concepts of Big Data and Hadoop Ecosystem",
                "description": "Đơn vị kiến thức này giới thiệu về các khái niệm cốt lõi của Big Data, bao gồm 5 V (Volume, Velocity, Variety, Veracity, Value) và sự cần thiết của các hệ thống xử lý dữ liệu lớn. Nó đi sâu vào kiến trúc và các thành phần chính của Hệ sinh thái Hadoop, bao gồm HDFS, YARN, và MapReduce, làm nền tảng cho việc xử lý dữ liệu phân tán. Kiến thức này rất quan trọng để hiểu cách dữ liệu lớn được lưu trữ, quản lý và xử lý hiệu quả trong môi trường phân tán.",
                "keywords": [
                  "Big Data",
                  "Hadoop Ecosystem",
                  "HDFS",
                  "MapReduce",
                  "YARN",
                  "NoSQL",
                  "Data Warehousing",
                  "Distributed Computing",
                  "Data Lakes",
                  "5 Vs of Big Data"
                ],
                "learningResources": [
                  {
                    "title": "Apache Hadoop Official Documentation",
                    "url": "https://hadoop.apache.org/docs/current/",
                    "type": "Official Documentation"
                  },
                  {
                    "title": "Introduction to Big Data (IBM Course on Coursera)",
                    "url": "https://www.coursera.org/learn/introduction-to-big-data",
                    "type": "Online Course"
                  },
                  {
                    "title": "Big Data: The Ultimate Guide",
                    "url": "https://www.ibm.com/topics/big-data",
                    "type": "Article/Guide"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Cài đặt và cấu hình một cụm Hadoop (dạng Pseudo-distributed hoặc Single-node) trên máy ảo hoặc Docker, sau đó thực hiện các thao tác cơ bản với HDFS (tạo thư mục, upload/download file).",
                  "Chạy một chương trình MapReduce đơn giản (ví dụ: đếm từ - WordCount) trên cụm Hadoop đã cài đặt để làm quen với quy trình xử lý dữ liệu phân tán."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Developer",
                  "Data Architect",
                  "Cloud Engineer",
                  "Data Scientist (with Big Data focus)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Apache Hadoop (HDFS, YARN, MapReduce)",
                  "Linux Command Line",
                  "Java (for MapReduce development - basic understanding)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "15-25 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Developer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "targetSkillId": "K002_Spark_Core_Concepts",
                    "type": "prerequisite",
                    "description": "Kiến thức về Big Data và Hadoop là nền tảng để hiểu các hệ thống xử lý phân tán như Apache Spark."
                  },
                  {
                    "targetSkillId": "K003_Hive_Pig_Introduction",
                    "type": "prerequisite",
                    "description": "Là nền tảng cho việc tìm hiểu các công cụ dựa trên Hadoop như Hive và Pig."
                  },
                  {
                    "targetSkillId": "K004_NoSQL_Databases",
                    "type": "complementary",
                    "description": "Các cơ sở dữ liệu NoSQL thường được sử dụng trong các kiến trúc Big Data, bổ sung cho HDFS."
                  }
                ]
              },
              {
                "id": "spark_core_data_structures_api",
                "name": "Cấu trúc dữ liệu và API chính của Apache Spark (Core Data Structures and APIs of Apache Spark: RDD, DataFrame, Dataset)",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào các cấu trúc dữ liệu cốt lõi của Apache Spark: Resilient Distributed Datasets (RDDs), DataFrames và Datasets. Hiểu rõ sự khác biệt, ưu nhược điểm và cách sử dụng các API tương ứng là nền tảng để viết các ứng dụng Spark hiệu quả. Đây là kiến thức không thể thiếu để xử lý dữ liệu lớn một cách phân tán và tối ưu.",
                "keywords": [
                  "Apache Spark",
                  "RDD",
                  "DataFrame",
                  "Dataset",
                  "Spark SQL",
                  "Spark Core API",
                  "Distributed Data Structures",
                  "Big Data Processing",
                  "ETL",
                  "Data Transformation"
                ],
                "learningResources": [
                  {
                    "name": "Apache Spark Programming Guide (Official Documentation)",
                    "url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
                  },
                  {
                    "name": "Databricks - Mastering Apache Spark",
                    "url": "https://www.databricks.com/resources/ebook/mastering-apache-spark"
                  },
                  {
                    "name": "Learning Spark, 2nd Edition (O'Reilly)",
                    "url": "https://www.oreilly.com/library/view/learning-spark-2nd/9781492048598/"
                  }
                ],
                "prerequisites": [
                  "basic_python_programming",
                  "sql_fundamentals",
                  "distributed_computing_concepts_intro",
                  "spark_introduction_and_architecture"
                ],
                "projectIdeas": [
                  "Xây dựng một pipeline ETL đơn giản để đọc dữ liệu từ CSV/JSON, thực hiện các phép biến đổi cơ bản (filter, select, join) bằng DataFrame API và ghi kết quả ra Parquet.",
                  "Phân tích dữ liệu nhật ký (log data) bằng RDD API để đếm số lần xuất hiện của các mã lỗi hoặc các sự kiện cụ thể, so sánh hiệu suất với DataFrame API."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Spark Developer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Spark",
                  "PySpark",
                  "Scala",
                  "Java",
                  "R",
                  "Jupyter Notebook",
                  "Databricks Runtime",
                  "Apache Zeppelin"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architecture",
                    "score": 9
                  },
                  {
                    "path": "Data Science",
                    "score": 8
                  },
                  {
                    "path": "Machine Learning Engineering",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetSkillId": "basic_python_programming"
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "sql_fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "distributed_computing_concepts_intro"
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "spark_introduction_and_architecture"
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "spark_streaming_applications"
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "spark_ml_lib_for_machine_learning"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "advanced_spark_optimization_and_tuning"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "building_complex_etl_pipelines_with_spark"
                  }
                ]
              },
              {
                "id": "DS.BD.DDPAS.FPDP",
                "name": "Nguyên lý lập trình hàm và xử lý dữ liệu (Principles of Functional Programming and Data Processing)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các nguyên lý cốt lõi của lập trình hàm, bao gồm tính bất biến (immutability), hàm thuần túy (pure functions) và các hàm bậc cao (higher-order functions). Nắm vững lập trình hàm là nền tảng để viết mã hiệu quả, dễ bảo trì, và song song hóa tốt trong các hệ thống xử lý dữ liệu lớn như Apache Spark.",
                "keywords": [
                  "Lập trình hàm",
                  "Functional Programming",
                  "Tính bất biến",
                  "Immutability",
                  "Hàm thuần túy",
                  "Pure Functions",
                  "Hàm bậc cao",
                  "Higher-Order Functions",
                  "Xử lý dữ liệu song song",
                  "Apache Spark",
                  "Biến đổi dữ liệu",
                  "Data Transformations"
                ],
                "learningResources": [
                  {
                    "name": "Apache Spark RDD Programming Guide",
                    "url": "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
                  },
                  {
                    "name": "Real Python: Functional Programming in Python",
                    "url": "https://realpython.com/python-functional-programming/"
                  },
                  {
                    "name": "Coursera: Functional Programming Principles in Scala (by Martin Odersky)",
                    "url": "https://www.coursera.org/learn/scala-functional-programming"
                  }
                ],
                "prerequisites": [
                  {
                    "id": "DS.BD.Prog.Fundamentals",
                    "name": "Kiến thức lập trình cơ bản"
                  },
                  {
                    "id": "DS.BD.Prog.PythonBasics",
                    "name": "Lập trình Python cơ bản"
                  }
                ],
                "projectIdeas": [
                  "Xây dựng một chương trình Python nhỏ sử dụng các nguyên lý lập trình hàm (map, filter, reduce) để phân tích một tập dữ liệu văn bản đơn giản (ví dụ: đếm tần suất từ).",
                  "Chuyển đổi một đoạn mã xử lý dữ liệu imperative (ví dụ: dùng vòng lặp for) sang phong cách lập trình hàm, sử dụng các hàm bất biến và thuần túy để xử lý một DataFrame Pandas."
                ],
                "relatedJobTitles": [
                  "Kỹ sư dữ liệu (Data Engineer)",
                  "Nhà khoa học dữ liệu (Data Scientist)",
                  "Kỹ sư dữ liệu lớn (Big Data Engineer)",
                  "Kỹ sư học máy (Machine Learning Engineer)",
                  "Kỹ sư phần mềm Backend (Backend Software Engineer)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Apache Spark",
                  "Scala",
                  "Python",
                  "Jupyter Notebook",
                  "Apache Zeppelin"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-50 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Software Engineer (Backend/Distributed Systems)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS.BD.Prog.Fundamentals",
                    "type": "prerequisite",
                    "level": "conceptual"
                  },
                  {
                    "id": "DS.BD.Prog.PythonBasics",
                    "type": "prerequisite",
                    "level": "conceptual"
                  },
                  {
                    "id": "DS.BD.DDPAS.SparkCoreAPIs",
                    "type": "complementary",
                    "level": "conceptual"
                  },
                  {
                    "id": "DS.BD.Prog.Scala",
                    "type": "complementary",
                    "level": "conceptual"
                  },
                  {
                    "id": "DS.BD.DDPAS.AdvancedSparkDevelopment",
                    "type": "unlocks",
                    "level": "practical"
                  },
                  {
                    "id": "DS.BD.DDPAS.PerformanceOptimization",
                    "type": "unlocks",
                    "level": "practical"
                  }
                ]
              }
            ],
            "skillName": "Xử lý Dữ liệu Phân tán với Apache Spark (Distributed Data Processing with Apache Spark)",
            "competency": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
            "description": "Kỹ năng này trang bị cho bạn khả năng xử lý và phân tích hiệu quả các tập dữ liệu lớn trên các cụm máy tính phân tán. Bạn sẽ nắm vững các khái niệm cốt lõi, cấu trúc dữ liệu và API của Apache Spark để phát triển các ứng dụng xử lý dữ liệu có khả năng mở rộng. Điều này cho phép bạn thiết kế và triển khai các giải pháp mạnh mẽ cho các thách thức dữ liệu lớn bằng cách sử dụng các mô hình phân tán.",
            "learningResources": [
              {
                "title": "Apache Spark in 24 Hours, Sams Teach Yourself - Chapter on 'Spark Core and Distributed Data Processing'",
                "type": "book_chapter",
                "url": "https://www.amazon.com/Apache-Spark-Hours-Teach-Yourself/dp/0672337777"
              },
              {
                "title": "Databricks Academy: Apache Spark Programming (Comprehensive Course/Playlist)",
                "type": "video_playlist",
                "url": "https://databricks.com/learn/training/apache-spark-programming"
              }
            ],
            "projectIdeas": [
              {
                "title": "Hệ thống Phân tích Dữ liệu Nhật ký Web Phân tán",
                "description": "Xây dựng một ứng dụng Spark để thu thập, làm sạch và phân tích các tệp nhật ký web lớn (log files) được lưu trữ trên hệ thống lưu trữ phân tán như HDFS hoặc S3. Dự án này sẽ yêu cầu sử dụng RDDs hoặc DataFrames để thực hiện các tác vụ ETL (Extract, Transform, Load), tính toán các chỉ số quan trọng (ví dụ: số lượt truy cập duy nhất, các trang được truy cập nhiều nhất, thời gian truy cập cao điểm) và lưu trữ kết quả phân tích vào một kho dữ liệu phân tán khác để truy vấn."
              }
            ],
            "tools": [
              "Apache Spark",
              "Hadoop Distributed File System (HDFS)",
              "Scala",
              "Python (PySpark)",
              "Java",
              "Jupyter Notebook",
              "Apache Zeppelin",
              "YARN / Kubernetes (Cluster Managers)"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "40-60 hours",
            "importanceScore": 9
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_3_3",
            "name": "Xây dựng và Vận hành Dòng dữ liệu (Data Pipelines and ETL/ELT Development)",
            "type": "skill",
            "children": [
              {
                "id": "data-pipelines-principles-architecture-etl-elt",
                "name": "Nguyên lý và Kiến trúc Dòng dữ liệu (Data Pipelines), ETL/ELT",
                "type": "knowledge",
                "children": [],
                "title": "Nguyên lý và Kiến trúc Dòng dữ liệu (Data Pipelines), ETL/ELT",
                "description": "Kiến thức này bao gồm các nguyên tắc cơ bản, thiết kế và triển khai các hệ thống dòng dữ liệu (data pipelines) để di chuyển, biến đổi và tải dữ liệu từ nhiều nguồn khác nhau. Nó tập trung vào các mô hình ETL (Extract, Transform, Load) và ELT (Extract, Load, Transform) cũng như kiến trúc tổng thể để đảm bảo dữ liệu sạch, đáng tin cậy và sẵn sàng cho phân tích. Đây là nền tảng cốt lõi để xây dựng các giải pháp dữ liệu hiệu quả trong môi trường dữ liệu lớn và khoa học dữ liệu.",
                "keywords": [
                  "Data Pipeline",
                  "ETL",
                  "ELT",
                  "Data Integration",
                  "Data Ingestion",
                  "Data Transformation",
                  "Data Loading",
                  "Data Architecture",
                  "Workflow Orchestration",
                  "Batch Processing",
                  "Stream Processing"
                ],
                "learningResources": [
                  {
                    "name": "Apache Airflow Documentation: DAGs",
                    "url": "https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html"
                  },
                  {
                    "name": "AWS Glue Documentation: What is AWS Glue?",
                    "url": "https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html"
                  },
                  {
                    "name": "Fivetran Blog: What is a Data Pipeline?",
                    "url": "https://www.fivetran.com/blog/what-is-a-data-pipeline"
                  }
                ],
                "prerequisites": [
                  "programming-fundamentals",
                  "database-fundamentals",
                  "data-modeling-basics",
                  "cloud-computing-basics"
                ],
                "projectIdeas": [
                  "Xây dựng một Data Pipeline đơn giản để trích xuất dữ liệu từ API công khai (ví dụ: dữ liệu thời tiết, tỷ giá hối đoái), biến đổi dữ liệu (làm sạch, chuẩn hóa) và tải vào một cơ sở dữ liệu (PostgreSQL/MongoDB).",
                  "Thiết kế và triển khai một quy trình ETL/ELT để tổng hợp dữ liệu bán hàng từ các file CSV và một CSDL quan hệ, sau đó tải vào một kho dữ liệu (Data Warehouse) đơn giản để phân tích và báo cáo."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "ETL Developer",
                  "Big Data Engineer",
                  "Cloud Data Engineer",
                  "Data Architect",
                  "Analytics Engineer",
                  "Data Scientist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Airflow",
                  "Apache Kafka",
                  "Apache Spark",
                  "AWS Glue",
                  "Azure Data Factory",
                  "Google Cloud Dataflow",
                  "dbt (data build tool)",
                  "Fivetran",
                  "Talend",
                  "Informatica PowerCenter",
                  "Python"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-60 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Cloud Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Data Architecture",
                    "score": 9
                  },
                  {
                    "path": "Analytics Engineering",
                    "score": 9
                  },
                  {
                    "path": "Data Science",
                    "score": 7
                  },
                  {
                    "path": "Machine Learning Engineering",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "programming-fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "database-fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-modeling-basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "cloud-computing-basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-warehousing-principles",
                    "type": "complementary"
                  },
                  {
                    "id": "stream-processing-frameworks",
                    "type": "complementary"
                  },
                  {
                    "id": "data-quality-monitoring",
                    "type": "complementary"
                  },
                  {
                    "id": "orchestration-scheduling-tools",
                    "type": "unlocks"
                  },
                  {
                    "id": "real-time-data-processing",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "data-storage-data-warehousing",
                "name": "Các Hệ thống Lưu trữ Dữ liệu và Kho dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này bao gồm các khái niệm cơ bản, kiến trúc và công nghệ đằng sau việc lưu trữ, tổ chức và quản lý một lượng lớn dữ liệu một cách hiệu quả. Nó cực kỳ quan trọng để thiết kế cơ sở hạ tầng dữ liệu mạnh mẽ, cho phép phân tích dữ liệu chuyên sâu, báo cáo đáng tin cậy và hỗ trợ các quyết định dựa trên dữ liệu trong các tổ chức.",
                "keywords": [
                  "Data Storage",
                  "Data Warehousing",
                  "ETL",
                  "ELT",
                  "Data Lake",
                  "Data Lakehouse",
                  "OLTP",
                  "OLAP",
                  "Dimensional Modeling",
                  "Star Schema",
                  "Snowflake Schema",
                  "Data Mart"
                ],
                "learningResources": [
                  {
                    "name": "What is Data Warehousing?",
                    "url": "https://www.ibm.com/topics/data-warehousing"
                  },
                  {
                    "name": "Amazon Redshift Documentation",
                    "url": "https://docs.aws.amazon.com/redshift/latest/dg/welcome.html"
                  },
                  {
                    "name": "Data Warehouse vs. Data Lake: What's the difference?",
                    "url": "https://cloud.google.com/learn/data-warehouse-vs-data-lake"
                  }
                ],
                "prerequisites": [
                  "basic-database-concepts",
                  "sql-fundamentals"
                ],
                "projectIdeas": [
                  "Thiết kế một kho dữ liệu đơn giản (ví dụ: dữ liệu bán hàng, khách hàng) cho một hệ thống bán lẻ nhỏ, sử dụng mô hình chiều (Dimensional Modeling) và triển khai trên một cơ sở dữ liệu quan hệ (ví dụ PostgreSQL hoặc MySQL).",
                  "Xây dựng một pipeline ETL/ELT cơ bản để di chuyển dữ liệu từ một nguồn giao dịch (OLTP) sang một kho dữ liệu (OLAP) và tạo các báo cáo tổng hợp bằng SQL."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "BI Developer",
                  "Data Architect",
                  "Analytics Engineer",
                  "Database Administrator"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "SQL Databases (PostgreSQL, MySQL, SQL Server)",
                  "Data Warehouse Platforms (Snowflake, Google BigQuery, AWS Redshift, Azure Synapse Analytics)",
                  "Data Lake Technologies (Apache Hadoop HDFS, AWS S3, Azure Data Lake Storage)",
                  "ETL/ELT Tools (Apache Airflow, Talend, DBT, Azure Data Factory, AWS Glue)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-60 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Architect",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "id": "basic-database-concepts",
                    "type": "prerequisite"
                  },
                  {
                    "id": "sql-fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-modeling-design",
                    "type": "complementary"
                  },
                  {
                    "id": "etl-elt-development",
                    "type": "complementary"
                  },
                  {
                    "id": "cloud-data-warehousing",
                    "type": "unlocks"
                  },
                  {
                    "id": "big-data-storage-processing",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "DS_BD_BDPT",
                "name": "Các Nền tảng và Công nghệ Xử lý Dữ liệu lớn",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị hiểu biết sâu sắc về các nền tảng và công nghệ tiên tiến được sử dụng để lưu trữ, xử lý và phân tích dữ liệu ở quy mô lớn. Nó bao gồm các hệ thống phân tán, framework xử lý theo lô (batch) và thời gian thực (real-time) cần thiết để xây dựng các giải pháp dữ liệu lớn mạnh mẽ và hiệu quả, giải quyết các thách thức về khối lượng, tốc độ và sự đa dạng của dữ liệu. Nắm vững các công nghệ này là cốt lõi để xây dựng và vận hành các dòng dữ liệu (data pipelines) hiện đại.",
                "keywords": [
                  "Big Data",
                  "Apache Hadoop",
                  "Apache Spark",
                  "Apache Kafka",
                  "Distributed Systems",
                  "Data Lake",
                  "Stream Processing",
                  "Batch Processing",
                  "NoSQL",
                  "Cloud Big Data Services",
                  "Data Pipelines"
                ],
                "learningResources": [
                  {
                    "name": "Apache Spark Documentation",
                    "url": "https://spark.apache.org/docs/latest/"
                  },
                  {
                    "name": "Apache Kafka Documentation",
                    "url": "https://kafka.apache.org/documentation/"
                  },
                  {
                    "name": "Google Cloud Data Engineer Professional Certificate (Coursera)",
                    "url": "https://www.coursera.org/professional-certificates/google-cloud-data-engineer"
                  }
                ],
                "prerequisites": [
                  "DS_BigData_Fundamentals",
                  "CS_DistributedSystems_Intro",
                  "PROG_Python_Intermediate",
                  "DB_SQL_Fundamentals"
                ],
                "projectIdeas": [
                  "Xây dựng hệ thống phân tích dữ liệu nhật ký web quy mô lớn bằng Apache Spark: Thu thập dữ liệu nhật ký web, xử lý bằng Spark để trích xuất các chỉ số (ví dụ: lượt truy cập, trang phổ biến, lỗi) và lưu trữ kết quả vào một kho dữ liệu.",
                  "Thiết lập pipeline xử lý sự kiện thời gian thực với Kafka và Flink/Spark Streaming: Mô phỏng dữ liệu sự kiện (ví dụ: giao dịch, cảm biến IoT) vào Kafka, sau đó sử dụng Flink hoặc Spark Streaming để xử lý, biến đổi và lưu trữ dữ liệu đã xử lý vào một cơ sở dữ liệu thời gian thực."
                ],
                "relatedJobTitles": [
                  "Big Data Engineer",
                  "Data Engineer",
                  "Cloud Data Engineer",
                  "Big Data Architect",
                  "Data Solutions Architect"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Hadoop (HDFS, YARN, MapReduce)",
                  "Apache Spark",
                  "Apache Kafka",
                  "Apache Flink",
                  "Apache Hive",
                  "Apache Cassandra",
                  "AWS EMR",
                  "GCP Dataproc",
                  "Azure HDInsight",
                  "Databricks",
                  "Google Cloud Dataflow"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "60-100 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Architect",
                    "score": 10
                  },
                  {
                    "path": "Cloud Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS_BigData_Fundamentals",
                    "type": "prerequisite",
                    "description": "Cung cấp các khái niệm nền tảng về dữ liệu lớn và các thách thức liên quan."
                  },
                  {
                    "id": "CS_DistributedSystems_Intro",
                    "type": "prerequisite",
                    "description": "Giúp hiểu rõ kiến trúc và nguyên lý hoạt động của các hệ thống phân tán."
                  },
                  {
                    "id": "DS_DataPipelines_ETLDev",
                    "type": "complementary",
                    "description": "Là kiến thức cốt lõi để thực hiện việc xây dựng và vận hành các dòng dữ liệu ETL/ELT."
                  },
                  {
                    "id": "DS_AdvancedDataEngineering",
                    "type": "unlocks",
                    "description": "Mở ra cơ hội học tập các kỹ thuật và tối ưu hóa Kỹ thuật Dữ liệu nâng cao."
                  },
                  {
                    "id": "DS_RealtimeAnalytics",
                    "type": "unlocks",
                    "description": "Cho phép phát triển các hệ thống phân tích và báo cáo dữ liệu theo thời gian thực."
                  },
                  {
                    "id": "DS_ML_BigData",
                    "type": "unlocks",
                    "description": "Tạo tiền đề để xây dựng và triển khai các mô hình học máy trên các tập dữ liệu lớn."
                  }
                ]
              },
              {
                "id": "data_science_big_data.data_pipelines_etl_elt.workflow_management_orchestration",
                "name": "Hệ thống Quản lý và Điều phối luồng công việc (Workflow Management & Orchestration Systems)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các hệ thống tự động hóa, lập lịch và giám sát các tác vụ (tasks) trong một luồng công việc phức tạp, đặc biệt là trong các pipeline dữ liệu. Việc nắm vững các hệ thống này là tối quan trọng để đảm bảo tính nhất quán, khả năng mở rộng và độ tin cậy của các quy trình xử lý dữ liệu từ khâu thu thập đến phân tích và báo cáo.",
                "keywords": [
                  "Workflow Orchestration",
                  "ETL Automation",
                  "DAG (Directed Acyclic Graph)",
                  "Task Scheduling",
                  "Dependency Management",
                  "Data Pipeline Management",
                  "Distributed Workflow",
                  "Fault Tolerance",
                  "Monitoring & Alerting"
                ],
                "learningResources": [
                  {
                    "name": "Apache Airflow Documentation",
                    "url": "https://airflow.apache.org/docs/",
                    "type": "Official Documentation"
                  },
                  {
                    "name": "Prefect Documentation",
                    "url": "https://docs.prefect.io/",
                    "type": "Official Documentation"
                  },
                  {
                    "name": "Practical Workflow Automation with Apache Airflow (Pluralsight/Coursera)",
                    "url": "https://www.coursera.org/learn/practical-workflow-automation-apache-airflow",
                    "type": "Online Course"
                  }
                ],
                "prerequisites": [
                  "data_science_big_data.data_pipelines_etl_elt.fundamentals_of_data_pipelines",
                  "data_science_big_data.data_pipelines_etl_elt.etl_elt_methodologies",
                  "core_programming.python.advanced_concepts"
                ],
                "projectIdeas": [
                  "Xây dựng một DAG Airflow để tự động hóa quá trình trích xuất dữ liệu từ API công cộng, chuyển đổi và tải vào cơ sở dữ liệu (ví dụ: PostgreSQL).",
                  "Phát triển một hệ thống điều phối sử dụng Prefect để quản lý các tác vụ tiền xử lý, huấn luyện mô hình và triển khai một mô hình Machine Learning đơn giản."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "MLOps Engineer",
                  "Cloud Data Engineer",
                  "Data Architect"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Airflow",
                  "Prefect",
                  "Dagster",
                  "Luigi",
                  "AWS Step Functions",
                  "Azure Data Factory",
                  "Google Cloud Composer"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-60 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Big Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "MLOps Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Architect",
                    "score": 9
                  },
                  {
                    "path": "Cloud Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data_science_big_data.data_pipelines_etl_elt.fundamentals_of_data_pipelines",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_big_data.data_pipelines_etl_elt.etl_elt_methodologies",
                    "type": "prerequisite"
                  },
                  {
                    "id": "core_programming.python.advanced_concepts",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_big_data.data_pipelines_etl_elt.data_quality_monitoring",
                    "type": "complementary"
                  },
                  {
                    "id": "data_science_big_data.data_pipelines_etl_elt.data_pipeline_monitoring_logging",
                    "type": "unlocks"
                  },
                  {
                    "id": "data_science_big_data.data_pipelines_etl_elt.advanced_data_pipeline_patterns",
                    "type": "unlocks"
                  }
                ]
              }
            ],
            "skillName": "Xây dựng và Vận hành Dòng dữ liệu (Data Pipelines and ETL/ELT Development)",
            "competencyName": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
            "description": "Kỹ năng này cho phép bạn thiết kế, xây dựng và vận hành các hệ thống tự động hóa quá trình thu thập, làm sạch, biến đổi và tải dữ liệu từ nhiều nguồn khác nhau vào kho lưu trữ tập trung. Bạn sẽ có khả năng đảm bảo dữ liệu luôn sẵn sàng, chính xác và nhất quán, hỗ trợ hiệu quả cho các hoạt động phân tích, báo cáo và học máy, đặc biệt trong môi trường dữ liệu lớn.",
            "learningResources": [
              {
                "title": "Chương 4: Kiến trúc ETL/ELT từ Sách 'The Data Warehouse Toolkit' của Ralph Kimball",
                "type": "Sách/Chương sách",
                "url": "https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/books/data-warehouse-toolkit-books/"
              },
              {
                "title": "Playlist 'Data Engineering Bootcamp' (ví dụ từ Datacamp, Udacity hoặc các kênh chuyên về Data Engineering)",
                "type": "Video Playlist/Khóa học trực tuyến",
                "url": "https://www.youtube.com/results?search_query=data+engineering+bootcamp+playlist"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng Dòng dữ liệu Chứng khoán thời gian thực/gần thực",
                "description": "Thiết kế và triển khai một dòng dữ liệu (data pipeline) để thu thập dữ liệu giá chứng khoán từ một API công cộng (ví dụ: Alpha Vantage, Finnhub). Dữ liệu sau đó sẽ được biến đổi (tính toán các chỉ số đơn giản như EMA, SMA), lưu trữ vào một Data Lake (ví dụ: Amazon S3, Google Cloud Storage), và cuối cùng tải vào một Data Warehouse (ví dụ: Snowflake, Google BigQuery, Redshift) để sẵn sàng cho phân tích. Sử dụng một công cụ điều phối luồng công việc (ví dụ: Apache Airflow) để tự động hóa toàn bộ quá trình."
              }
            ],
            "tools": [
              "Apache Airflow",
              "Apache Spark",
              "Apache Kafka",
              "AWS Glue",
              "Google Cloud Dataflow",
              "Azure Data Factory",
              "Python (Pandas, Dask)",
              "SQL",
              "Docker",
              "Kubernetes",
              "Snowflake",
              "Amazon S3/Google Cloud Storage/Azure Data Lake Storage",
              "Apache Hadoop",
              "Delta Lake/Iceberg/Hudi"
            ],
            "difficultyLevel": "Nâng cao",
            "estimatedTimeToComplete": "80-120 giờ",
            "importanceScore": 5
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_3_4",
            "name": "Quản lý và Tối ưu hóa Hồ dữ liệu/Kho dữ liệu trên Nền tảng Đám mây (Cloud Data Lake/Warehouse Management and Optimization)",
            "type": "skill",
            "children": [
              {
                "id": "cloud-data-lake-warehouse-architecture-principles",
                "name": "Kiến trúc và Các nguyên tắc cơ bản của Data Lake và Data Warehouse trên đám mây",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào việc tìm hiểu sự khác biệt cơ bản về kiến trúc, các mẫu thiết kế và nguyên tắc vận hành của Data Lake và Data Warehouse khi được triển khai trong môi trường đám mây. Việc nắm vững kiến thức này là rất quan trọng để thiết kế các nền tảng dữ liệu có khả năng mở rộng, tiết kiệm chi phí và hiệu suất cao, hỗ trợ tốt cho các tác vụ phân tích và Machine Learning/AI, giúp các tổ chức khai thác thông tin chuyên sâu từ khối lượng dữ liệu khổng lồ.",
                "keywords": [
                  "Data Lake",
                  "Data Warehouse",
                  "Cloud Architecture",
                  "AWS S3",
                  "Azure Data Lake Storage",
                  "Google Cloud Storage",
                  "OLAP",
                  "ELT/ETL",
                  "Schema-on-read",
                  "Schema-on-write",
                  "Data Governance",
                  "Scalability"
                ],
                "learningResources": [
                  {
                    "name": "Building a Data Lake on Amazon S3",
                    "url": "https://aws.amazon.com/blogs/big-data/building-a-data-lake-on-amazon-s3/"
                  },
                  {
                    "name": "Introduction to Azure Data Lake Storage Gen2",
                    "url": "https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction"
                  },
                  {
                    "name": "Data Lake vs. Data Warehouse: What's the difference?",
                    "url": "https://cloud.google.com/blog/products/data-analytics/data-lake-vs-data-warehouse"
                  }
                ],
                "prerequisites": [
                  "cloud-computing-fundamentals",
                  "database-fundamentals"
                ],
                "projectIdeas": [
                  "Thiết kế kiến trúc dữ liệu lý thuyết cho một công ty bán lẻ sử dụng data lake và data warehouse trên đám mây, phác thảo luồng dữ liệu, các tầng lưu trữ và dịch vụ tính toán.",
                  "Phác thảo một quy trình ELT đơn giản trên một nền tảng đám mây (ví dụ: AWS S3 + Glue + Redshift hoặc Azure Data Lake + Data Factory + Synapse) để nhập và chuyển đổi dữ liệu mẫu vào một kho dữ liệu nhỏ."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Cloud Data Architect",
                  "Data Platform Engineer",
                  "Big Data Engineer",
                  "Analytics Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "AWS S3",
                  "Azure Data Lake Storage",
                  "Google Cloud Storage",
                  "Amazon Redshift",
                  "Azure Synapse Analytics",
                  "Google BigQuery",
                  "Snowflake",
                  "Databricks",
                  "Apache Spark"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 10,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Cloud Architecture",
                    "score": 9
                  },
                  {
                    "path": "Big Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Data Science",
                    "score": 7
                  },
                  {
                    "path": "Data Analytics",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "cloud-computing-fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "id": "database-fundamentals"
                  },
                  {
                    "type": "unlocks",
                    "id": "cloud-data-lake-implementation"
                  },
                  {
                    "type": "unlocks",
                    "id": "cloud-data-warehouse-implementation"
                  },
                  {
                    "type": "unlocks",
                    "id": "data-governance-on-cloud"
                  },
                  {
                    "type": "complementary",
                    "id": "data-modeling-for-big-data"
                  },
                  {
                    "type": "complementary",
                    "id": "data-security-on-cloud"
                  }
                ]
              },
              {
                "id": "core_cloud_data_services",
                "name": "Các dịch vụ lưu trữ và xử lý dữ liệu đám mây cốt lõi (ví dụ: AWS S3/Glue/Athena/Redshift, Azure Data Lake Storage/Synapse, GCP GCS/BigQuery)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này bao gồm việc tìm hiểu và thực hành các dịch vụ cốt lõi của các nhà cung cấp đám mây hàng đầu để lưu trữ và xử lý dữ liệu quy mô lớn. Nắm vững các dịch vụ này là nền tảng để xây dựng và quản lý hiệu quả các hệ thống hồ dữ liệu và kho dữ liệu hiện đại, hỗ trợ các phân tích phức tạp và ứng dụng AI/ML.",
                "keywords": [
                  "Cloud Data Storage",
                  "Cloud Data Processing",
                  "AWS S3",
                  "AWS Glue",
                  "AWS Athena",
                  "AWS Redshift",
                  "Azure Data Lake Storage",
                  "Azure Synapse Analytics",
                  "Google Cloud Storage",
                  "Google BigQuery",
                  "Data Lake",
                  "Data Warehouse",
                  "ETL",
                  "Serverless Analytics"
                ],
                "learningResources": [
                  {
                    "name": "AWS Storage Services Documentation (S3)",
                    "url": "https://docs.aws.amazon.com/s3/?id=docs_gateway"
                  },
                  {
                    "name": "Azure Data Lake Storage Gen2 documentation",
                    "url": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction"
                  },
                  {
                    "name": "Google Cloud BigQuery Documentation",
                    "url": "https://cloud.google.com/bigquery/docs"
                  }
                ],
                "prerequisites": [
                  "cloud_computing_basics",
                  "database_fundamentals",
                  "data_warehousing_concepts"
                ],
                "projectIdeas": [
                  "Xây dựng một Data Lake đơn giản: Ingest dữ liệu từ các nguồn khác nhau (CSV, JSON) vào AWS S3/Azure Data Lake Storage/GCP GCS, sau đó sử dụng AWS Glue/Azure Synapse Analytics/GCP BigQuery để tạo catalog và truy vấn dữ liệu.",
                  "Thực hiện ETL trên Cloud: Thiết kế và triển khai một quy trình ETL cơ bản sử dụng AWS Glue/Azure Synapse Pipelines/GCP Dataflow để chuyển đổi dữ liệu từ S3/ADLS/GCS và tải vào AWS Redshift/Azure Synapse Dedicated SQL Pool/GCP BigQuery."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Cloud Data Engineer",
                  "Big Data Engineer",
                  "Data Architect",
                  "Analytics Engineer",
                  "Cloud Solution Architect"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "AWS S3",
                  "AWS Glue",
                  "AWS Athena",
                  "AWS Redshift",
                  "Azure Data Lake Storage Gen2",
                  "Azure Synapse Analytics",
                  "Google Cloud Storage",
                  "Google BigQuery"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "40-60 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Cloud Architecture",
                    "score": 9
                  },
                  {
                    "path": "Big Data Development",
                    "score": 10
                  },
                  {
                    "path": "Data Science (Data Ops focus)",
                    "score": 7
                  },
                  {
                    "path": "Business Intelligence Development",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "cloud_computing_basics"
                  },
                  {
                    "type": "prerequisite",
                    "id": "database_fundamentals"
                  },
                  {
                    "type": "complementary",
                    "id": "etl_data_pipeline_design"
                  },
                  {
                    "type": "complementary",
                    "id": "data_modeling_techniques"
                  },
                  {
                    "type": "unlocks",
                    "id": "data_lake_architecture_and_design"
                  },
                  {
                    "type": "unlocks",
                    "id": "cloud_data_warehouse_optimization"
                  },
                  {
                    "type": "unlocks",
                    "id": "real_time_data_streaming_platforms"
                  }
                ]
              },
              {
                "id": "DS-BD-DL-DW-OPTIM-DATA-FORMATS",
                "name": "Định dạng dữ liệu tối ưu hóa cho phân tích lớn (ví dụ: Parquet, ORC, Delta Lake, Iceberg, Hudi)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các định dạng tệp và bảng được thiết kế đặc biệt để tối ưu hóa việc lưu trữ và truy vấn dữ liệu lớn. Việc sử dụng các định dạng này giúp cải thiện đáng kể hiệu suất phân tích, giảm chi phí lưu trữ và mang lại các tính năng quan trọng như giao dịch ACID và tiến hóa lược đồ trong môi trường hồ dữ liệu.",
                "keywords": [
                  "Parquet",
                  "ORC",
                  "Delta Lake",
                  "Apache Iceberg",
                  "Apache Hudi",
                  "Columnar Storage",
                  "Data Lake Optimization",
                  "Big Data Analytics",
                  "ACID Transactions",
                  "Schema Evolution",
                  "Data Versioning",
                  "Query Performance"
                ],
                "learningResources": [
                  {
                    "name": "Apache Parquet - Official Website",
                    "url": "https://parquet.apache.org/"
                  },
                  {
                    "name": "Delta Lake - Documentation",
                    "url": "https://docs.delta.io/"
                  },
                  {
                    "name": "Apache Iceberg - Documentation",
                    "url": "https://iceberg.apache.org/docs/latest/"
                  }
                ],
                "prerequisites": [
                  "DS-BD-INTRO-BIGDATA",
                  "DS-BD-DL-DW-CONCEPTS",
                  "DS-BD-CLOUD-STORAGE"
                ],
                "projectIdeas": [
                  "Chuyển đổi một tập dữ liệu CSV/JSON lớn sang Parquet/ORC và so sánh hiệu suất truy vấn trên Apache Spark. Đồng thời, áp dụng nén và phân vùng để đánh giá tác động.",
                  "Xây dựng một hồ dữ liệu nhỏ sử dụng Delta Lake hoặc Iceberg, triển khai các thao tác như upsert, xóa, và tiến hóa lược đồ để mô phỏng một quy trình ETL thực tế."
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Big Data Engineer",
                  "Data Architect",
                  "Cloud Data Engineer",
                  "Analytics Engineer",
                  "MLOps Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Apache Spark",
                  "Apache Hive",
                  "Apache Flink",
                  "AWS Glue",
                  "Azure Synapse Analytics",
                  "Google BigQuery (external tables)",
                  "Databricks Lakehouse Platform",
                  "Snowflake (external tables)",
                  "Trino (Presto)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-30 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Architect",
                    "score": 10
                  },
                  {
                    "path": "Cloud Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Analytics Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Scientist",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetSkillId": "DS-BD-INTRO-BIGDATA",
                    "description": "Cần hiểu về các thách thức và khái niệm cơ bản của Dữ liệu lớn để nhận ra giá trị của các định dạng tối ưu."
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "DS-BD-DL-DW-CONCEPTS",
                    "description": "Cần có kiến thức về cấu trúc và mục tiêu của Hồ dữ liệu/Kho dữ liệu trên đám mây để hiểu cách các định dạng này tích hợp và cải thiện chúng."
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "DS-BD-CLOUD-STORAGE",
                    "description": "Cần hiểu cách lưu trữ dữ liệu trên các dịch vụ đám mây (ví dụ: S3, ADLS, GCS) vì đây là nơi các tệp định dạng này thường được lưu trữ."
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "DS-BD-DISTRIBUTED-COMPUTING",
                    "description": "Kiến thức về các nền tảng điện toán phân tán (ví dụ: Spark) giúp tận dụng tối đa hiệu suất của các định dạng dữ liệu này."
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "DS-BD-DATA-PIPELINES",
                    "description": "Việc triển khai các định dạng này là một phần cốt lõi trong việc xây dựng các đường ống dữ liệu hiệu quả và đáng tin cậy."
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS-BD-DL-DW-ADV-OPTIM",
                    "description": "Nắm vững các định dạng này là nền tảng để học và triển khai các kỹ thuật tối ưu hóa hồ dữ liệu/kho dữ liệu nâng cao."
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS-BD-REALTIME-ANALYTICS",
                    "description": "Các định dạng bảng như Delta Lake, Iceberg, Hudi là chìa khóa để xây dựng các giải pháp phân tích gần thời gian thực trên hồ dữ liệu."
                  }
                ]
              },
              {
                "id": "cloud_data_performance_cost_lifecycle_optimization",
                "name": "Nguyên tắc tối ưu hóa hiệu suất truy vấn, quản lý chi phí và quản lý vòng đời dữ liệu trên nền tảng đám mây",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các chiến lược và kỹ thuật để tối đa hóa hiệu quả hoạt động, kiểm soát chi phí và quản lý hiệu quả dữ liệu trong suốt vòng đời của nó trên các nền tảng đám mây. Nó bao gồm việc tinh chỉnh các truy vấn, lựa chọn cấu hình lưu trữ phù hợp và tự động hóa các chính sách dịch chuyển/xóa dữ liệu để đảm bảo hệ thống dữ liệu đám mây luôn hoạt động tối ưu và tiết kiệm. Nắm vững các nguyên tắc này là cực kỳ quan trọng để xây dựng và duy trì các giải pháp dữ liệu đám mây bền vững và hiệu quả về chi phí.",
                "keywords": [
                  "Cloud Query Optimization",
                  "Cost Management",
                  "Data Lifecycle Management",
                  "Cloud Data Warehouse",
                  "Cloud Data Lake",
                  "Performance Tuning",
                  "Serverless Computing",
                  "Storage Tiers",
                  "Data Archiving",
                  "Data Retention",
                  "Cost Explorer",
                  "Data Governance"
                ],
                "learningResources": [
                  {
                    "title": "Amazon Redshift Query Performance Tuning Best Practices",
                    "url": "https://docs.aws.amazon.com/redshift/latest/dg/c_tune_query_performance.html"
                  },
                  {
                    "title": "Google Cloud BigQuery Cost Control and Best Practices",
                    "url": "https://cloud.google.com/bigquery/docs/best-practices-costs"
                  },
                  {
                    "title": "Azure Synapse Analytics SQL Pool Performance and Cost Optimization",
                    "url": "https://learn.microsoft.com/en-us/azure/synapse-analytics/guidance/sql-pool-optimization-performance-tuning"
                  }
                ],
                "prerequisites": [
                  "cloud_fundamentals",
                  "data_warehousing_fundamentals",
                  "data_lake_fundamentals",
                  "sql_querying_advanced",
                  "big_data_concepts"
                ],
                "projectIdeas": [
                  {
                    "title": "Phân tích và Tối ưu hóa Truy vấn trên Nền tảng Đám mây",
                    "description": "Chọn một bộ dữ liệu công khai lớn (ví dụ: trên AWS S3, GCP Public Datasets), viết một số truy vấn phức tạp trên một nền tảng DW/DL đám mây (BigQuery/Redshift/Synapse). Sử dụng các công cụ giám sát hiệu suất và chi phí của nền tảng để xác định điểm nghẽn, sau đó tối ưu hóa các truy vấn đó để giảm thời gian chạy và chi phí."
                  },
                  {
                    "title": "Thiết kế Chính sách Vòng đời Dữ liệu và Ước tính Chi phí",
                    "description": "Với một kịch bản về hệ thống hồ dữ liệu lưu trữ dữ liệu nhật ký và dữ liệu lịch sử, thiết kế một chính sách vòng đời dữ liệu hoàn chỉnh. Chính sách này phải bao gồm việc chuyển đổi dữ liệu qua các tầng lưu trữ khác nhau (ví dụ: Standard -> Infrequent Access -> Archive) và thiết lập thời gian lưu trữ/xóa cụ thể. Sau đó, ước tính chi phí tiết kiệm được từ việc áp dụng chính sách này so với việc lưu trữ tất cả dữ liệu ở tầng tiêu chuẩn."
                  }
                ],
                "relatedJobTitles": [
                  "Data Engineer",
                  "Cloud Data Architect",
                  "Big Data Engineer",
                  "Data Platform Engineer",
                  "Solutions Architect (Data)",
                  "Database Administrator (Cloud-focused)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Amazon Redshift",
                  "Google BigQuery",
                  "Azure Synapse Analytics",
                  "Amazon S3",
                  "Azure Data Lake Storage",
                  "Google Cloud Storage",
                  "AWS Athena",
                  "AWS Cost Explorer",
                  "Azure Cost Management",
                  "Google Cloud Billing",
                  "Amazon CloudWatch",
                  "Azure Monitor",
                  "Google Cloud Operations Suite",
                  "SQL Clients (DBeaver, DataGrip)"
                ],
                "difficultyLevel": "Advanced",
                "estimatedTimeToComplete": "25-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Engineering",
                    "score": 10
                  },
                  {
                    "path": "Cloud Architecture",
                    "score": 10
                  },
                  {
                    "path": "Big Data Development",
                    "score": 9
                  },
                  {
                    "path": "Data Platform Management",
                    "score": 10
                  },
                  {
                    "path": "Data Science",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "cloud_fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data_warehousing_fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "id": "data_lake_fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "id": "sql_querying_advanced"
                  },
                  {
                    "type": "prerequisite",
                    "id": "big_data_concepts"
                  },
                  {
                    "type": "complementary",
                    "id": "data_governance_and_compliance_cloud"
                  },
                  {
                    "type": "complementary",
                    "id": "cloud_security_for_data_platforms"
                  },
                  {
                    "type": "unlocks",
                    "id": "advanced_cloud_data_platform_architecture"
                  },
                  {
                    "type": "unlocks",
                    "id": "data_platform_cost_management_specialist"
                  },
                  {
                    "type": "unlocks",
                    "id": "real_time_data_processing_optimization"
                  }
                ]
              }
            ],
            "skillName": "Quản lý và Tối ưu hóa Hồ dữ liệu/Kho dữ liệu trên Nền tảng Đám mây (Cloud Data Lake/Warehouse Management and Optimization)",
            "competency": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
            "description": "Kỹ năng này cho phép bạn thiết kế, triển khai và quản lý hiệu quả các hệ thống Data Lake và Data Warehouse trên các nền tảng đám mây hàng đầu. Bạn sẽ học cách tối ưu hóa hiệu suất truy vấn, quản lý chi phí và vòng đời dữ liệu, đảm bảo dữ liệu lớn được lưu trữ và truy cập một cách hiệu quả nhất cho các hoạt động phân tích.",
            "learningResources": [
              {
                "name": "Khóa học chuyên sâu về Kiến trúc Dữ liệu trên Đám mây (ví dụ: Architecting Data on AWS/Azure/GCP)",
                "type": "Online Course",
                "url": "https://www.coursera.org/specializations/aws-data-analytics"
              },
              {
                "name": "Sách: Data Lakehouse: A Modern Approach to Data Management",
                "type": "Book Chapter/Ebook",
                "url": "https://databricks.com/resources/ebook/data-lakehouse-modern-approach-data-management"
              }
            ],
            "projectIdeas": [
              {
                "name": "Xây dựng Hồ dữ liệu/Kho dữ liệu cho Dữ liệu IoT",
                "description": "Thiết kế và triển khai một kiến trúc Data Lake/Warehouse trên nền tảng đám mây (AWS S3/Redshift, Azure Data Lake Storage/Synapse, hoặc GCP GCS/BigQuery) để thu thập, lưu trữ và phân tích dữ liệu từ các thiết bị IoT. Áp dụng các định dạng dữ liệu tối ưu (Parquet/Delta Lake) và các chiến lược phân vùng, nén để cải thiện hiệu suất truy vấn và quản lý chi phí. Xây dựng một pipeline đơn giản để nhập dữ liệu và một số truy vấn phân tích mẫu.",
                "difficulty": "Intermediate"
              }
            ],
            "tools": [
              "AWS S3",
              "AWS Glue",
              "AWS Athena",
              "AWS Redshift",
              "Azure Data Lake Storage",
              "Azure Synapse Analytics",
              "GCP Cloud Storage",
              "GCP BigQuery",
              "Apache Parquet",
              "Apache ORC",
              "Delta Lake",
              "Apache Iceberg",
              "Apache Hudi",
              "SQL (cho truy vấn)",
              "Python/PySpark (cho xử lý dữ liệu)"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "40-60 giờ",
            "importanceScore": 5
          }
        ],
        "abilityName": "Quản lý và Xử lý Dữ liệu lớn (Big Data Management & Processing)",
        "specialization": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
        "description": "Năng lực này là cốt lõi để thiết kế, xây dựng và duy trì các hệ thống dữ liệu có khả năng xử lý khối lượng dữ liệu khổng lồ với tốc độ và độ tin cậy cao. Nó bao gồm việc triển khai kiến trúc dữ liệu phân tán, phát triển các quy trình ETL/ELT hiệu quả và tối ưu hóa hạ tầng lưu trữ trên nền tảng đám mây. Thành thạo năng lực này giúp các tổ chức khai thác tối đa giá trị từ dữ liệu lớn, thúc đẩy các quyết định kinh doanh dựa trên dữ liệu và tạo lợi thế cạnh tranh.",
        "learningResources": [
          {
            "title": "Designing Data-Intensive Applications",
            "type": "Book",
            "author": "Martin Kleppmann",
            "description": "Cuốn sách kinh điển này cung cấp cái nhìn sâu sắc về các nguyên tắc cốt lõi của việc thiết kế hệ thống dữ liệu phân tán, giải quyết các thách thức về khả năng mở rộng, độ tin cậy và khả năng bảo trì."
          },
          {
            "title": "Specialization: Data Engineering with Google Cloud Professional Certificate",
            "type": "Online Course",
            "platform": "Coursera",
            "description": "Chương trình chuyên sâu này bao gồm các kỹ năng cần thiết để xây dựng và quản lý kiến trúc dữ liệu trên Google Cloud, từ thu thập dữ liệu đến xử lý và phân tích dữ liệu lớn."
          }
        ],
        "projectIdeas": [
          {
            "title": "Xây dựng Nền tảng Phân tích Dữ liệu Chuỗi Cung ứng Thời gian Thực",
            "description": "Thiết kế và triển khai một nền tảng dữ liệu lớn trên nền tảng đám mây (ví dụ: AWS, Azure hoặc GCP) để phân tích dữ liệu chuỗi cung ứng (kho hàng, vận chuyển, đơn hàng). Dự án sẽ bao gồm việc thiết kế kiến trúc hồ dữ liệu, xây dựng các data pipeline ETL/ELT để nhập dữ liệu từ nhiều nguồn khác nhau (cơ sở dữ liệu giao dịch, API của đối tác), sử dụng Apache Spark để xử lý và phân tích dữ liệu ở quy mô lớn, và tạo ra các dashboard phân tích hiệu suất chuỗi cung ứng và dự đoán nhu cầu sản phẩm."
          }
        ],
        "tools": [
          "Apache Spark",
          "Apache Hadoop",
          "Apache Kafka",
          "Apache Airflow",
          "AWS S3 / Google Cloud Storage / Azure Data Lake Storage",
          "Databricks",
          "Snowflake",
          "Docker / Kubernetes"
        ],
        "difficultyLevel": "Nâng cao",
        "estimatedTimeToComplete": "6 - 12 tháng (để đạt được sự thành thạo vững chắc)",
        "importanceScore": 5
      },
      {
        "id": "ability_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4",
        "name": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
        "type": "ability",
        "children": [
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_1",
            "name": "Xử lý giá trị thiếu (Missing Value Imputation)",
            "type": "skill",
            "children": [
              {
                "id": "data_missing_value_types_mcar_mar_mnar",
                "name": "Các loại giá trị thiếu và cơ chế phát sinh (MCAR, MAR, MNAR)",
                "type": "knowledge",
                "children": [],
                "description": "Hiểu rõ các cơ chế phát sinh giá trị thiếu – Missing Completely At Random (MCAR), Missing At Random (MAR), và Missing Not At Random (MNAR) – là nền tảng trong khoa học dữ liệu. Kiến thức này giúp nhà phân tích lựa chọn phương pháp xử lý giá trị thiếu phù hợp, giảm thiểu sai lệch thống kê và đảm bảo độ tin cậy của các mô hình phân tích cũng như học máy.",
                "keywords": [
                  "Missing Data",
                  "MCAR",
                  "MAR",
                  "MNAR",
                  "Data Imputation",
                  "Missing Data Mechanisms",
                  "Bias",
                  "Data Quality",
                  "Data Preprocessing",
                  "Statistical Inference"
                ],
                "learningResources": [
                  {
                    "name": "Understanding Missing Data Mechanisms: MCAR, MAR, MNAR",
                    "url": "https://towardsdatascience.com/understanding-missing-data-mechanisms-mcar-mar-mnar-34208d172782",
                    "type": "Article"
                  },
                  {
                    "name": "Missing Data - ETH Zurich Lecture Notes",
                    "url": "https://stat.ethz.ch/education/semesters/fs2018/seminar/docs/lecture2.pdf",
                    "type": "Lecture Notes"
                  },
                  {
                    "name": "Missing Data: MCAR, MAR, MNAR - IBM Data Science Professional Certificate (Coursera)",
                    "url": "https://www.coursera.org/learn/data-scientists-tools/lecture/qfN4M/missing-data-mcar-mar-mnar",
                    "type": "Online Course Module"
                  }
                ],
                "prerequisites": [
                  "data_quality_concepts",
                  "basic_statistical_concepts",
                  "types_of_data_variables"
                ],
                "projectIdeas": [
                  "Phân tích và mô phỏng các cơ chế giá trị thiếu: Chọn một bộ dữ liệu công khai, phân tích các cột có giá trị thiếu để suy luận cơ chế (MCAR, MAR, MNAR). Sau đó, viết code Python để mô phỏng việc tạo ra giá trị thiếu theo từng cơ chế trên một bộ dữ liệu sạch và quan sát sự khác biệt trong phân phối dữ liệu.",
                  "Đánh giá tác động của cơ chế thiếu lên mô hình: Với các bộ dữ liệu có giá trị thiếu theo các cơ chế khác nhau (MCAR, MAR, MNAR), áp dụng cùng một phương pháp xử lý giá trị thiếu đơn giản (ví dụ: mean imputation). Xây dựng một mô hình Machine Learning và so sánh hiệu suất để thấy sự khác biệt trong tác động của từng cơ chế thiếu."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Statistician",
                  "Quantitative Researcher"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "Pandas",
                  "NumPy",
                  "Scikit-learn",
                  "R"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "3-6 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 8
                  },
                  {
                    "path": "Quantitative Researcher",
                    "score": 10
                  }
                ],
                "skillRelations": [
                  {
                    "type": "part_of",
                    "targetId": "missing_value_imputation",
                    "description": "Là kiến thức nền tảng để hiểu và lựa chọn phương pháp xử lý giá trị thiếu."
                  },
                  {
                    "type": "complementary",
                    "targetId": "missing_value_imputation_methods",
                    "description": "Hiểu các loại giá trị thiếu giúp lựa chọn phương pháp xử lý hiệu quả và phù hợp nhất."
                  },
                  {
                    "type": "prerequisite",
                    "targetId": "bias_detection_and_mitigation_in_data",
                    "description": "Nền tảng để nhận diện và giảm thiểu thiên lệch (bias) có thể phát sinh do giá trị thiếu."
                  },
                  {
                    "type": "unlocks",
                    "targetId": "advanced_missing_value_imputation_techniques",
                    "description": "Cho phép tiếp cận và áp dụng các kỹ thuật xử lý giá trị thiếu nâng cao một cách có căn cứ và hiệu quả."
                  }
                ]
              },
              {
                "id": "DS-MV-BasicImputation-MeanMedianMode",
                "name": "Các kỹ thuật xử lý giá trị thiếu cơ bản (Điền trung bình, trung vị, mode)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các phương pháp đơn giản nhất để xử lý dữ liệu thiếu bằng cách thay thế chúng bằng giá trị trung bình (mean), trung vị (median) hoặc mode của cột tương ứng. Việc áp dụng các kỹ thuật này rất quan trọng để duy trì tính toàn vẹn của tập dữ liệu, cho phép phân tích và xây dựng mô hình mà không bị lỗi do dữ liệu thiếu, đặc biệt khi dữ liệu thiếu ngẫu nhiên hoặc không đáng kể.",
                "keywords": [
                  "Missing Values",
                  "Data Imputation",
                  "Mean Imputation",
                  "Median Imputation",
                  "Mode Imputation",
                  "Data Preprocessing",
                  "Data Cleaning",
                  "Missing Data Handling",
                  "Univariate Imputation",
                  "Statistical Imputation"
                ],
                "learningResources": [
                  {
                    "name": "Pandas `fillna()` documentation",
                    "url": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html"
                  },
                  {
                    "name": "Scikit-learn `SimpleImputer`",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
                  },
                  {
                    "name": "A Gentle Introduction to Handling Missing Values",
                    "url": "https://towardsdatascience.com/a-gentle-introduction-to-handling-missing-values-for-data-scientists-c0a6b72a6b4a"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Sử dụng một tập dữ liệu công khai (ví dụ: Titanic, hoặc một tập dữ liệu nhà ở có sửa đổi để tạo thiếu giá trị) để thực hành điền giá trị thiếu bằng trung bình, trung vị và mode. So sánh ảnh hưởng của từng phương pháp đến phân tích thống kê cơ bản hoặc hiệu suất của một mô hình học máy đơn giản (ví dụ: hồi quy logistic/linear).",
                  "Xây dựng một hàm hoặc script Python sử dụng thư viện Pandas để tự động phát hiện và điền giá trị thiếu trong một DataFrame bằng các phương pháp trung bình, trung vị hoặc mode, dựa trên kiểu dữ liệu của cột (ví dụ: trung bình/trung vị cho số, mode cho phân loại)."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Machine Learning Engineer",
                  "Business Intelligence Analyst",
                  "Data Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (Pandas, NumPy, Scikit-learn)",
                  "R (dplyr, tidyr)",
                  "SQL (cơ bản để trích xuất và kiểm tra dữ liệu)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 giờ",
                "importanceScore": 8,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  },
                  {
                    "path": "Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "complementary",
                    "skill": "Các kỹ thuật xử lý giá trị thiếu nâng cao (Hồi quy, KNN, MICE)"
                  },
                  {
                    "type": "prerequisite",
                    "skill": "Phân tích thống kê mô tả cơ bản"
                  },
                  {
                    "type": "unlocks",
                    "skill": "Xử lý dữ liệu để xây dựng mô hình học máy"
                  }
                ]
              },
              {
                "id": "advanced_missing_value_imputation_regression_knn_mice",
                "name": "Các kỹ thuật xử lý giá trị thiếu nâng cao (Hồi quy, K-NN, MICE)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các kỹ thuật tiên tiến để điền các giá trị thiếu trong tập dữ liệu, bao gồm hồi quy dự đoán, K-Nearest Neighbors (K-NN) và Multiple Imputation by Chained Equations (MICE). Việc sử dụng các phương pháp này giúp tạo ra các bộ dữ liệu chính xác và ít sai lệch hơn, từ đó nâng cao độ tin cậy và hiệu suất của các mô hình học máy.",
                "keywords": [
                  "Missing Value Imputation",
                  "Regression Imputation",
                  "K-NN Imputation",
                  "MICE",
                  "Multiple Imputation",
                  "Data Preprocessing",
                  "Missing Data",
                  "Predictive Imputation",
                  "Data Cleaning",
                  "Imputer"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn Documentation: Imputation of missing values",
                    "url": "https://scikit-learn.org/stable/modules/impute.html"
                  },
                  {
                    "name": "A Comprehensive Guide to Data Imputation with MICE Algorithm",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-data-imputation-with-multiple-imputation-by-chained-equations-mice-algorithm-49b8006b527b"
                  },
                  {
                    "name": "Introduction to Missing Data Imputation in Python",
                    "url": "https://www.datacamp.com/tutorial/introduction-to-missing-data-imputation"
                  }
                ],
                "prerequisites": [
                  "basic_missing_value_imputation_mean_median_mode",
                  "introduction_to_linear_regression",
                  "knn_algorithm_fundamentals",
                  "basic_statistical_concepts"
                ],
                "projectIdeas": [
                  "Áp dụng các kỹ thuật Hồi quy, K-NN và MICE để điền giá trị thiếu trên một tập dữ liệu thực (ví dụ: Titanic, House Prices) và so sánh hiệu suất của các mô hình học máy được xây dựng trên các bộ dữ liệu đã được xử lý khác nhau, đánh giá độ chính xác và độ bền.",
                  "Phát triển một pipeline xử lý dữ liệu tự động tích hợp K-NN Imputer và Iterative Imputer (MICE) của Scikit-learn, cho phép người dùng lựa chọn và cấu hình các phương pháp điền khác nhau và trực quan hóa tác động của chúng lên phân phối dữ liệu."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Research Scientist (Data Science)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python",
                  "Pandas",
                  "NumPy",
                  "Scikit-learn",
                  "Statsmodels"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "basic_missing_value_imputation_mean_median_mode",
                    "type": "prerequisite"
                  },
                  {
                    "id": "introduction_to_linear_regression",
                    "type": "prerequisite"
                  },
                  {
                    "id": "knn_algorithm_fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_preprocessing_techniques",
                    "type": "complementary"
                  },
                  {
                    "id": "model_evaluation_and_selection",
                    "type": "complementary"
                  },
                  {
                    "id": "building_robust_ml_models",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "ds_bd_mvi_impact_missing_values",
                "name": "Tác động của giá trị thiếu đến phân tích dữ liệu và hiệu suất mô hình",
                "type": "knowledge",
                "children": [],
                "description": "Hiểu rõ tác động của giá trị thiếu là nền tảng quan trọng trong Khoa học Dữ liệu. Đơn vị kiến thức này giải thích cách dữ liệu thiếu có thể làm sai lệch kết quả phân tích thống kê, tạo ra các mô hình học máy kém chính xác và đưa đến những quyết định kinh doanh sai lầm. Nắm vững điều này giúp các nhà khoa học dữ liệu nhận diện tầm quan trọng của việc xử lý giá trị thiếu một cách hiệu quả để đảm bảo tính toàn vẹn và độ tin cậy của dữ liệu.",
                "keywords": [
                  "missing data impact",
                  "data bias",
                  "model performance degradation",
                  "statistical inference",
                  "data integrity",
                  "machine learning robustness",
                  "data quality",
                  "imputation necessity",
                  "exploratory data analysis"
                ],
                "learningResources": [
                  {
                    "name": "Missing Data and How to Deal With It - IBM",
                    "url": "https://www.ibm.com/topics/missing-data"
                  },
                  {
                    "name": "Impact of Missing Data on Machine Learning Models - Analytics Vidhya",
                    "url": "https://www.analyticsvidhya.com/blog/2021/07/impact-of-missing-data-on-machine-learning-models/"
                  },
                  {
                    "name": "Missing Data in Data Science – Causes, Impacts and Solutions - Simplilearn",
                    "url": "https://www.simplilearn.com/tutorials/data-science-tutorial/missing-data-in-data-science"
                  }
                ],
                "prerequisites": [
                  "ds_bd_core_descriptive_statistics",
                  "ds_bd_core_data_types_and_structures",
                  "ds_bd_ml_intro_concepts"
                ],
                "projectIdeas": [
                  {
                    "title": "Phân tích tác động của giá trị thiếu trên bộ dữ liệu thực tế",
                    "description": "Chọn một bộ dữ liệu công khai có giá trị thiếu (ví dụ: Titanic, House Prices). Thực hiện Phân tích Dữ liệu Khám phá (EDA) để xác định các mẫu thiếu, tỷ lệ thiếu và trực quan hóa chúng. Đánh giá sơ bộ tác động tiềm ẩn đến các biến chính hoặc mục tiêu."
                  },
                  {
                    "title": "So sánh hiệu suất mô hình với dữ liệu thiếu vs. dữ liệu bị loại bỏ",
                    "description": "Sử dụng một mô hình học máy đơn giản (ví dụ: Hồi quy tuyến tính, Cây quyết định). Huấn luyện mô hình trên dữ liệu gốc (có giá trị thiếu) và so sánh hiệu suất với mô hình huấn luyện trên dữ liệu mà các hàng/cột có giá trị thiếu đã bị loại bỏ hoàn toàn. Đánh giá sự khác biệt về các chỉ số như R-squared, MAE, MSE."
                  }
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Machine Learning Engineer",
                  "Business Intelligence Developer",
                  "Data Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Pandas",
                  "NumPy",
                  "Matplotlib",
                  "Seaborn",
                  "Scikit-learn"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 7
                  },
                  {
                    "path": "Data Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ds_bd_core_descriptive_statistics",
                    "description": "Kiến thức cơ bản về các khái niệm thống kê mô tả là cần thiết để hiểu cách giá trị thiếu có thể ảnh hưởng đến các số liệu tổng hợp."
                  },
                  {
                    "type": "prerequisite",
                    "id": "ds_bd_ml_intro_concepts",
                    "description": "Hiểu biết về các khái niệm cơ bản về học máy (huấn luyện, kiểm thử, đánh giá mô hình) giúp nhận thức tác động của giá trị thiếu đến hiệu suất mô hình."
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bd_mvi_techniques_and_strategies",
                    "description": "Nắm vững tác động của giá trị thiếu là động lực để học hỏi và áp dụng các kỹ thuật xử lý giá trị thiếu hiệu quả."
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bd_preprocessing_data_cleaning_basics",
                    "description": "Đơn vị này bổ trợ cho các kỹ năng làm sạch và tiền xử lý dữ liệu chung, nhấn mạnh một khía cạnh cụ thể của chất lượng dữ liệu."
                  }
                ]
              },
              {
                "id": "knowledge_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_1_5",
                "name": "Phương pháp lựa chọn và đánh giá kỹ thuật xử lý giá trị thiếu phù hợp",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào việc lựa chọn có chiến lược các kỹ thuật xử lý giá trị thiếu (imputation) phù hợp nhất cho từng tập dữ liệu cụ thể và đánh giá tác động của chúng. Việc nắm vững các phương pháp này là rất quan trọng để đảm bảo chất lượng dữ liệu, giảm thiểu sai lệch (bias) và tối ưu hóa hiệu suất của các mô hình học máy và phân tích dữ liệu tiếp theo.",
                "keywords": [
                  "Missing data imputation",
                  "Imputation techniques",
                  "Evaluation metrics",
                  "Data quality",
                  "Bias mitigation",
                  "Model performance",
                  "Missing data mechanisms",
                  "Data preprocessing",
                  "Cross-validation",
                  "Sensitivity analysis"
                ],
                "learningResources": [
                  {
                    "title": "Imputation of missing values - Scikit-learn documentation",
                    "url": "https://scikit-learn.org/stable/modules/impute.html",
                    "type": "Documentation"
                  },
                  {
                    "title": "A Comprehensive Guide to Handling Missing Values",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-handling-missing-values-40545f478a57",
                    "type": "Blog Post"
                  },
                  {
                    "title": "How to Evaluate Missing Data Imputation Methods",
                    "url": "https://www.statology.org/evaluate-missing-data-imputation-methods/",
                    "type": "Tutorial"
                  }
                ],
                "prerequisites": [
                  "hieu_biet_ve_cac_loai_gia_tri_thieu_mcar_mar_mnar",
                  "cac_ky_thuat_xu_ly_gia_tri_thieu_co_ban",
                  "ky_thuat_tien_xu_ly_du_lieu_co_ban",
                  "gioi_thieu_ve_danh_gia_mo_hinh_hoc_may"
                ],
                "projectIdeas": [
                  "Chọn một tập dữ liệu công khai có giá trị thiếu (ví dụ: từ Kaggle). Áp dụng ít nhất ba phương pháp điền giá trị khác nhau (như trung bình, kNN, MICE) và so sánh tác động của chúng lên hiệu suất của một mô hình phân loại hoặc hồi quy đơn giản.",
                  "Thiết kế và triển khai một pipeline tự động để thử nghiệm và so sánh các kỹ thuật điền giá trị thiếu khác nhau dựa trên các tiêu chí đánh giá định lượng (ví dụ: RMSE, MAE cho dữ liệu liên tục, F1-score cho dữ liệu phân loại sau khi điền và huấn luyện mô hình)."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Research Scientist (AI/ML)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (Pandas, Scikit-learn, FancyImpute, MissingPy)",
                  "R (mice, VIM)",
                  "SQL (cho phân tích và xử lý dữ liệu sơ bộ)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "12-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Research Scientist (AI/ML)",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "hieu_biet_ve_cac_loai_gia_tri_thieu_mcar_mar_mnar",
                    "type": "prerequisite"
                  },
                  {
                    "id": "cac_ky_thuat_xu_ly_gia_tri_thieu_co_ban",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ky_thuat_tien_xu_ly_du_lieu_co_ban",
                    "type": "prerequisite"
                  },
                  {
                    "id": "gioi_thieu_ve_danh_gia_mo_hinh_hoc_may",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ky_thuat_toi_uu_hoa_tham_so_mo_hinh",
                    "type": "complementary"
                  },
                  {
                    "id": "mo_hinh_hoa_thong_ke_manh_me",
                    "type": "complementary"
                  },
                  {
                    "id": "thiet_ke_pipeline_hoc_may_manh_me",
                    "type": "unlocks"
                  },
                  {
                    "id": "xay_dung_he_thong_ml_san_sang_cho_san_xuat",
                    "type": "unlocks"
                  }
                ]
              }
            ],
            "skillName": "Xử lý giá trị thiếu (Missing Value Imputation)",
            "competency": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
            "description": "Kỹ năng này trang bị cho bạn kiến thức và công cụ để xác định, phân loại các loại giá trị thiếu trong dữ liệu, hiểu tác động của chúng. Bạn sẽ học cách áp dụng các kỹ thuật điền giá trị thiếu phù hợp, từ cơ bản đến nâng cao, nhằm đảm bảo chất lượng dữ liệu và tối ưu hóa hiệu suất mô hình học máy.",
            "learningResources": [
              {
                "title": "Chương về 'Xử lý dữ liệu thiếu' trong sách 'Python for Data Analysis'",
                "type": "Sách/Chương sách",
                "url": "https://wesmckinney.com/book/"
              },
              {
                "title": "Module 'Handling Missing Values' trong khóa học 'Data Cleaning' trên Coursera/edX",
                "type": "Khóa học/Module",
                "url": "https://www.coursera.org/browse/data-science/data-analysis"
              }
            ],
            "projectIdeas": [
              {
                "title": "Phân tích và Xử lý Giá trị Thiếu trên Bộ Dữ liệu Thực tế",
                "description": "Chọn một bộ dữ liệu thực tế từ Kaggle hoặc UCI Machine Learning Repository có chứa nhiều giá trị thiếu. Phân tích các loại giá trị thiếu (MCAR, MAR, MNAR) nếu có thể. Áp dụng ít nhất ba kỹ thuật điền giá trị thiếu khác nhau (ví dụ: trung vị, K-NN, MICE) và đánh giá tác động của chúng lên hiệu suất của một mô hình học máy đơn giản (ví dụ: Hồi quy tuyến tính hoặc Cây quyết định) bằng các chỉ số phù hợp. Trình bày lựa chọn kỹ thuật tối ưu và lý do."
              }
            ],
            "tools": [
              "Python (pandas, scikit-learn, fancyimpute)",
              "R (dplyr, mice, VIM)",
              "SQL (cho việc xác định và xử lý NULL cơ bản)",
              "Thư viện hỗ trợ trực quan hóa dữ liệu thiếu (ví dụ: missingno trong Python)"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "12-20 giờ",
            "importanceScore": "Cao"
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_2",
            "name": "Phát hiện và xử lý dữ liệu ngoại lai (Outlier Detection & Treatment)",
            "type": "skill",
            "children": [
              {
                "id": "DS-BDA-ODT-DEF-001",
                "name": "Định nghĩa, đặc điểm và các loại dữ liệu ngoại lai (Outlier Definition, Characteristics, and Types)",
                "type": "knowledge",
                "children": [],
                "title": "Định nghĩa, đặc điểm và các loại dữ liệu ngoại lai (Outlier Definition, Characteristics, and Types)",
                "description": "Dữ liệu ngoại lai (outlier) là những điểm dữ liệu khác biệt đáng kể so với phần lớn các quan sát khác trong tập dữ liệu. Việc hiểu rõ định nghĩa, nhận diện các đặc điểm và phân loại các loại ngoại lai khác nhau là cực kỳ quan trọng để đánh giá chất lượng dữ liệu và tránh những sai lệch nghiêm trọng trong phân tích thống kê và mô hình học máy. Kiến thức này đặt nền tảng cho các kỹ thuật phát hiện và xử lý dữ liệu ngoại lai hiệu quả, đảm bảo tính chính xác và tin cậy của các kết quả phân tích.",
                "keywords": [
                  "Dữ liệu ngoại lai",
                  "Outlier",
                  "Điểm bất thường",
                  "Anomalies",
                  "Data quality",
                  "Phân tích dữ liệu",
                  "Thống kê mô tả",
                  "Loại ngoại lai",
                  "Đặc điểm ngoại lai",
                  "Ảnh hưởng ngoại lai"
                ],
                "learningResources": [
                  {
                    "name": "What are Outliers in Data Science?",
                    "url": "https://www.geeksforgeeks.org/what-are-outliers-in-data-science/"
                  },
                  {
                    "name": "A Quick Introduction to Outliers in Data Science",
                    "url": "https://towardsdatascience.com/a-quick-introduction-to-outliers-in-data-science-a5a411132207"
                  },
                  {
                    "name": "Outlier Detection - IBM",
                    "url": "https://www.ibm.com/topics/outlier-detection"
                  }
                ],
                "prerequisites": [
                  "DS-BDA-STAT-001",
                  "DS-BDA-DATA-TYPES-001",
                  "DS-BDA-DQ-001"
                ],
                "projectIdeas": [
                  "Phân tích tập dữ liệu nhỏ và xác định các điểm dữ liệu tiềm năng là ngoại lai chỉ bằng cách quan sát và sử dụng các thống kê mô tả cơ bản (min/max, quartiles, IQR).",
                  "Tạo một báo cáo ngắn gọn (ví dụ: 1 trang A4) giải thích định nghĩa, các đặc điểm có thể nhận biết và liệt kê các loại ngoại lai có thể có trong một tập dữ liệu giả định, kèm theo ví dụ minh họa."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Machine Learning Engineer",
                  "Business Intelligence Developer",
                  "Research Analyst",
                  "Statistician"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (Pandas, NumPy)",
                  "R",
                  "Excel / Google Sheets"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "3-5 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 7
                  },
                  {
                    "path": "Statistician",
                    "score": 10
                  },
                  {
                    "path": "Financial Analyst",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS-BDA-STAT-001",
                    "type": "prerequisite",
                    "name": "Kiến thức cơ bản về thống kê mô tả"
                  },
                  {
                    "id": "DS-BDA-DATA-TYPES-001",
                    "type": "prerequisite",
                    "name": "Kiến thức về các loại dữ liệu"
                  },
                  {
                    "id": "DS-BDA-DQ-001",
                    "type": "prerequisite",
                    "name": "Khái niệm cơ bản về chất lượng dữ liệu"
                  },
                  {
                    "id": "DS-BDA-ODT-DETECTION-002",
                    "type": "unlocks",
                    "name": "Các phương pháp phát hiện dữ liệu ngoại lai"
                  },
                  {
                    "id": "DS-BDA-ODT-TREATMENT-003",
                    "type": "unlocks",
                    "name": "Các kỹ thuật xử lý dữ liệu ngoại lai"
                  },
                  {
                    "id": "DS-BDA-EDA-001",
                    "type": "complementary",
                    "name": "Phân tích khám phá dữ liệu (EDA)"
                  }
                ]
              },
              {
                "id": "outlier_detection_statistical_graphical_methods",
                "name": "Các phương pháp phát hiện dữ liệu ngoại lai dựa trên thống kê (Z-score, IQR) và đồ thị (Box Plot, Scatter Plot)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị các kỹ thuật cơ bản và phổ biến để xác định dữ liệu ngoại lai (outliers) trong tập dữ liệu. Việc hiểu và áp dụng các phương pháp này là rất quan trọng để đảm bảo chất lượng dữ liệu và độ tin cậy của các mô hình phân tích, tránh sai lệch kết quả do các giá trị bất thường gây ra.",
                "keywords": [
                  "Dữ liệu ngoại lai",
                  "Outlier Detection",
                  "Z-score",
                  "IQR (Interquartile Range)",
                  "Box Plot",
                  "Scatter Plot",
                  "Phân tích thống kê",
                  "Trực quan hóa dữ liệu",
                  "Tiền xử lý dữ liệu",
                  "Data Quality"
                ],
                "learningResources": [
                  {
                    "title": "A Quick Guide to Outlier Detection in Python using Z-score and IQR",
                    "url": "https://towardsdatascience.com/a-quick-guide-to-outlier-detection-in-python-using-z-score-and-iqr-ec91e57c6883",
                    "type": "Article"
                  },
                  {
                    "title": "What are Outliers?",
                    "url": "https://www.ibm.com/topics/outliers",
                    "type": "Article"
                  },
                  {
                    "title": "Detect and Remove Outliers Using Python",
                    "url": "https://www.geeksforgeeks.org/detect-and-remove-outliers-using-python/",
                    "type": "Tutorial"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Áp dụng các phương pháp Z-score, IQR, Box Plot và Scatter Plot để phát hiện ngoại lai trên một tập dữ liệu công khai (ví dụ: dữ liệu giá nhà, dữ liệu y tế từ Kaggle). So sánh kết quả của các phương pháp khác nhau.",
                  "Viết một script Python để tự động hóa quy trình phát hiện ngoại lai bằng Z-score và IQR, sau đó trực quan hóa các ngoại lai đã phát hiện bằng Box Plot và Scatter Plot."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Business Intelligence Analyst",
                  "Data Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (Pandas, NumPy, Matplotlib, Seaborn)",
                  "R",
                  "Excel (cho phân tích cơ bản)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "6-10 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  },
                  {
                    "path": "Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "targetSkillId": "outlier_treatment_methods",
                    "type": "complementary",
                    "description": "Hoàn thiện quy trình xử lý dữ liệu ngoại lai bằng cách học các phương pháp xử lý sau khi phát hiện."
                  },
                  {
                    "targetSkillId": "advanced_outlier_detection_methods",
                    "type": "unlocks",
                    "description": "Tạo nền tảng để học các phương pháp phát hiện ngoại lai phức tạp và nâng cao hơn (ví dụ: Isolation Forest, LOF)."
                  },
                  {
                    "targetSkillId": "data_analysis_preprocessing",
                    "type": "unlocks",
                    "description": "Là một phần cốt lõi của quy trình tiền xử lý và làm sạch dữ liệu trong phân tích dữ liệu tổng thể."
                  }
                ]
              },
              {
                "id": "knowledge_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_2_3",
                "name": "Các phương pháp phát hiện dữ liệu ngoại lai nâng cao (Isolation Forest, LOF, DBSCAN)",
                "type": "knowledge",
                "children": [],
                "description": "Các phương pháp này (Isolation Forest, Local Outlier Factor - LOF, và DBSCAN) là những kỹ thuật nâng cao, không giám sát dùng để xác định các điểm dữ liệu bất thường (outliers) khác biệt đáng kể so với phần lớn dữ liệu. Nắm vững chúng là cực kỳ quan trọng để tiền xử lý dữ liệu một cách mạnh mẽ, nâng cao hiệu suất mô hình học máy, và phát hiện các sự kiện quan trọng như gian lận hoặc lỗi hệ thống trong các tập dữ liệu phức tạp nơi các phương pháp truyền thống không hiệu quả.",
                "keywords": [
                  "Outlier Detection",
                  "Anomaly Detection",
                  "Isolation Forest",
                  "Local Outlier Factor (LOF)",
                  "DBSCAN",
                  "Unsupervised Learning",
                  "Data Preprocessing",
                  "Machine Learning",
                  "Novelty Detection",
                  "Density-based Clustering"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn Documentation: Isolation Forest",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
                  },
                  {
                    "name": "Scikit-learn Documentation: Local Outlier Factor (LOF)",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html"
                  },
                  {
                    "name": "A Comprehensive Guide to Outlier Detection Techniques",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-outlier-detection-techniques-5dfdd302d9dd"
                  }
                ],
                "prerequisites": [
                  "data_science_fundamentals_001",
                  "statistical_thinking_002",
                  "intro_to_machine_learning_003",
                  "basic_outlier_detection_004",
                  "unsupervised_learning_basics_005"
                ],
                "projectIdeas": [
                  "Xây dựng hệ thống phát hiện gian lận giao dịch tài chính bằng cách áp dụng Isolation Forest hoặc LOF trên tập dữ liệu lịch sử.",
                  "Phát triển giải pháp nhận diện bất thường trong mạng (network intrusion detection) bằng cách sử dụng DBSCAN để phân loại lưu lượng truy cập mạng."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Fraud Analyst",
                  "Data Analyst (Senior)",
                  "Research Scientist (Data)"
                ],
                "marketDemand": "High",
                "tools": [
                  "scikit-learn",
                  "pandas",
                  "numpy",
                  "matplotlib",
                  "seaborn"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "AI Engineer",
                    "score": 8
                  },
                  {
                    "path": "Fraud Analyst",
                    "score": 10
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "id": "basic_outlier_detection_004",
                    "type": "prerequisite",
                    "description": "Nắm vững các phương pháp phát hiện ngoại lai cơ bản (ví dụ: Z-score, IQR)."
                  },
                  {
                    "id": "unsupervised_learning_basics_005",
                    "type": "prerequisite",
                    "description": "Hiểu biết về các thuật toán học không giám sát và khái niệm về khoảng cách."
                  },
                  {
                    "id": "feature_engineering_006",
                    "type": "complementary",
                    "description": "Kỹ thuật tiền xử lý dữ liệu để tạo ra các đặc trưng tốt hơn sau khi phát hiện và xử lý ngoại lai."
                  },
                  {
                    "id": "data_visualization_007",
                    "type": "complementary",
                    "description": "Trực quan hóa ngoại lai để hiểu rõ hơn bản chất, phân bố và tác động của chúng."
                  },
                  {
                    "id": "advanced_anomaly_detection_systems_008",
                    "type": "unlocks",
                    "description": "Thiết kế và triển khai các hệ thống phát hiện bất thường phức tạp trong thời gian thực hoặc quy mô lớn."
                  },
                  {
                    "id": "robust_machine_learning_models_009",
                    "type": "unlocks",
                    "description": "Xây dựng các mô hình học máy vững chắc, ít bị ảnh hưởng bởi dữ liệu nhiễu và ngoại lai."
                  }
                ]
              },
              {
                "id": "data-outlier-treatment-strategies",
                "name": "Các chiến lược và kỹ thuật xử lý dữ liệu ngoại lai (Xóa bỏ, Capping, Biến đổi, Imputation)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị các phương pháp xử lý dữ liệu ngoại lai như xóa bỏ, giới hạn (capping), biến đổi dữ liệu (data transformation) và điền giá trị (imputation). Việc xử lý hiệu quả dữ liệu ngoại lai là cực kỳ quan trọng để đảm bảo chất lượng dữ liệu, cải thiện độ chính xác và tính ổn định của các mô hình phân tích và máy học, từ đó dẫn đến những kết luận và dự đoán đáng tin cậy hơn.",
                "keywords": [
                  "Xử lý dữ liệu ngoại lai",
                  "Loại bỏ ngoại lai",
                  "Capping ngoại lai",
                  "Winsorization",
                  "Biến đổi dữ liệu",
                  "Imputation ngoại lai",
                  "Làm sạch dữ liệu",
                  "Tiền xử lý dữ liệu",
                  "Dữ liệu nhiễu",
                  "Chất lượng dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "A Comprehensive Guide to Outlier Detection and Treatment in Data Science",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-outlier-detection-and-treatment-in-data-science-13f50800be3c"
                  },
                  {
                    "name": "Methods to Handle Outliers in Data",
                    "url": "https://www.analyticsvidhya.com/blog/2021/05/methods-to-handle-outliers-in-data/"
                  },
                  {
                    "name": "Outlier Detection and Treatment using Python",
                    "url": "https://www.kaggle.com/code/prasadmladg/outlier-detection-and-treatment-using-python/notebook"
                  }
                ],
                "prerequisites": [
                  "outlier-detection-techniques",
                  "basic-statistics-for-data-science",
                  "python-pandas-basics"
                ],
                "projectIdeas": [
                  "Áp dụng các kỹ thuật xử lý ngoại lai khác nhau (ví dụ: xóa bỏ, capping IQR, biến đổi log) trên một tập dữ liệu thực tế (ví dụ: giá nhà, dữ liệu tài chính) và phân tích sự thay đổi trong phân phối dữ liệu và các thống kê mô tả.",
                  "Xây dựng một mô hình Machine Learning (ví dụ: Hồi quy tuyến tính hoặc Random Forest) trên một tập dữ liệu có ngoại lai. So sánh hiệu suất (RMSE, MAE, R2) của mô hình trước và sau khi áp dụng ít nhất hai chiến lược xử lý ngoại lai khác nhau để đánh giá tác động."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas, NumPy, SciPy, Scikit-learn)",
                  "R",
                  "Excel (cho các tập dữ liệu nhỏ và phân tích cơ bản)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 7
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "outlier-detection-techniques",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-cleaning-and-preprocessing",
                    "type": "complementary"
                  },
                  {
                    "id": "feature-engineering",
                    "type": "complementary"
                  },
                  {
                    "id": "building-robust-ml-models",
                    "type": "unlocks"
                  },
                  {
                    "id": "advanced-statistical-modeling",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "DS_ODT_Impact",
                "name": "Tác động của dữ liệu ngoại lai đến phân tích thống kê và hiệu suất mô hình học máy",
                "type": "knowledge",
                "children": [],
                "title": "Tác động của dữ liệu ngoại lai đến phân tích thống kê và hiệu suất mô hình học máy",
                "description": "Kiến thức này khám phá cách dữ liệu ngoại lai (outliers) có thể làm sai lệch các thống kê mô tả truyền thống (ví dụ: giá trị trung bình, độ lệch chuẩn), ảnh hưởng đến giả định của các mô hình thống kê, và làm giảm đáng kể hiệu suất dự đoán, độ tin cậy của các mô hình học máy. Hiểu rõ tác động này là rất quan trọng để đảm bảo tính chính xác và độ vững chắc của mọi phân tích và mô hình.",
                "keywords": [
                  "Dữ liệu ngoại lai",
                  "Tác động ngoại lai",
                  "Phân tích thống kê",
                  "Hiệu suất mô hình",
                  "Học máy",
                  "Độ vững chắc thống kê",
                  "Độ lệch (Bias)",
                  "Phương sai (Variance)",
                  "Chất lượng dữ liệu",
                  "Phát hiện bất thường"
                ],
                "learningResources": [
                  {
                    "name": "How Outliers Impact Your Data Analysis and Machine Learning Models",
                    "url": "https://towardsdatascience.com/how-outliers-impact-your-data-analysis-and-machine-learning-models-1628d022b793"
                  },
                  {
                    "name": "How Outliers Affect Data Analysis and Machine Learning Models",
                    "url": "https://www.geeksforgeeks.org/how-outliers-affect-data-analysis-and-machine-learning-models/"
                  },
                  {
                    "name": "Robust Statistics: Theory and Methods (Introductory Concepts)",
                    "url": "https://en.wikipedia.org/wiki/Robust_statistics"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích bộ dữ liệu (ví dụ: Boston Housing, Titanic) để xác định các biến có dữ liệu ngoại lai, sau đó xây dựng mô hình hồi quy hoặc phân loại trước và sau khi xử lý ngoại lai để định lượng tác động lên các chỉ số hiệu suất (MAE, RMSE, Accuracy, F1-score).",
                  "So sánh tác động của dữ liệu ngoại lai trên hai thuật toán học máy khác nhau (ví dụ: Hồi quy Tuyến tính vs. Random Forest) trên cùng một tập dữ liệu, trình bày sự khác biệt về độ nhạy và hiệu suất."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "Statistician",
                  "Data Quality Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (Pandas, NumPy, SciPy, Scikit-learn)",
                  "R (dplyr, ggplot2, stats)",
                  "Jupyter Notebook",
                  "Google Colab"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Research Scientist (AI/ML)",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "knowledgeUnitId": "DS_ODT_BasicConcepts",
                    "description": "Khái niệm cơ bản về dữ liệu ngoại lai và các phương pháp phát hiện ban đầu."
                  },
                  {
                    "type": "complementary",
                    "knowledgeUnitId": "DS_ODT_TreatmentMethods",
                    "description": "Các phương pháp xử lý dữ liệu ngoại lai để giảm thiểu tác động tiêu cực."
                  },
                  {
                    "type": "unlocks",
                    "knowledgeUnitId": "DS_ML_RobustDesign",
                    "description": "Kỹ năng thiết kế và xây dựng các mô hình học máy vững chắc, ít bị ảnh hưởng bởi dữ liệu nhiễu."
                  }
                ]
              }
            ],
            "skillName": "Phát hiện và xử lý dữ liệu ngoại lai (Outlier Detection & Treatment)",
            "competence": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
            "description": "Kỹ năng này trang bị cho bạn khả năng nhận diện và xử lý các điểm dữ liệu bất thường (outlier) có thể làm sai lệch kết quả phân tích thống kê hoặc làm giảm độ chính xác của các mô hình học máy. Bạn sẽ học cách áp dụng đa dạng các phương pháp từ cơ bản đến nâng cao để phát hiện chúng, đồng thời lựa chọn và triển khai các chiến lược xử lý hiệu quả nhằm cải thiện chất lượng và độ tin cậy của dữ liệu.",
            "learningResources": [
              "Chương 'Outlier Detection and Treatment' trong các sách về Tiền xử lý Dữ liệu hoặc Khoa học Dữ liệu (ví dụ: 'Feature Engineering for Machine Learning' của Alice Zheng và Amanda Casari hoặc 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow' của Aurélien Géron).",
              "Playlist video hoặc khóa học trực tuyến về 'Data Preprocessing: Outlier Handling' trên các nền tảng như Coursera, Udemy, hoặc YouTube (tìm kiếm các kênh chuyên về Khoa học Dữ liệu)."
            ],
            "projectIdeas": [
              "Xây dựng một pipeline tiền xử lý dữ liệu cho tập dữ liệu khách hàng vay vốn. Áp dụng ít nhất hai phương pháp phát hiện outlier (ví dụ: IQR và Isolation Forest) trên các biến số quan trọng, phân tích tác động của các outlier này đến phân phối dữ liệu. Sau đó, lựa chọn và triển khai một chiến lược xử lý (ví dụ: capping hoặc transformation) và đánh giá sự thay đổi trong các thống kê mô tả và hiệu suất của một mô hình dự đoán khả năng vỡ nợ ban đầu."
            ],
            "tools": [
              "Pandas (Python) để thao tác và phân tích dữ liệu",
              "NumPy (Python) để tính toán số học",
              "Scikit-learn (Python) cho các thuật toán phát hiện outlier nâng cao (IsolationForest, LocalOutlierFactor, DBSCAN)",
              "Matplotlib và Seaborn (Python) để trực quan hóa dữ liệu và phát hiện outlier bằng đồ thị"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "12-16 hours",
            "importanceScore": "8/10"
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_3",
            "name": "Chuẩn hóa và co giãn dữ liệu (Data Normalization & Scaling)",
            "type": "skill",
            "children": [
              {
                "id": "DS.DNS.01.01",
                "name": "Định nghĩa và Mục đích của Chuẩn hóa & Co giãn Dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Chuẩn hóa và co giãn dữ liệu là các kỹ thuật tiền xử lý dữ liệu thiết yếu nhằm chuyển đổi các giá trị số trong tập dữ liệu về một phạm vi hoặc phân phối nhất định. Mục đích chính là loại bỏ thiên vị do sự khác biệt về thang đo giữa các đặc trưng, giúp các thuật toán học máy hoạt động hiệu quả và ổn định hơn, đặc biệt là với các thuật toán dựa trên khoảng cách hoặc độ dốc.",
                "keywords": [
                  "chuẩn hóa dữ liệu",
                  "co giãn dữ liệu",
                  "tiền xử lý dữ liệu",
                  "feature scaling",
                  "data preprocessing",
                  "Min-Max Scaling",
                  "Standardization",
                  "Z-score normalization",
                  "Robust Scaling",
                  "Machine Learning pipelines"
                ],
                "learningResources": [
                  {
                    "name": "scikit-learn: Preprocessing data",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html"
                  },
                  {
                    "name": "Towards Data Science: A Comprehensive Guide to Data Scaling in Machine Learning",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-data-scaling-in-machine-learning-d6e35593833d"
                  },
                  {
                    "name": "IBM Data Science Learning: Understanding Feature Scaling",
                    "url": "https://www.ibm.com/topics/feature-scaling"
                  }
                ],
                "prerequisites": [
                  "DS.DP.01.01",
                  "DS.DM.01.01"
                ],
                "projectIdeas": [
                  "Áp dụng Min-Max Scaling và Standardization lên một tập dữ liệu nhỏ (ví dụ: Iris, Boston Housing) và trực quan hóa sự thay đổi trong phân phối dữ liệu của các đặc trưng bằng biểu đồ histogram hoặc box plot.",
                  "Huấn luyện một mô hình Machine Learning đơn giản (ví dụ: K-Nearest Neighbors hoặc Logistic Regression) trên một tập dữ liệu chưa được xử lý và một tập dữ liệu đã được chuẩn hóa/co giãn, sau đó so sánh và phân tích sự khác biệt về hiệu suất và thời gian huấn luyện."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI/ML Researcher",
                  "Big Data Engineer"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Scikit-learn, Pandas, NumPy)",
                  "R",
                  "Apache Spark (MLlib)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "3-5 hours",
                "importanceScore": 10,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 7
                  },
                  {
                    "path": "AI/ML Researcher",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetId": "DS.ML.01.01"
                  },
                  {
                    "type": "unlocks",
                    "targetId": "DS.DNS.02.01"
                  },
                  {
                    "type": "complementary",
                    "targetId": "DS.FE.01.01"
                  }
                ]
              },
              {
                "id": "data-science-big-data-data-normalization-scaling-min-max-scaling",
                "name": "Các Kỹ thuật Chuẩn hóa (Normalization) phổ biến: Min-Max Scaling",
                "type": "knowledge",
                "children": [],
                "description": "Min-Max Scaling là một kỹ thuật chuẩn hóa dữ liệu giúp biến đổi các đặc trưng (features) về một phạm vi cố định, thường là [0, 1]. Kỹ thuật này rất quan trọng vì nó giúp ngăn chặn các đặc trưng có giá trị lớn hơn hoặc phạm vi rộng hơn chi phối quá trình học của các thuật toán nhạy cảm với thang đo dữ liệu, như K-Nearest Neighbors (K-NN), Support Vector Machines (SVMs), và mạng nơ-ron.",
                "keywords": [
                  "Min-Max Scaling",
                  "Normalization",
                  "Data Preprocessing",
                  "Feature Scaling",
                  "Data Transformation",
                  "Machine Learning",
                  "Data Science",
                  "Range Transformation",
                  "0-1 Scaling",
                  "Scikit-learn MinMaxScaler"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn User Guide: Preprocessing data",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range"
                  },
                  {
                    "name": "Scikit-learn MinMaxScaler Documentation",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
                  },
                  {
                    "name": "Towards Data Science: Min-Max Normalization",
                    "url": "https://towardsdatascience.com/min-max-normalization-what-is-it-why-do-it-and-how-to-do-it-in-python-65d1372861c2"
                  }
                ],
                "prerequisites": [
                  "data-science-big-data-data-normalization-scaling-introduction-to-data-normalization",
                  "data-science-big-data-foundations-statistics-descriptive-statistics",
                  "data-science-big-data-programming-python-data-manipulation-pandas-numpy"
                ],
                "projectIdeas": [
                  "Áp dụng Min-Max Scaling cho một bộ dữ liệu công khai (ví dụ: Iris, Titanic) và so sánh hiệu suất của thuật toán K-NN hoặc SVM trước và sau khi chuẩn hóa.",
                  "Viết một hàm Python tùy chỉnh để thực hiện Min-Max Scaling từ đầu, sau đó so sánh kết quả với thư viện Scikit-learn MinMaxScaler để hiểu rõ hơn về cơ chế hoạt động."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Quantitative Analyst"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python",
                  "Scikit-learn",
                  "Pandas",
                  "NumPy"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 hours",
                "importanceScore": 8,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "AI Engineer",
                    "score": 8
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data-science-big-data-data-normalization-scaling-introduction-to-data-normalization",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-science-big-data-foundations-statistics-descriptive-statistics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-science-big-data-programming-python-data-manipulation-pandas-numpy",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data-science-big-data-data-normalization-scaling-standard-scaling",
                    "type": "complementary"
                  },
                  {
                    "id": "data-science-big-data-data-normalization-scaling-robust-scaling",
                    "type": "complementary"
                  },
                  {
                    "id": "data-science-big-data-feature-engineering-feature-transformation",
                    "type": "complementary"
                  },
                  {
                    "id": "data-science-big-data-machine-learning-model-training-evaluation",
                    "type": "complementary"
                  },
                  {
                    "id": "data-science-big-data-deep-learning-neural-networks-basics",
                    "type": "unlocks"
                  },
                  {
                    "id": "data-science-big-data-machine-learning-advanced-algorithms",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "data_preprocessing_scaling_z_score",
                "name": "Các Kỹ thuật Co giãn (Scaling/Standardization) phổ biến: Z-score Scaling",
                "type": "knowledge",
                "children": [],
                "description": "Z-score Scaling (còn gọi là Standardization) là một kỹ thuật chuẩn hóa dữ liệu quan trọng, biến đổi dữ liệu để có giá trị trung bình bằng 0 và độ lệch chuẩn bằng 1. Kỹ thuật này đặc biệt hữu ích khi dữ liệu tuân theo phân phối chuẩn hoặc khi các thuật toán học máy nhạy cảm với thang đo của dữ liệu, như SVM, K-Means, PCA và các mô hình sử dụng gradient descent, giúp cải thiện hiệu suất và tốc độ hội tụ của chúng.",
                "keywords": [
                  "Z-score",
                  "Standardization",
                  "Data Scaling",
                  "Feature Scaling",
                  "Mean Normalization",
                  "Standard Deviation",
                  "Gaussian Distribution",
                  "Data Preprocessing",
                  "Machine Learning",
                  "Statistical Transformation"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn: StandardScaler Documentation",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
                  },
                  {
                    "name": "Towards Data Science: Standardization vs Normalization",
                    "url": "https://towardsdatascience.com/standardization-vs-normalization-for-machine-learning-d615e47a96a0"
                  },
                  {
                    "name": "GeeksforGeeks: Z-Score Normalization in Machine Learning",
                    "url": "https://www.geeksforgeeks.org/z-score-normalization/"
                  }
                ],
                "prerequisites": [
                  "data_fundamentals_basic_statistics",
                  "data_preprocessing_overview",
                  "data_preprocessing_normalization_scaling_introduction"
                ],
                "projectIdeas": [
                  "Áp dụng Z-score Scaling lên một bộ dữ liệu thực tế (ví dụ: Iris, Boston Housing) bằng Python/R, sau đó trực quan hóa sự thay đổi của phân phối dữ liệu trước và sau khi co giãn trên các biến số khác nhau.",
                  "Xây dựng một mô hình phân loại (ví dụ: SVM, Logistic Regression) trên một bộ dữ liệu, so sánh hiệu suất (accuracy, precision, recall) của mô hình khi dữ liệu chưa được co giãn và khi đã được co giãn bằng Z-score Scaling."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Quantitative Analyst",
                  "MLOps Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python (scikit-learn, pandas, numpy)",
                  "R (caret, base R)",
                  "Jupyter Notebooks",
                  "Google Colab"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 9
                  },
                  {
                    "path": "AI Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Quantitative Analyst",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "data_fundamentals_basic_statistics",
                    "description": "Hiểu về khái niệm trung bình (mean) và độ lệch chuẩn (standard deviation) là nền tảng để nắm vững Z-score."
                  },
                  {
                    "type": "prerequisite",
                    "id": "data_preprocessing_normalization_scaling_introduction",
                    "description": "Hiểu biết cơ bản về lý do và tầm quan trọng của việc co giãn/chuẩn hóa dữ liệu trong tiền xử lý."
                  },
                  {
                    "type": "complementary",
                    "id": "data_preprocessing_scaling_min_max",
                    "description": "Kỹ thuật co giãn Min-Max là một phương pháp khác, cần được hiểu để so sánh và lựa chọn phù hợp cho từng trường hợp."
                  },
                  {
                    "type": "complementary",
                    "id": "ml_linear_models_intro",
                    "description": "Các mô hình học máy tuyến tính (như Hồi quy Tuyến tính, Hồi quy Logistic) thường hưởng lợi đáng kể từ dữ liệu được chuẩn hóa."
                  },
                  {
                    "type": "unlocks",
                    "id": "ml_optimization_gradient_descent",
                    "description": "Kiến thức về Z-score scaling giúp cải thiện hiệu suất và tốc độ hội tụ của các thuật toán tối ưu hóa dựa trên gradient trong học máy."
                  },
                  {
                    "type": "unlocks",
                    "id": "ml_dimensionality_reduction_pca",
                    "description": "Phân tích thành phần chính (PCA) thường yêu cầu dữ liệu được chuẩn hóa (như Z-score) để các thành phần chính không bị chi phối bởi các biến có thang đo lớn."
                  }
                ]
              },
              {
                "id": "ds_bigdata_data_scaling_method_selection",
                "name": "Lựa chọn phương pháp Chuẩn hóa/Co giãn phù hợp dựa trên phân bố dữ liệu và yêu cầu thuật toán",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc hiểu sâu các phương pháp chuẩn hóa và co giãn dữ liệu khác nhau (ví dụ: Min-Max Scaling, Standardization, Robust Scaling) và cách chọn lựa phương pháp tối ưu dựa trên đặc điểm phân bố của dữ liệu (chuẩn, lệch, có outliers) và yêu cầu cụ thể của thuật toán Machine Learning được sử dụng. Việc lựa chọn đúng phương pháp là rất quan trọng để cải thiện hiệu suất mô hình, tránh các vấn đề về hội tụ và đảm bảo tính công bằng của mô hình.",
                "keywords": [
                  "Min-Max Scaling",
                  "Standardization (Z-score)",
                  "Robust Scaling",
                  "Normalization",
                  "Feature Scaling",
                  "Data Distribution",
                  "Outliers",
                  "Gaussian Distribution",
                  "Machine Learning Algorithms",
                  "Data Preprocessing"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn User Guide: Preprocessing data",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html",
                    "type": "Documentation"
                  },
                  {
                    "name": "Towards Data Science: Standardization vs. Normalization",
                    "url": "https://towardsdatascience.com/standardization-vs-normalization-in-machine-learning-b152341ac97d",
                    "type": "Article"
                  },
                  {
                    "name": "GeeksforGeeks: Data Normalization and Standardization",
                    "url": "https://www.geeksforgeeks.org/data-normalization-and-standardization-in-machine-learning/",
                    "type": "Article"
                  }
                ],
                "prerequisites": [
                  "ds_bigdata_data_preprocessing_fundamentals",
                  "ds_bigdata_statistical_distributions_basics",
                  "ds_bigdata_data_normalization_scaling_intro"
                ],
                "projectIdeas": [
                  "So sánh hiệu suất của ít nhất ba thuật toán Machine Learning (ví dụ: Logistic Regression, SVM, K-NN) trên cùng một bộ dữ liệu, áp dụng các phương pháp co giãn khác nhau (Min-Max, Standardization, Robust Scaling) và phân tích ảnh hưởng của chúng.",
                  "Phát triển một công cụ phân tích tự động đề xuất phương pháp co giãn phù hợp dựa trên thống kê mô tả (mean, median, std, skewness, kurtosis) và biểu đồ phân bố của từng đặc trưng dữ liệu."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Applied Scientist",
                  "Data Analyst (with ML focus)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "scikit-learn",
                  "pandas",
                  "numpy",
                  "matplotlib",
                  "seaborn"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "6-10 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "id": "ds_bigdata_data_preprocessing_fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds_bigdata_statistical_distributions_basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds_bigdata_data_normalization_scaling_intro",
                    "type": "prerequisite"
                  },
                  {
                    "id": "ds_bigdata_ml_algorithm_sensitivities",
                    "type": "complementary"
                  },
                  {
                    "id": "ds_bigdata_advanced_feature_engineering",
                    "type": "unlocks"
                  },
                  {
                    "id": "ds_bigdata_model_performance_tuning",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "data-leakage-prevention-scaling",
                "name": "Quy trình áp dụng Chuẩn hóa/Co giãn để tránh rò rỉ dữ liệu (Data Leakage)",
                "type": "knowledge",
                "children": [],
                "title": "Quy trình áp dụng Chuẩn hóa/Co giãn để tránh rò rỉ dữ liệu (Data Leakage)",
                "description": "Kiến thức này tập trung vào quy trình chuẩn hóa và co giãn dữ liệu một cách chính xác để ngăn chặn rò rỉ dữ liệu, một lỗi nghiêm trọng có thể làm sai lệch kết quả đánh giá mô hình học máy. Việc hiểu và áp dụng các kỹ thuật như chỉ huấn luyện trên tập huấn luyện và áp dụng phép biến đổi cho tập kiểm tra là rất quan trọng để đảm bảo tính toàn vẹn và độ tin cậy của mô hình.",
                "keywords": [
                  "Data Leakage",
                  "Feature Scaling",
                  "Normalization",
                  "Standardization",
                  "Train-Test Split",
                  "Preprocessing Pipeline",
                  "Scikit-learn Pipeline",
                  "Cross-Validation",
                  "Data Transformation",
                  "Model Evaluation Integrity"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn Documentation: Preprocessing data",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html"
                  },
                  {
                    "name": "Scikit-learn Documentation: Pipelines and composite estimators",
                    "url": "https://scikit-learn.org/stable/modules/compose.html"
                  },
                  {
                    "name": "Kaggle Blog: Data Leakage",
                    "url": "https://www.kaggle.com/code/alexisbcook/data-leakage"
                  }
                ],
                "prerequisites": [
                  "data-normalization-scaling-fundamentals",
                  "machine-learning-data-splitting-validation",
                  "machine-learning-model-evaluation-basics"
                ],
                "projectIdeas": [
                  "Phát triển một quy trình tiền xử lý dữ liệu (preprocessing pipeline) sử dụng Scikit-learn để chuẩn hóa/co giãn dữ liệu cho một tập dữ liệu phân loại hoặc hồi quy, đảm bảo rằng các phép biến đổi chỉ được huấn luyện trên tập huấn luyện và áp dụng cho cả tập huấn luyện và tập kiểm tra.",
                  "Thực hiện một thí nghiệm so sánh: một mô hình được huấn luyện với chuẩn hóa/co giãn *có rò rỉ dữ liệu* (ví dụ: fit_transform trên toàn bộ dữ liệu trước khi chia) và một mô hình khác được huấn luyện *không rò rỉ dữ liệu*. Trình bày sự khác biệt về hiệu suất đánh giá trên tập kiểm tra và giải thích nguyên nhân."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "MLOps Engineer",
                  "Applied Scientist",
                  "Data Analyst (Advanced)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Scikit-learn",
                  "Pandas",
                  "NumPy",
                  "Jupyter Notebooks",
                  "VS Code"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "6-10 hours",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "MLOps Engineer",
                    "score": 9
                  },
                  {
                    "path": "Research Scientist (ML/AI)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "data-normalization-scaling-fundamentals",
                    "description": "Hiểu các phương pháp chuẩn hóa và co giãn cơ bản như StandardScaler, MinMaxScaler."
                  },
                  {
                    "type": "prerequisite",
                    "id": "machine-learning-data-splitting-validation",
                    "description": "Nắm vững kỹ thuật chia tách dữ liệu huấn luyện/kiểm tra và kiểm định chéo."
                  },
                  {
                    "type": "complementary",
                    "id": "feature-engineering-advanced-techniques",
                    "description": "Bổ trợ cho các kỹ thuật tiền xử lý dữ liệu và kỹ thuật đặc trưng tổng thể."
                  },
                  {
                    "type": "unlocks",
                    "id": "robust-ml-system-design",
                    "description": "Là nền tảng để thiết kế và triển khai các hệ thống học máy đáng tin cậy."
                  },
                  {
                    "type": "unlocks",
                    "id": "reliable-model-performance-evaluation",
                    "description": "Đảm bảo việc đánh giá hiệu suất mô hình là công bằng và phản ánh đúng khả năng của mô hình trong thực tế."
                  }
                ]
              }
            ],
            "skillName": "Chuẩn hóa và co giãn dữ liệu (Data Normalization & Scaling)",
            "competencyName": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
            "description": "Kỹ năng này cho phép bạn biến đổi các đặc trưng dữ liệu số về một phạm vi hoặc phân phối nhất định, giúp cải thiện hiệu suất của nhiều thuật toán học máy. Bạn sẽ học cách loại bỏ ảnh hưởng của thang đo khác nhau giữa các đặc trưng, đảm bảo không có đặc trưng nào chi phối quá mức quá trình huấn luyện mô hình. Thành thạo kỹ năng này giúp bạn chuẩn bị dữ liệu một cách hiệu quả cho việc xây dựng mô hình mạnh mẽ và ổn định hơn.",
            "learningResources": [
              {
                "title": "Chương về Tiền xử lý Dữ liệu",
                "type": "Book Chapter",
                "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/",
                "description": "Chương về 'Working with Data' hoặc 'Preprocessing the Data' trong sách 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow' của Aurélien Géron sẽ cung cấp cái nhìn tổng quan và chi tiết về các kỹ thuật chuẩn hóa/co giãn."
              },
              {
                "title": "Data Preprocessing for Machine Learning (Playlist)",
                "type": "Video Playlist",
                "url": "https://www.youtube.com/results?search_query=data+preprocessing+machine+learning+playlist",
                "description": "Một playlist video trên YouTube từ các kênh uy tín như Krish Naik, StatQuest, hoặc CodeBasics cung cấp các bài giảng thực hành và lý thuyết về các phương pháp tiền xử lý dữ liệu, bao gồm normalization và scaling."
              }
            ],
            "projectIdeas": [
              {
                "title": "Phân tích và Chuẩn hóa Dữ liệu Giá Nhà để Huấn luyện Mô hình",
                "description": "Sử dụng một tập dữ liệu giá nhà công khai (ví dụ: Boston Housing, California Housing trên Kaggle hoặc UCI). Thực hiện phân tích thống kê mô tả để hiểu phân phối của các đặc trưng số. Áp dụng ít nhất hai kỹ thuật chuẩn hóa/co giãn khác nhau (ví dụ: Min-Max Scaling và Z-score Scaling) lên các đặc trưng này, đảm bảo chia tập huấn luyện/kiểm tra trước để tránh rò rỉ dữ liệu. Sau đó, xây dựng một mô hình hồi quy (ví dụ: Linear Regression, Random Forest Regressor) và so sánh hiệu suất (MAE, RMSE, R²) của mô hình khi sử dụng dữ liệu chưa chuẩn hóa và dữ liệu đã được chuẩn hóa/co giãn bằng các phương pháp khác nhau. Trình bày kết quả và rút ra kết luận về phương pháp tốt nhất cho tập dữ liệu đã cho.",
                "estimatedTime": "6-8 giờ",
                "difficulty": "Intermediate"
              }
            ],
            "tools": [
              "Python",
              "scikit-learn (StandardScaler, MinMaxScaler)",
              "pandas",
              "numpy",
              "matplotlib/seaborn (để trực quan hóa phân phối dữ liệu)"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "8-12 hours",
            "importanceScore": 4
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_4_4",
            "name": "Mã hóa dữ liệu phân loại (Categorical Data Encoding)",
            "type": "skill",
            "children": [
              {
                "id": "data_science_big_data_categorical_encoding_nominal_ordinal_understanding",
                "name": "Hiểu biết về các loại dữ liệu phân loại (Nominal, Ordinal)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị khả năng phân biệt giữa dữ liệu phân loại danh nghĩa (Nominal) và dữ liệu phân loại có thứ tự (Ordinal). Việc hiểu rõ bản chất của từng loại là nền tảng quan trọng để lựa chọn phương pháp phân tích thống kê và kỹ thuật mã hóa dữ liệu phù hợp, ảnh hưởng trực tiếp đến chất lượng và hiệu quả của các mô hình học máy. Nắm vững sự khác biệt giúp ngăn ngừa các sai sót trong tiền xử lý dữ liệu và đưa ra kết luận chính xác hơn từ phân tích.",
                "keywords": [
                  "Dữ liệu phân loại",
                  "Dữ liệu danh nghĩa",
                  "Dữ liệu thứ tự",
                  "Thang đo đo lường",
                  "Loại biến",
                  "Xử lý sơ bộ dữ liệu",
                  "Kỹ thuật đặc trưng",
                  "Phân tích thống kê",
                  "Thang đo danh nghĩa",
                  "Thang đo thứ tự"
                ],
                "learningResources": [
                  {
                    "name": "Nominal, Ordinal, Interval, Ratio Scales With Examples - Investopedia",
                    "url": "https://www.investopedia.com/terms/n/nominal-ordinal-interval-ratio-scales.asp"
                  },
                  {
                    "name": "Understanding Data Types: Nominal, Ordinal, Interval, and Ratio - Statistics By Jim",
                    "url": "https://statisticsbyjim.com/basics/nominal-ordinal-interval-ratio-data/"
                  },
                  {
                    "name": "The Four Levels of Measurement in Research - Scribbr",
                    "url": "https://www.scribbr.com/statistics/levels-of-measurement/"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  {
                    "name": "Phân loại biến trong tập dữ liệu khảo sát",
                    "description": "Chọn một tập dữ liệu khảo sát công khai (ví dụ: từ Kaggle) và xác định từng cột dữ liệu là Nominal, Ordinal, hay các loại khác. Giải thích chi tiết lý do phân loại của bạn cho mỗi cột."
                  },
                  {
                    "name": "Đề xuất phương pháp mã hóa cho các biến phân loại",
                    "description": "Với một tập dữ liệu bất kỳ, nhận diện các biến phân loại Nominal và Ordinal. Sau đó, thảo luận về những thách thức và đề xuất sơ bộ các phương pháp mã hóa có thể phù hợp cho từng loại biến (chưa cần thực hiện mã hóa)."
                  }
                ],
                "relatedJobTitles": [
                  "Chuyên viên Khoa học Dữ liệu (Data Scientist)",
                  "Chuyên viên Phân tích Dữ liệu (Data Analyst)",
                  "Kỹ sư Học máy (Machine Learning Engineer)",
                  "Chuyên viên Phân tích Kinh doanh Thông minh (Business Intelligence Analyst)",
                  "Chuyên viên Thống kê (Statistician)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas, NumPy - cho việc quan sát dữ liệu)",
                  "R",
                  "Microsoft Excel (cho việc xem xét dữ liệu)",
                  "SQL (cho định nghĩa kiểu dữ liệu trong cơ sở dữ liệu)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Khoa học Dữ liệu (Data Science)",
                    "score": 10
                  },
                  {
                    "path": "Phân tích Dữ liệu (Data Analytics)",
                    "score": 10
                  },
                  {
                    "path": "Kỹ sư Học máy (Machine Learning Engineering)",
                    "score": 9
                  },
                  {
                    "path": "Kỹ sư Dữ liệu (Data Engineering)",
                    "score": 7
                  },
                  {
                    "path": "Phân tích Kinh doanh (Business Intelligence)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "skillId": "data_science_big_data_categorical_data_encoding_techniques",
                    "type": "unlocks",
                    "description": "Kiến thức về các loại dữ liệu Nominal và Ordinal là nền tảng để hiểu và lựa chọn các kỹ thuật mã hóa dữ liệu phân loại phù hợp (ví dụ: One-Hot Encoding, Label Encoding)."
                  },
                  {
                    "skillId": "data_science_big_data_basic_statistical_concepts_and_variables",
                    "type": "prerequisite",
                    "description": "Nắm vững các khái niệm thống kê cơ bản và sự khác biệt giữa các loại biến (định tính, định lượng) là điều kiện tiên quyết để hiểu sâu về dữ liệu phân loại."
                  },
                  {
                    "skillId": "data_science_big_data_data_cleaning_and_preprocessing",
                    "type": "complementary",
                    "description": "Việc nhận diện đúng loại dữ liệu giúp áp dụng các bước làm sạch và tiền xử lý dữ liệu (ví dụ: xử lý thiếu giá trị, chuẩn hóa) một cách hiệu quả và chính xác hơn."
                  }
                ]
              },
              {
                "id": "DS_BD_CatDataEncoding_Necessity",
                "name": "Lý do và sự cần thiết của việc mã hóa dữ liệu phân loại cho các thuật toán ML",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này giải thích tại sao dữ liệu phân loại cần được chuyển đổi thành định dạng số trước khi đưa vào các thuật toán học máy. Hầu hết các mô hình ML không thể xử lý trực tiếp dữ liệu phi số, do đó việc mã hóa là bước tiền xử lý thiết yếu để đảm bảo mô hình hoạt động hiệu quả và chính xác.",
                "keywords": [
                  "Dữ liệu phân loại",
                  "Mã hóa dữ liệu",
                  "Tiền xử lý dữ liệu",
                  "Học máy",
                  "Thuật toán ML",
                  "Kỹ thuật đặc trưng (Feature Engineering)",
                  "Dữ liệu số",
                  "Chuyển đổi dữ liệu",
                  "Hiệu suất mô hình",
                  "Đầu vào thuật toán"
                ],
                "learningResources": [
                  {
                    "title": "Preprocessing data - scikit-learn documentation",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"
                  },
                  {
                    "title": "A Comprehensive Guide to Categorical Data Encoding",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-categorical-data-encoding-5e04664a754"
                  },
                  {
                    "title": "Machine Learning Interview Questions: Categorical Data Encoding",
                    "url": "https://www.geeksforgeeks.org/machine-learning-interview-questions-categorical-data-encoding/"
                  }
                ],
                "prerequisites": [
                  "DS_BD_ML_BasicConcepts",
                  "DS_BD_DataTypes_Understanding"
                ],
                "projectIdeas": [
                  "Phân tích bộ dữ liệu 'Titanic' (hoặc một bộ dữ liệu khác có nhiều cột phân loại) bằng cách thử chạy một mô hình ML mà không mã hóa dữ liệu phân loại, sau đó mã hóa và so sánh kết quả. Viết báo cáo ngắn gọn về sự khác biệt.",
                  "Chọn một thuật toán phân loại (ví dụ: Hồi quy Logistic, Cây quyết định) và một bộ dữ liệu nhỏ có các biến phân loại. Thực hành các bước tiền xử lý, bao gồm mã hóa dữ liệu, và quan sát cách dữ liệu đầu vào ảnh hưởng đến khả năng huấn luyện của mô hình."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Data Engineer (tiền xử lý dữ liệu cho ML pipelines)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "Pandas",
                  "Scikit-learn"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 hours",
                "importanceScore": 9,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "complementary",
                    "id": "DS_BD_CatDataEncoding_OneHot",
                    "description": "Hiểu các lý do và sự cần thiết là nền tảng để học các kỹ thuật mã hóa cụ thể như One-Hot Encoding."
                  },
                  {
                    "type": "complementary",
                    "id": "DS_BD_CatDataEncoding_LabelOrdinal",
                    "description": "Tương tự, là cơ sở để nắm vững Label Encoding và Ordinal Encoding."
                  },
                  {
                    "type": "prerequisite",
                    "id": "DS_BD_DataPreprocessing_ForML",
                    "description": "Kiến thức này là một phần cốt lõi của quy trình tiền xử lý dữ liệu cho học máy."
                  },
                  {
                    "type": "unlocks",
                    "id": "DS_BD_ModelPerformance_Optimization",
                    "description": "Nắm vững nguyên tắc này giúp tối ưu hóa hiệu suất mô hình thông qua việc lựa chọn kỹ thuật mã hóa phù hợp."
                  }
                ]
              },
              {
                "id": "ds_bigdata_label_encoding_method_001",
                "name": "Phương pháp mã hóa nhãn (Label Encoding) và trường hợp sử dụng",
                "type": "knowledge",
                "children": [],
                "title": "Phương pháp mã hóa nhãn (Label Encoding) và trường hợp sử dụng",
                "description": "Mã hóa nhãn (Label Encoding) là một kỹ thuật tiền xử lý dữ liệu cơ bản, trong đó các giá trị phân loại (categorical values) được chuyển đổi thành các số nguyên (integers) duy nhất. Phương pháp này đặc biệt hữu ích khi các danh mục có mối quan hệ thứ tự (ordinal relationship) rõ ràng, giúp các thuật toán học máy có thể hiểu và xử lý dữ liệu hiệu quả hơn mà không ngụ ý mối quan hệ thứ bậc sai lệch giữa các danh mục.",
                "keywords": [
                  "Label Encoding",
                  "Categorical Data",
                  "Ordinal Data",
                  "Integer Encoding",
                  "Data Preprocessing",
                  "Feature Engineering",
                  "Scikit-learn",
                  "Machine Learning",
                  "Preprocessing"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn: LabelEncoder Documentation",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
                  },
                  {
                    "name": "Towards Data Science: A Comprehensive Guide to Encoding Categorical Features",
                    "url": "https://towardsdatascience.com/categorical-feature-encoding-67a627a858ae"
                  },
                  {
                    "name": "GeeksforGeeks: Label Encoding in Python with Scikit-learn",
                    "url": "https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/"
                  }
                ],
                "prerequisites": [
                  "ds_bigdata_categorical_data_intro_001"
                ],
                "projectIdeas": [
                  "Áp dụng Label Encoding cho cột 'Education Level' (ví dụ: 'High School', 'Bachelor', 'Master', 'PhD') trong một tập dữ liệu giả định về khách hàng và sau đó sử dụng dữ liệu đã mã hóa để huấn luyện một mô hình dự đoán mức độ hài lòng của khách hàng.",
                  "Phân tích sự khác biệt về hiệu suất của một mô hình phân loại (ví dụ: Random Forest) khi sử dụng Label Encoding so với không mã hóa (nếu thuật toán hỗ trợ) trên một tập dữ liệu có các biến phân loại thứ tự."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Business Intelligence Analyst"
                ],
                "marketDemand": "High",
                "tools": [
                  "Python",
                  "scikit-learn",
                  "Pandas",
                  "Jupyter Notebook"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 hours",
                "importanceScore": 8,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "AI Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "ds_bigdata_categorical_data_intro_001",
                    "description": "Hiểu biết cơ bản về dữ liệu phân loại và các loại dữ liệu trong học máy."
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bigdata_one_hot_encoding_002",
                    "description": "Phương pháp mã hóa One-Hot (One-Hot Encoding)"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bigdata_ml_preprocessing_003",
                    "description": "Tiền xử lý dữ liệu nâng cao cho Học máy và xây dựng pipeline"
                  }
                ]
              },
              {
                "id": "ds-bd-categorical-encoding-one-hot",
                "name": "Phương pháp mã hóa One-Hot (One-Hot Encoding) và trường hợp sử dụng",
                "type": "knowledge",
                "children": [],
                "category": "Mã hóa dữ liệu phân loại (Categorical Data Encoding)",
                "domain": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
                "description": "One-Hot Encoding là một kỹ thuật chuyển đổi biến phân loại (categorical variables) thành một định dạng số mà các thuật toán học máy có thể hiểu và xử lý. Phương pháp này tạo ra các biến nhị phân mới (còn gọi là biến giả - dummy variables) cho mỗi giá trị duy nhất trong cột phân loại gốc, giúp ngăn chặn thuật toán ngầm định một mối quan hệ thứ tự sai lệch giữa các danh mục.",
                "keywords": [
                  "One-Hot Encoding",
                  "Categorical Data",
                  "Feature Engineering",
                  "Dummy Variables",
                  "Sparse Matrix",
                  "Data Preprocessing",
                  "Nominal Data",
                  "Scikit-learn",
                  "Pandas",
                  "Machine Learning"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn: OneHotEncoder documentation",
                    "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
                  },
                  {
                    "name": "Towards Data Science: Understanding One-Hot Encoding",
                    "url": "https://towardsdatascience.com/understanding-one-hot-encoding-a-quick-and-intuitive-explanation-for-the-layman-908ae325e83"
                  },
                  {
                    "name": "IBM: What is One-Hot Encoding?",
                    "url": "https://www.ibm.com/topics/one-hot-encoding"
                  }
                ],
                "prerequisites": [
                  "ds-bd-data-types-fundamentals",
                  "ds-bd-data-preprocessing-overview"
                ],
                "projectIdeas": [
                  "Áp dụng One-Hot Encoding để tiền xử lý dữ liệu phân loại trong bộ dữ liệu Ames House Price Prediction và so sánh hiệu suất mô hình với và không có mã hóa.",
                  "Sử dụng One-Hot Encoding để chuyển đổi các đặc trưng địa lý (quốc gia, thành phố) trong bộ dữ liệu khách hàng và xây dựng mô hình phân loại khách hàng tiềm năng."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "Data Analyst",
                  "AI Engineer",
                  "Quantitative Analyst"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "Pandas",
                  "Scikit-learn"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "2-4 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Data Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "targetSkillId": "ds-bd-data-types-fundamentals"
                  },
                  {
                    "type": "prerequisite",
                    "targetSkillId": "ds-bd-data-preprocessing-overview"
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "ds-bd-categorical-encoding-label-encoding"
                  },
                  {
                    "type": "complementary",
                    "targetSkillId": "ds-bd-categorical-encoding-target-encoding"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "ds-bd-ml-model-building-categorical"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "ds-bd-feature-engineering-optimization"
                  }
                ]
              },
              {
                "id": "data_science_categorical_encoding_evaluation_selection",
                "name": "Đánh giá ưu nhược điểm và lựa chọn kỹ thuật mã hóa phù hợp",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc hiểu rõ các kỹ thuật mã hóa dữ liệu phân loại khác nhau, phân tích ưu nhược điểm của chúng, và cách lựa chọn kỹ thuật phù hợp nhất cho từng tập dữ liệu và mô hình học máy cụ thể. Việc lựa chọn đúng kỹ thuật mã hóa có ảnh hưởng lớn đến hiệu suất mô hình, giúp tránh các vấn đề như dữ liệu nhiễu, sai lệch thứ tự, hoặc tăng kích thước dữ liệu không cần thiết.",
                "keywords": [
                  "Mã hóa dữ liệu phân loại",
                  "Categorical Encoding",
                  "One-Hot Encoding",
                  "Label Encoding",
                  "Target Encoding",
                  "Binary Encoding",
                  "Feature Engineering",
                  "Xử lý tiền dữ liệu",
                  "Ưu nhược điểm mã hóa",
                  "Lựa chọn kỹ thuật mã hóa",
                  "Machine Learning Preprocessing"
                ],
                "learningResources": [
                  {
                    "name": "Scikit-learn - Encoding categorical features",
                    "url": "https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"
                  },
                  {
                    "name": "Different Types of Categorical Encoding - GeeksforGeeks",
                    "url": "https://www.geeksforgeeks.org/different-types-of-categorical-encoding/"
                  },
                  {
                    "name": "A Comprehensive Guide to Categorical Feature Encoding - Towards Data Science",
                    "url": "https://towardsdatascience.com/a-comprehensive-guide-to-categorical-feature-encoding-b0429f4b5d23"
                  }
                ],
                "prerequisites": [
                  "data_science_categorical_encoding_one_hot",
                  "data_science_categorical_encoding_label",
                  "data_science_categorical_encoding_target",
                  "data_science_categorical_encoding_binary",
                  "data_science_data_preprocessing_basics",
                  "data_science_machine_learning_model_types"
                ],
                "projectIdeas": [
                  {
                    "name": "So sánh hiệu suất mã hóa",
                    "description": "Chọn một tập dữ liệu có các biến phân loại, áp dụng ít nhất ba kỹ thuật mã hóa khác nhau (ví dụ: One-Hot, Label, Target Encoding). Huấn luyện một mô hình học máy đơn giản (ví dụ: Logistic Regression hoặc Decision Tree) với từng phiên bản dữ liệu đã mã hóa và so sánh hiệu suất mô hình bằng các chỉ số phù hợp (precision, recall, F1-score, accuracy)."
                  },
                  {
                    "name": "Xử lý dữ liệu có cardinality cao",
                    "description": "Tìm một tập dữ liệu có ít nhất một biến phân loại có cardinality cao. Nghiên cứu và áp dụng các kỹ thuật mã hóa chuyên biệt cho cardinality cao (ví dụ: Frequency Encoding, Feature Hashing, Weight of Evidence) và đánh giá tác động của chúng đến hiệu suất mô hình so với các kỹ thuật mã hóa truyền thống."
                  }
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Machine Learning Engineer",
                  "AI Engineer",
                  "Data Analyst",
                  "Big Data Engineer",
                  "Research Scientist (AI/ML)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "scikit-learn",
                  "pandas",
                  "numpy",
                  "category_encoders"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "10-15 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 10
                  },
                  {
                    "path": "AI Engineer",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 7
                  },
                  {
                    "path": "Big Data Engineer",
                    "score": 8
                  },
                  {
                    "path": "Research Scientist (AI/ML)",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data_science_categorical_encoding_one_hot",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_categorical_encoding_label",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_categorical_encoding_target",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_categorical_encoding_binary",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_data_preprocessing_basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "data_science_machine_learning_model_evaluation",
                    "type": "complementary"
                  },
                  {
                    "id": "data_science_feature_engineering_advanced",
                    "type": "unlocks"
                  },
                  {
                    "id": "data_science_building_production_ml_pipelines",
                    "type": "unlocks"
                  }
                ]
              }
            ],
            "skillName": "Mã hóa dữ liệu phân loại (Categorical Data Encoding)",
            "capability": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
            "description": "Kỹ năng này trang bị cho bạn kiến thức và khả năng chuyển đổi các loại dữ liệu phân loại (danh nghĩa, thứ tự) thành định dạng số mà các thuật toán học máy có thể xử lý. Bạn sẽ hiểu được sự cần thiết, ưu nhược điểm của các phương pháp mã hóa phổ biến như Label Encoding và One-Hot Encoding, từ đó lựa chọn kỹ thuật phù hợp nhất cho tập dữ liệu và mô hình của mình.",
            "learningResources": [
              {
                "title": "Chương 'Preprocessing Data' trong sách 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow'",
                "type": "Sách",
                "url": "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"
              },
              {
                "title": "Playlist: Categorical Data Encoding Tutorials",
                "type": "Video Series",
                "url": "https://www.youtube.com/results?search_query=categorical+data+encoding+tutorial+python"
              }
            ],
            "projectIdeas": [
              "Phát triển một quy trình tiền xử lý dữ liệu toàn diện cho một tập dữ liệu có chứa các biến phân loại. Thực hiện và so sánh ít nhất hai phương pháp mã hóa khác nhau (ví dụ: Label Encoding và One-Hot Encoding) trên cùng một biến, sau đó huấn luyện một mô hình học máy đơn giản (như Logistic Regression hoặc Decision Tree) với mỗi phiên bản dữ liệu được mã hóa để phân tích tác động của phương pháp mã hóa đến hiệu suất mô hình."
            ],
            "tools": [
              "Python",
              "pandas",
              "scikit-learn"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "4-8 giờ",
            "importanceScore": 8
          }
        ],
        "abilityName": "Tiền xử lý và Làm sạch Dữ liệu (Data Preprocessing & Cleaning)",
        "specialization": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
        "description": "Tiền xử lý và làm sạch dữ liệu là một năng lực cốt lõi trong Khoa học dữ liệu, vì dữ liệu thực tế thường không hoàn chỉnh, không nhất quán hoặc chứa nhiễu. Năng lực này đảm bảo chất lượng dữ liệu cao, tạo nền tảng vững chắc để xây dựng các mô hình phân tích và học máy hiệu quả, từ đó mang lại kết quả đáng tin cậy và có giá trị.",
        "skills": [
          "Xử lý giá trị thiếu (Missing Value Imputation)",
          "Phát hiện và xử lý dữ liệu ngoại lai (Outlier Detection & Treatment)",
          "Chuẩn hóa và co giãn dữ liệu (Data Normalization & Scaling)",
          "Mã hóa dữ liệu phân loại (Categorical Data Encoding)"
        ],
        "learningResources": [
          {
            "title": "Sách 'Python for Data Analysis' (chương Cleaning & Preparing Data) của Wes McKinney",
            "type": "Sách",
            "url": "https://wesmckinney.com/book/"
          },
          {
            "title": "Khóa học 'Practical Data Science Specialization' (đặc biệt các module về Data Wrangling) trên Coursera",
            "type": "Khóa học trực tuyến",
            "url": "https://www.coursera.org/specializations/practical-data-science"
          }
        ],
        "projectIdeas": [
          {
            "title": "Xây dựng Hệ thống Tiền xử lý Dữ liệu Tự động cho Tập dữ liệu Đa dạng",
            "description": "Phát triển một pipeline tiền xử lý dữ liệu hoàn chỉnh cho một tập dữ liệu lớn và phức tạp được thu thập từ nhiều nguồn (ví dụ: dữ liệu giao dịch bán lẻ kết hợp với dữ liệu đánh giá khách hàng và dữ liệu sản phẩm từ API). Dự án sẽ bao gồm các bước từ hợp nhất dữ liệu, xử lý giá trị thiếu, phát hiện và loại bỏ ngoại lai, đến chuẩn hóa và mã hóa các biến để sẵn sàng cho phân tích dự đoán về hành vi mua sắm hoặc khuyến nghị sản phẩm."
          }
        ],
        "tools": [
          "Python (Pandas, NumPy, Scikit-learn)",
          "R",
          "SQL",
          "Apache Spark (PySpark/SparkR cho dữ liệu lớn)",
          "OpenRefine"
        ],
        "difficultyLevel": "Trung bình",
        "estimatedTimeToComplete": "40-80 giờ học và thực hành chuyên sâu",
        "importanceScore": "5/5"
      },
      {
        "id": "ability_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5",
        "name": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
        "type": "ability",
        "children": [
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5_1",
            "name": "Kể chuyện bằng dữ liệu (Data Storytelling)",
            "type": "skill",
            "children": [
              {
                "id": "data_storytelling_audience_goals",
                "name": "Xác định Đối tượng và Mục tiêu Câu chuyện Dữ liệu",
                "type": "knowledge",
                "children": [],
                "skill": "Kể chuyện bằng dữ liệu (Data Storytelling)",
                "domain": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
                "description": "Kiến thức này trang bị khả năng nhận diện chính xác đối tượng người nghe hoặc người đọc của một câu chuyện dữ liệu, cũng như xác định rõ ràng mục tiêu mà câu chuyện đó cần đạt được. Việc hiểu rõ đối tượng và mục tiêu là nền tảng để xây dựng một câu chuyện dữ liệu thuyết phục, phù hợp và có tác động, đảm bảo thông điệp được truyền tải hiệu quả và thúc đẩy hành động mong muốn.",
                "keywords": [
                  "Phân tích đối tượng",
                  "Xác định mục tiêu",
                  "Câu chuyện dữ liệu",
                  "Thông điệp chính",
                  "Kêu gọi hành động",
                  "Người ra quyết định",
                  "Nhu cầu kinh doanh",
                  "Stakeholder analysis",
                  "Business objectives",
                  "Data narrative"
                ],
                "learningResources": [
                  {
                    "name": "How to Tell a Story with Data - Harvard Business Review",
                    "url": "https://hbr.org/2021/04/how-to-tell-a-story-with-data"
                  },
                  {
                    "name": "Data Storytelling: What It Is, Why It Matters, & How to Do It - Tableau",
                    "url": "https://www.tableau.com/about/blog/2020/4/data-storytelling"
                  },
                  {
                    "name": "How to Tell a Data Story: Step by Step Guide (with Examples) - DataCamp",
                    "url": "https://www.datacamp.com/blog/how-to-tell-a-data-story"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Chọn một tập dữ liệu công khai (ví dụ: dữ liệu COVID-19, dữ liệu kinh tế địa phương) và xác định ít nhất hai đối tượng khác nhau (ví dụ: công chúng, nhà hoạch định chính sách, doanh nghiệp). Với mỗi đối tượng, hãy đặt ra một mục tiêu cụ thể cho câu chuyện dữ liệu bạn muốn kể, bao gồm thông điệp chính và kêu gọi hành động.",
                  "Phỏng vấn một người trong lĩnh vực kinh doanh hoặc một stakeholder giả định để thu thập yêu cầu và xác định mục tiêu chính mà họ mong muốn thấy từ một phân tích dữ liệu cụ thể, sau đó trình bày lại những phát hiện về đối tượng và mục tiêu này."
                ],
                "relatedJobTitles": [
                  "Nhà khoa học dữ liệu (Data Scientist)",
                  "Nhà phân tích dữ liệu (Data Analyst)",
                  "Chuyên viên Phân tích kinh doanh (Business Intelligence Analyst)",
                  "Chuyên gia Kể chuyện dữ liệu (Data Storyteller)",
                  "Tư vấn viên (Consultant)",
                  "Quản lý sản phẩm (Product Manager)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Miro (bảng trắng cộng tác)",
                  "Google Docs / Confluence (ghi chú và tài liệu)",
                  "Các công cụ phỏng vấn/khảo sát (như Google Forms, SurveyMonkey)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "3-5 giờ",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 10
                  },
                  {
                    "path": "Consultant",
                    "score": 9
                  },
                  {
                    "path": "Product Manager",
                    "score": 8
                  },
                  {
                    "path": "Marketing Analyst",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data_storytelling_structure_flow",
                    "type": "unlocks",
                    "description": "Là tiền đề để xây dựng cấu trúc và luồng câu chuyện dữ liệu hiệu quả, đảm bảo thông điệp được truyền tải một cách mạch lạc và có logic."
                  },
                  {
                    "id": "data_visualization_selection_design",
                    "type": "unlocks",
                    "description": "Hướng dẫn lựa chọn biểu đồ và hình ảnh trực quan phù hợp nhất với đối tượng và mục tiêu đã xác định, tăng cường khả năng truyền tải thông điệp."
                  },
                  {
                    "id": "data_report_dashboard_design",
                    "type": "unlocks",
                    "description": "Đảm bảo thiết kế báo cáo và bảng điều khiển (dashboard) truyền tải thông điệp chính xác và hấp dẫn đến đối tượng người xem."
                  },
                  {
                    "id": "data_story_communication_presentation",
                    "type": "unlocks",
                    "description": "Nền tảng cho việc luyện tập và thực hiện các buổi thuyết trình, truyền đạt câu chuyện dữ liệu một cách thuyết phục và có tác động."
                  },
                  {
                    "id": "stakeholder_management_communication",
                    "type": "complementary",
                    "description": "Củng cố kỹ năng quản lý và giao tiếp với các bên liên quan để thu thập yêu cầu và truyền đạt kết quả hiệu quả."
                  },
                  {
                    "id": "critical_thinking_problem_solving",
                    "type": "complementary",
                    "description": "Hỗ trợ đánh giá và tinh chỉnh mục tiêu, đối tượng một cách khách quan, cũng như xác định các vấn đề cốt lõi cần giải quyết bằng dữ liệu."
                  },
                  {
                    "id": "business_acumen_domain_knowledge",
                    "type": "complementary",
                    "description": "Giúp hiểu rõ ngữ cảnh kinh doanh và các vấn đề chuyên ngành để xác định mục tiêu và đối tượng một cách sâu sắc và chính xác."
                  }
                ]
              },
              {
                "id": "data_science_data_storytelling_discover_insights",
                "name": "Khám phá và Phát hiện Insight Quan trọng từ Dữ liệu",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các kỹ thuật phân tích và diễn giải dữ liệu để tìm ra các mẫu, xu hướng, ngoại lệ và mối quan hệ ẩn. Mục tiêu là chuyển đổi dữ liệu thô thành những thông tin chi tiết có giá trị, giúp đưa ra quyết định sáng suốt và thúc đẩy hành động. Đây là nền tảng cốt lõi cho việc kể chuyện bằng dữ liệu hiệu quả.",
                "keywords": [
                  "Phân tích khám phá dữ liệu",
                  "EDA",
                  "Phát hiện mẫu",
                  "Nhận diện xu hướng",
                  "Phân tích ngoại lệ",
                  "Tương quan",
                  "Diễn giải dữ liệu",
                  "Insight từ dữ liệu",
                  "Phân tích định tính",
                  "Suy luận từ dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "Google Data Analytics Professional Certificate",
                    "url": "https://www.coursera.org/professional-certificates/google-data-analytics",
                    "type": "Course"
                  },
                  {
                    "name": "Storytelling with Data: A Data Visualization Guide for Business Professionals by Cole Nussbaumer Knaflic",
                    "url": "https://www.storytellingwithdata.com/book",
                    "type": "Book"
                  },
                  {
                    "name": "Exploratory Data Analysis (EDA): A Step-by-Step Guide",
                    "url": "https://www.geeksforgeeks.org/exploratory-data-analysis-eda-a-step-by-step-guide/",
                    "type": "Article"
                  }
                ],
                "prerequisites": [
                  "data_science_basic_data_cleaning",
                  "data_science_statistical_fundamentals",
                  "data_science_basic_data_visualization",
                  "data_science_data_querying_sql"
                ],
                "projectIdeas": [
                  "Phân tích bộ dữ liệu bán hàng của một cửa hàng thương mại điện tử để xác định sản phẩm bán chạy nhất, thời điểm mua hàng cao điểm và mối quan hệ giữa các danh mục sản phẩm. Trình bày 3-5 insight quan trọng.",
                  "Sử dụng bộ dữ liệu về dịch vụ khách hàng (ví dụ: phản hồi khảo sát, nhật ký cuộc gọi) để khám phá các điểm đau (pain points) phổ biến của khách hàng, xác định các xu hướng tiêu cực hoặc tích cực và đề xuất các cải tiến dịch vụ dựa trên dữ liệu."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Scientist",
                  "Product Analyst",
                  "Market Research Analyst",
                  "Consultant",
                  "Reporting Specialist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python (Pandas, NumPy, Matplotlib, Seaborn)",
                  "R (dplyr, ggplot2)",
                  "SQL",
                  "Tableau",
                  "Microsoft Power BI",
                  "Excel (Advanced Features)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "30-50 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Product Manager",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "skillId": "data_science_basic_data_cleaning",
                    "type": "prerequisite",
                    "description": "Khả năng làm sạch và chuẩn bị dữ liệu là cần thiết để đảm bảo chất lượng đầu vào trước khi phân tích khám phá."
                  },
                  {
                    "skillId": "data_science_statistical_fundamentals",
                    "type": "prerequisite",
                    "description": "Hiểu biết về thống kê cơ bản giúp phân tích, diễn giải các mẫu, mối quan hệ và sự khác biệt có ý nghĩa thống kê trong dữ liệu."
                  },
                  {
                    "skillId": "data_science_basic_data_visualization",
                    "type": "prerequisite",
                    "description": "Kỹ năng trực quan hóa dữ liệu cơ bản là công cụ mạnh mẽ để khám phá các insight ban đầu và trình bày chúng một cách rõ ràng."
                  },
                  {
                    "skillId": "data_storytelling_crafting_narratives",
                    "type": "complementary",
                    "description": "Các insight được phát hiện là nguyên liệu chính và cốt lõi để xây dựng câu chuyện dữ liệu thuyết phục và có tác động."
                  },
                  {
                    "skillId": "data_science_advanced_analytics_modeling",
                    "type": "unlocks",
                    "description": "Những insight ban đầu thường định hướng cho các phân tích sâu hơn, xây dựng mô hình dự đoán và các phương pháp phân tích nâng cao khác."
                  }
                ]
              },
              {
                "id": "cau_truc_dong_chay_cau_chuyen_du_lieu",
                "name": "Cấu trúc và Dòng chảy của Câu chuyện Dữ liệu",
                "type": "knowledge",
                "children": [],
                "title": "Cấu trúc và Dòng chảy của Câu chuyện Dữ liệu",
                "description": "Kiến thức này tập trung vào việc định hình và tổ chức các thông tin chi tiết từ dữ liệu thành một câu chuyện mạch lạc và hấp dẫn. Nó bao gồm việc xác định điểm khởi đầu, phát triển tình tiết, tạo ra cao trào và kết luận rõ ràng, giúp người nghe dễ dàng tiếp thu và ghi nhớ thông điệp. Nắm vững kiến thức này là thiết yếu để truyền tải những hiểu biết phức tạp từ dữ liệu một cách hiệu quả, thuyết phục người ra quyết định và thúc đẩy hành động.",
                "keywords": [
                  "câu chuyện dữ liệu",
                  "cấu trúc kể chuyện",
                  "dòng chảy thông tin",
                  "điểm nhấn dữ liệu",
                  "kịch bản dữ liệu",
                  "thông điệp cốt lõi",
                  "tương tác khán giả",
                  "call to action",
                  "tính thuyết phục",
                  "diễn giải dữ liệu"
                ],
                "learningResources": [
                  {
                    "title": "How to Tell a Story with Data",
                    "url": "https://hbr.org/2021/04/how-to-tell-a-story-with-data"
                  },
                  {
                    "title": "What is data storytelling? A complete guide",
                    "url": "https://www.tableau.com/learn/articles/data-storytelling"
                  },
                  {
                    "title": "The Narrative Arc: Definition and Examples",
                    "url": "https://www.thoughtco.com/narrative-arc-1857143"
                  }
                ],
                "prerequisites": [
                  "phan_tich_du_lieu_co_ban",
                  "truc_quan_hoa_du_lieu_co_ban",
                  "xac_dinh_muc_tieu_va_doi_tuong_cau_chuyen_du_lieu"
                ],
                "projectIdeas": [
                  "Chọn một tập dữ liệu công khai (ví dụ: từ Kaggle hoặc dữ liệu chính phủ), phân tích và xây dựng một câu chuyện dữ liệu hoàn chỉnh, bao gồm phần giới thiệu, phát triển các điểm dữ liệu chính, cao trào (insight quan trọng nhất) và lời kêu gọi hành động.",
                  "Với một báo cáo hoặc dashboard hiện có, hãy đánh giá cấu trúc câu chuyện của nó và đề xuất các cải tiến để làm cho thông điệp rõ ràng, mạch lạc và thuyết phục hơn, có thể sắp xếp lại các biểu đồ hoặc thêm văn bản dẫn dắt."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Storyteller",
                  "Analytics Consultant",
                  "Data Evangelist",
                  "Product Manager",
                  "Marketing Analyst"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Google Data Studio",
                  "Microsoft PowerPoint",
                  "Google Slides",
                  "Keynote",
                  "Python (Matplotlib, Seaborn)",
                  "R (ggplot2)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 10
                  },
                  {
                    "path": "Analytics Consultant",
                    "score": 10
                  },
                  {
                    "path": "Product Manager",
                    "score": 7
                  },
                  {
                    "path": "Marketing Analyst",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "phan_tich_du_lieu_co_ban",
                    "type": "prerequisite"
                  },
                  {
                    "id": "truc_quan_hoa_du_lieu_co_ban",
                    "type": "prerequisite"
                  },
                  {
                    "id": "xac_dinh_muc_tieu_va_doi_tuong_cau_chuyen_du_lieu",
                    "type": "prerequisite"
                  },
                  {
                    "id": "thiet_ke_truc_quan_hoa_du_lieu_nang_cao",
                    "type": "complementary"
                  },
                  {
                    "id": "ky_nang_thuyet_trinh_hieu_qua",
                    "type": "complementary"
                  },
                  {
                    "id": "tu_duy_phe_phan_va_giai_quyet_van_de_voi_du_lieu",
                    "type": "complementary"
                  },
                  {
                    "id": "phat_trien_bao_cao_va_bang_dieu_khien_tuong_tac",
                    "type": "unlocks"
                  },
                  {
                    "id": "xay_dung_chien_luoc_du_lieu",
                    "type": "unlocks"
                  },
                  {
                    "id": "truyen_thong_khoa_hoc_du_lieu",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "DS_DT_VP_Principles",
                "name": "Nguyên tắc Thiết kế và Lựa chọn Trực quan hóa Hiệu quả",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào các nguyên tắc cốt lõi để tạo ra các biểu đồ và trực quan hóa dữ liệu rõ ràng, chính xác và có sức thuyết phục. Nắm vững các nguyên tắc này là rất quan trọng để đảm bảo thông điệp được truyền tải hiệu quả, giúp người xem hiểu dữ liệu nhanh chóng và đưa ra quyết định dựa trên các hiểu biết sâu sắc, từ đó nâng cao chất lượng của việc kể chuyện bằng dữ liệu.",
                "keywords": [
                  "Trực quan hóa Dữ liệu",
                  "Thiết kế Trực quan",
                  "Nguyên tắc Gestalt",
                  "Tỷ lệ Data-Ink",
                  "Lựa chọn Biểu đồ",
                  "Lý thuyết Màu sắc",
                  "Tính dễ tiếp cận",
                  "Kể chuyện bằng Dữ liệu",
                  "Độ chính xác Thị giác",
                  "Khả năng đọc hiểu"
                ],
                "learningResources": [
                  {
                    "title": "Storytelling with Data: A Data Visualization Guide for Business Professionals",
                    "url": "https://www.storytellingwithdata.com/book"
                  },
                  {
                    "title": "Data Visualization Best Practices (Tableau)",
                    "url": "https://www.tableau.com/learn/articles/data-visualization-best-practices"
                  },
                  {
                    "title": "The Visual Display of Quantitative Information by Edward Tufte",
                    "url": "https://www.edwardtufte.com/tufte/books_vdqi"
                  }
                ],
                "prerequisites": [
                  "DS_DT_001_Cơ_bản_về_Trực_quan_hóa_Dữ_liệu",
                  "DS_DT_002_Hiểu_biết_về_Các_loại_Dữ_liệu"
                ],
                "projectIdeas": [
                  {
                    "name": "Phân tích và Tối ưu hóa Biểu đồ Hiện có",
                    "description": "Lựa chọn một biểu đồ kém hiệu quả (tìm trên internet hoặc từ báo cáo thực tế), phân tích các điểm yếu dựa trên các nguyên tắc thiết kế đã học, sau đó thiết kế lại biểu đồ đó để truyền tải thông điệp rõ ràng và hiệu quả hơn, kèm theo giải thích chi tiết."
                  },
                  {
                    "name": "Xây dựng Trực quan hóa theo Kịch bản Kinh doanh",
                    "description": "Với một bộ dữ liệu nhỏ và một câu hỏi kinh doanh cụ thể (ví dụ: 'Doanh số bán hàng theo khu vực nào đang tăng trưởng nhanh nhất?'), hãy lựa chọn loại biểu đồ phù hợp nhất và thiết kế trực quan hóa dữ liệu, áp dụng ít nhất 3 nguyên tắc thiết kế hiệu quả. Viết một đoạn mô tả ngắn giải thích lý do lựa chọn."
                  }
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Data Scientist",
                  "Business Intelligence Developer",
                  "Data Visualization Specialist",
                  "Data Storyteller",
                  "UX/UI Designer (Data-focused)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "D3.js",
                  "Google Looker Studio"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 10
                  },
                  {
                    "path": "Data Visualization Specialist",
                    "score": 10
                  },
                  {
                    "path": "Consultant (Data/Analytics)",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS_DT_001_Cơ_bản_về_Trực_quan_hóa_Dữ_liệu",
                    "type": "prerequisite",
                    "description": "Cần nắm vững các khái niệm cơ bản về trực quan hóa dữ liệu và các loại biểu đồ phổ biến trước khi đi sâu vào nguyên tắc thiết kế hiệu quả."
                  },
                  {
                    "id": "DS_DT_003_Tâm_lý_học_Nhận_thức_Thị_giác",
                    "type": "complementary",
                    "description": "Hiểu biết về cách bộ não con người xử lý thông tin thị giác giúp áp dụng các nguyên tắc thiết kế hiệu quả hơn và tạo ra các trực quan hóa dễ hiểu hơn."
                  },
                  {
                    "id": "DS_DT_004_Thiết_kế_Bảng_điều_khiển_Dashboard_và_Báo_cáo_Tương_tác",
                    "type": "unlocks",
                    "description": "Nắm vững nguyên tắc thiết kế là nền tảng cốt lõi để tạo ra các bảng điều khiển (dashboard) và báo cáo tương tác, hiệu quả, đáp ứng nhu cầu người dùng."
                  },
                  {
                    "id": "DS_DT_005_Kể_chuyện_bằng_Dữ_liệu_Nâng_cao_và_Thuyết_trình",
                    "type": "unlocks",
                    "description": "Các nguyên tắc thiết kế trực quan hiệu quả là thành phần cốt lõi để xây dựng một câu chuyện dữ liệu hấp dẫn và thuyết phục, đồng thời cải thiện kỹ năng thuyết trình dữ liệu."
                  }
                ]
              },
              {
                "id": "data_contextualization_interpretation",
                "name": "Bối cảnh hóa và Diễn giải Dữ liệu để tạo Ý nghĩa",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị khả năng đặt dữ liệu vào đúng bối cảnh, hiểu rõ nguồn gốc, hạn chế và các yếu tố bên ngoài ảnh hưởng đến nó. Việc diễn giải dữ liệu một cách chính xác cho phép chuyển đổi các con số và biểu đồ thành những câu chuyện có ý nghĩa, hỗ trợ quá trình ra quyết định hiệu quả. Đây là nền tảng để biến dữ liệu thô thành thông tin chi tiết có giá trị, dễ hiểu cho nhiều đối tượng.",
                "keywords": [
                  "Data Contextualization",
                  "Data Interpretation",
                  "Meaning-making",
                  "Data Storytelling",
                  "Business Acumen",
                  "Domain Knowledge",
                  "Causal Inference",
                  "Statistical Significance",
                  "Explanatory Analysis",
                  "Insights Generation"
                ],
                "learningResources": [
                  {
                    "title": "Learning to Tell a Story with Data",
                    "url": "https://hbr.org/2019/04/learning-to-tell-a-story-with-data",
                    "type": "Article"
                  },
                  {
                    "title": "Data Storytelling: How to tell a story with data",
                    "url": "https://www.tableau.com/learn/articles/data-storytelling",
                    "type": "Article/Guide"
                  },
                  {
                    "title": "Storytelling with Data: A Data Visualization Guide for Business Professionals (Website/Concepts)",
                    "url": "https://www.storytellingwithdata.com/",
                    "type": "Website/Book Concepts"
                  }
                ],
                "prerequisites": [
                  "data_exploration_analysis",
                  "basic_statistical_concepts",
                  "data_visualization_fundamentals",
                  "business_question_framing"
                ],
                "projectIdeas": [
                  "Phân tích một tập dữ liệu công khai (ví dụ: từ Kaggle hoặc WHO) và tạo một báo cáo giải thích các phát hiện chính, đặt chúng trong bối cảnh thực tế và đề xuất ý nghĩa cho một đối tượng không chuyên về kỹ thuật.",
                  "Chọn một vấn đề kinh doanh cụ thể, sử dụng dữ liệu giả định hoặc dữ liệu mở có sẵn để xác định các yếu tố ảnh hưởng, và trình bày một 'câu chuyện dữ liệu' mạch lạc giải thích nguyên nhân, tác động và gợi ý hành động."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Storyteller",
                  "Analytics Consultant",
                  "Product Manager"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Looker",
                  "Python (Pandas, Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "Microsoft Excel",
                  "PowerPoint/Google Slides"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Analytics Consultant",
                    "score": 10
                  },
                  {
                    "path": "Product Manager",
                    "score": 8
                  },
                  {
                    "path": "Marketing Analyst",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "data_exploration_analysis",
                    "description": "Khả năng khám phá và phân tích dữ liệu là cần thiết để có dữ liệu thô để diễn giải."
                  },
                  {
                    "type": "prerequisite",
                    "id": "basic_statistical_concepts",
                    "description": "Hiểu biết về thống kê cơ bản giúp diễn giải các con số một cách chính xác và đáng tin cậy."
                  },
                  {
                    "type": "prerequisite",
                    "id": "data_visualization_fundamentals",
                    "description": "Kỹ năng trực quan hóa dữ liệu là công cụ mạnh mẽ để truyền tải bối cảnh và ý nghĩa một cách trực quan."
                  },
                  {
                    "type": "prerequisite",
                    "id": "business_question_framing",
                    "description": "Hiểu rõ câu hỏi kinh doanh giúp định hướng việc bối cảnh hóa và diễn giải dữ liệu cho phù hợp."
                  },
                  {
                    "type": "complementary",
                    "id": "effective_data_visualization",
                    "description": "Bổ trợ cho việc trình bày các phát hiện dữ liệu một cách hấp dẫn và dễ hiểu."
                  },
                  {
                    "type": "complementary",
                    "id": "audience_tailoring_communication",
                    "description": "Khả năng điều chỉnh cách kể chuyện và diễn giải cho phù hợp với đối tượng khán giả cụ thể."
                  },
                  {
                    "type": "complementary",
                    "id": "ethical_considerations_data",
                    "description": "Đảm bảo việc diễn giải dữ liệu được thực hiện một cách có đạo đức, tránh sai lệch hoặc gây hiểu lầm."
                  },
                  {
                    "type": "unlocks",
                    "id": "advanced_data_storytelling_techniques",
                    "description": "Là nền tảng để phát triển các kỹ thuật kể chuyện dữ liệu phức tạp hơn, bao gồm kịch bản và diễn giải đa chiều."
                  },
                  {
                    "type": "unlocks",
                    "id": "executive_summary_reporting",
                    "description": "Cho phép tạo ra các báo cáo tóm tắt hiệu quả cho cấp quản lý, nhấn mạnh ý nghĩa và khuyến nghị."
                  },
                  {
                    "type": "unlocks",
                    "id": "data_driven_decision_making",
                    "description": "Kỹ năng quan trọng để áp dụng trực tiếp dữ liệu đã được bối cảnh hóa và diễn giải vào các quyết định kinh doanh chiến lược."
                  }
                ]
              }
            ],
            "skillName": "Kể chuyện bằng dữ liệu (Data Storytelling)",
            "competency": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
            "description": "Kỹ năng Kể chuyện bằng dữ liệu giúp bạn biến những con số và biểu đồ khô khan thành những câu chuyện hấp dẫn, dễ hiểu. Bạn sẽ học cách xây dựng một mạch truyện logic từ dữ liệu, truyền tải các insight quan trọng một cách thuyết phục để thu hút đối tượng và thúc đẩy hành động.",
            "knowledgeUnits": [
              "Xác định Đối tượng và Mục tiêu Câu chuyện Dữ liệu",
              "Khám phá và Phát hiện Insight Quan trọng từ Dữ liệu",
              "Cấu trúc và Dòng chảy của Câu chuyện Dữ liệu",
              "Nguyên tắc Thiết kế và Lựa chọn Trực quan hóa Hiệu quả",
              "Bối cảnh hóa và Diễn giải Dữ liệu để tạo Ý nghĩa"
            ],
            "learningResources": [
              "Sách: Storytelling with Data: A Data Visualization Guide for Business Professionals của Cole Nussbaumer Knaflic",
              "Playlist video: Các khóa học hoặc chuỗi bài giảng về Kể chuyện bằng Dữ liệu trên Coursera, edX, hoặc YouTube từ các chuyên gia/nền tảng như Tableau, DataCamp."
            ],
            "projectIdeas": [
              "Phân tích một bộ dữ liệu công khai (ví dụ: dữ liệu doanh số, xu hướng tiêu dùng, tác động xã hội) để xác định các insight then chốt, sau đó xây dựng một câu chuyện dữ liệu hoàn chỉnh dưới dạng một bài thuyết trình (presentation) hoặc báo cáo tương tác (interactive report) nhằm truyền đạt kết quả đến một đối tượng cụ thể (ví dụ: ban lãnh đạo, nhóm marketing) để hỗ trợ việc ra quyết định."
            ],
            "tools": [
              "Tableau",
              "Microsoft Power BI",
              "Google Looker Studio (trước đây là Google Data Studio)",
              "Microsoft PowerPoint",
              "Google Slides",
              "Keynote"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "25-35 giờ học và thực hành",
            "importanceScore": 5
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5_2",
            "name": "Thiết kế và phát triển Dashboard/Báo cáo tương tác",
            "type": "skill",
            "children": [
              {
                "id": "ds_bigdata_dashboard_dataviz_principles",
                "name": "Nguyên tắc Thiết kế Trực quan hóa Dữ liệu (Principles of Data Visualization Design)",
                "type": "knowledge",
                "children": [],
                "title": "Nguyên tắc Thiết kế Trực quan hóa Dữ liệu (Principles of Data Visualization Design)",
                "description": "Kiến thức này trang bị các nguyên tắc cốt lõi để thiết kế biểu đồ và trực quan hóa dữ liệu hiệu quả, giúp truyền tải thông điệp rõ ràng, chính xác và có sức thuyết phục. Việc nắm vững các nguyên tắc này là nền tảng để tạo ra các dashboard và báo cáo không chỉ đẹp mắt mà còn dễ hiểu, giảm thiểu sai lệch thông tin và tối ưu hóa trải nghiệm người dùng, từ đó hỗ trợ đưa ra quyết định dựa trên dữ liệu hiệu quả.",
                "keywords": [
                  "Data-ink ratio",
                  "Gestalt principles",
                  "Pre-attentive attributes",
                  "Visual encoding",
                  "Cognitive load",
                  "Data storytelling",
                  "Chart selection",
                  "Information hierarchy",
                  "Data ethics",
                  "Tufte principles"
                ],
                "learningResources": [
                  {
                    "name": "The Visual Display of Quantitative Information by Edward Tufte (Website)",
                    "url": "https://www.edwardtufte.com/tufte/"
                  },
                  {
                    "name": "Data Visualization Best Practices (Tableau)",
                    "url": "https://www.tableau.com/learn/articles/data-visualization-best-practices"
                  },
                  {
                    "name": "Perceptual Edge: Effective Data Visualization (Stephen Few)",
                    "url": "https://www.perceptualedge.com/articles.php"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích và thiết kế lại một biểu đồ/dashboard kém hiệu quả: Học viên sẽ chọn một biểu đồ hoặc dashboard hiện có (có thể từ ví dụ trên mạng hoặc dữ liệu giả định), phân tích các điểm yếu về thiết kế và áp dụng các nguyên tắc đã học để thiết kế lại, giải thích lý do cho từng thay đổi.",
                  "Xây dựng một báo cáo tương tác đơn giản: Sử dụng một bộ dữ liệu nhỏ, thiết kế và phát triển một báo cáo hoặc dashboard tương tác bằng công cụ lựa chọn (ví dụ: Tableau Public, Power BI Desktop), đảm bảo áp dụng ít nhất 3-5 nguyên tắc thiết kế trực quan hóa dữ liệu đã học."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "BI Developer",
                  "Data Visualization Specialist",
                  "Reporting Analyst",
                  "Product Manager (Data Focus)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Looker Studio",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "D3.js",
                  "Figma (for mockups)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "2-3 weeks",
                "importanceScore": 10,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "BI Developer",
                    "score": 10
                  },
                  {
                    "path": "Data Engineer",
                    "score": 6
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Product Manager",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "complementary",
                    "id": "ds_bigdata_dashboard_datastorytelling"
                  },
                  {
                    "type": "complementary",
                    "id": "ds_bigdata_dashboard_uxprinciples"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bigdata_dashboard_advanceddesign"
                  },
                  {
                    "type": "unlocks",
                    "id": "ds_bigdata_dashboard_interactivedev"
                  }
                ]
              },
              {
                "id": "ui_ux_design_principles_for_dashboards",
                "name": "Nguyên tắc Thiết kế Giao diện Người dùng (UI) và Trải nghiệm Người dùng (UX) cho Dashboard",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị các nguyên tắc và thực tiễn tốt nhất để thiết kế dashboard trực quan, dễ sử dụng và hiệu quả trong việc truyền tải thông tin dữ liệu. Nó bao gồm việc hiểu người dùng, cấu trúc thông tin hợp lý, lựa chọn biểu đồ phù hợp, và tạo ra trải nghiệm tương tác mượt mà, giúp người dùng ra quyết định nhanh chóng và chính xác.",
                "keywords": [
                  "Usability",
                  "Data Visualization",
                  "Information Architecture",
                  "Interaction Design",
                  "User-Centric Design",
                  "Dashboard Design",
                  "UX Best Practices",
                  "UI Guidelines",
                  "Cognitive Load",
                  "Accessibility"
                ],
                "learningResources": [
                  {
                    "name": "Dashboards: How to Design and Evaluate Them",
                    "url": "https://www.nngroup.com/articles/dashboards-design-evaluation/"
                  },
                  {
                    "name": "Tableau Dashboard Design Best Practices",
                    "url": "https://www.tableau.com/learn/articles/dashboard-design-best-practices"
                  },
                  {
                    "name": "The Ultimate Guide to Dashboard Design: Best Practices for 2024",
                    "url": "https://www.datapine.com/blog/dashboard-design-best-practices/"
                  }
                ],
                "prerequisites": [
                  "basic_data_visualization_principles",
                  "business_context_kpis_understanding"
                ],
                "projectIdeas": [
                  "Phân tích một dashboard hiện có (ví dụ: dashboard công khai từ chính phủ, tổ chức) và viết báo cáo đánh giá về điểm mạnh, điểm yếu dựa trên các nguyên tắc UI/UX, sau đó đề xuất cải tiến cụ thể.",
                  "Thiết kế wireframe và mockup cho một dashboard quản lý hiệu suất bán hàng (hoặc dự án tương tự) cho một công ty bán lẻ, tập trung vào việc áp dụng các nguyên tắc về phân cấp thông tin, tương tác và tính rõ ràng."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Developer",
                  "UX Designer",
                  "Product Manager (Data Products)",
                  "Data Visualization Specialist"
                ],
                "marketDemand": "High",
                "tools": [
                  "Figma",
                  "Sketch",
                  "Adobe XD",
                  "Tableau",
                  "Microsoft Power BI",
                  "Looker",
                  "Google Data Studio"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-25 hours (study & practical application)",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 8
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 10
                  },
                  {
                    "path": "UX Designer (Data Focus)",
                    "score": 8
                  },
                  {
                    "path": "Product Manager (Data Products)",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "basic_data_visualization_principles"
                  },
                  {
                    "type": "prerequisite",
                    "id": "business_context_kpis_understanding"
                  },
                  {
                    "type": "complementary",
                    "id": "data_storytelling_techniques"
                  },
                  {
                    "type": "unlocks",
                    "id": "advanced_interactive_dashboard_design"
                  }
                ]
              },
              {
                "id": "knowledge_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5_2_3",
                "name": "Kể chuyện bằng Dữ liệu và Truyền đạt Thông điệp (Data Storytelling and Message Communication)",
                "type": "knowledge",
                "children": [],
                "description": "Kể chuyện bằng dữ liệu là nghệ thuật xây dựng một câu chuyện thuyết phục và dễ hiểu xung quanh các insight được rút ra từ dữ liệu, giúp truyền đạt thông điệp hiệu quả đến đối tượng mục tiêu. Nó chuyển đổi các số liệu và biểu đồ thô thành một tường thuật có ý nghĩa, giải thích 'tại sao' và 'điều gì tiếp theo', từ đó thúc đẩy hành động và đưa ra quyết định kinh doanh sáng suốt.",
                "keywords": [
                  "Data Storytelling",
                  "Message Communication",
                  "Data Visualization",
                  "Narrative Design",
                  "Insight Communication",
                  "Audience Analysis",
                  "Presentation Skills",
                  "Data Interpretation",
                  "Strategic Communication",
                  "Business Intelligence Reporting"
                ],
                "learningResources": [
                  {
                    "title": "Data Storytelling: The Essential Guide",
                    "url": "https://www.tableau.com/learn/articles/data-storytelling"
                  },
                  {
                    "title": "How to Tell a Story with Data",
                    "url": "https://hbr.org/2020/03/how-to-tell-a-story-with-data"
                  },
                  {
                    "title": "Data Storytelling with Tableau (Coursera Course)",
                    "url": "https://www.coursera.org/learn/data-storytelling"
                  }
                ],
                "prerequisites": [
                  "data_visualization_fundamentals",
                  "dashboard_design_principles",
                  "basic_statistical_concepts"
                ],
                "projectIdeas": [
                  "Chọn một tập dữ liệu công khai (ví dụ: Kaggle) và xây dựng một câu chuyện dữ liệu hoàn chỉnh, bao gồm phân tích, trực quan hóa và một bài trình bày súc tích về các phát hiện chính và hàm ý kinh doanh.",
                  "Phân tích một báo cáo/dashboard hiện có và đề xuất các cải tiến về cách kể chuyện, biến nó từ một tập hợp các biểu đồ thành một tường thuật mạch lạc, có tính hành động cho một đối tượng cụ thể."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Analytics Consultant",
                  "Product Manager",
                  "Reporting Specialist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Google Looker Studio",
                  "Python (Matplotlib, Seaborn, Plotly, Dash)",
                  "R (ggplot2, Shiny)",
                  "Microsoft PowerPoint",
                  "Google Slides",
                  "Keynote"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Analyst",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 6
                  },
                  {
                    "path": "Data Engineer",
                    "score": 4
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data_visualization_fundamentals",
                    "type": "prerequisite"
                  },
                  {
                    "id": "dashboard_design_principles",
                    "type": "prerequisite"
                  },
                  {
                    "id": "stakeholder_communication_and_management",
                    "type": "complementary"
                  },
                  {
                    "id": "critical_thinking_and_problem_solving",
                    "type": "complementary"
                  },
                  {
                    "id": "advanced_dashboard_design_and_interactivity",
                    "type": "unlocks"
                  },
                  {
                    "id": "effective_business_intelligence_reporting",
                    "type": "unlocks"
                  }
                ]
              },
              {
                "id": "data-concepts-types-structures",
                "name": "Khái niệm về Dữ liệu, Các loại Dữ liệu và Cấu trúc Dữ liệu (Data Concepts, Data Types, and Data Structures)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này giới thiệu các khái niệm cơ bản về dữ liệu, bao gồm định nghĩa dữ liệu, phân loại các loại dữ liệu (ví dụ: số, chuỗi, boolean) và các cấu trúc dữ liệu cơ bản (như danh sách, mảng, từ điển, tập hợp). Nắm vững những khái niệm này là nền tảng thiết yếu để tổ chức, lưu trữ và thao tác dữ liệu hiệu quả, đóng vai trò sống còn trong phân tích dữ liệu và thiết kế báo cáo.",
                "keywords": [
                  "dữ liệu",
                  "khái niệm dữ liệu",
                  "loại dữ liệu",
                  "cấu trúc dữ liệu",
                  "kiểu dữ liệu nguyên thủy",
                  "kiểu dữ liệu phức hợp",
                  "mảng",
                  "danh sách",
                  "từ điển",
                  "tập hợp",
                  "chuỗi",
                  "số nguyên",
                  "số thực",
                  "boolean"
                ],
                "learningResources": [
                  {
                    "name": "Python Data Types and Structures - Real Python",
                    "url": "https://realpython.com/python-data-types/"
                  },
                  {
                    "name": "What are Data Structures? - GeeksforGeeks",
                    "url": "https://www.geeksforgeeks.org/data-structures/"
                  },
                  {
                    "name": "Data Types in Data Science - IBM",
                    "url": "https://www.ibm.com/topics/data-types-in-data-science"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  {
                    "title": "Phân loại và Lưu trữ Dữ liệu Khách hàng",
                    "description": "Tạo một chương trình nhỏ nhận vào các thông tin khách hàng (tên, tuổi, email, thu nhập, sở thích) và gán chúng vào các kiểu dữ liệu phù hợp (ví dụ: chuỗi cho tên, số nguyên cho tuổi, danh sách chuỗi cho sở thích). Sau đó, lưu trữ thông tin của nhiều khách hàng trong một cấu trúc dữ liệu phức tạp hơn như danh sách các từ điển."
                  },
                  {
                    "title": "Hệ thống Quản lý Kho hàng Đơn giản",
                    "description": "Xây dựng một hệ thống để lưu trữ các mặt hàng trong kho. Mỗi mặt hàng có thể được biểu diễn bằng một từ điển chứa tên, mã sản phẩm, số lượng, và giá. Sử dụng danh sách để quản lý tập hợp các mặt hàng trong kho, bao gồm các chức năng thêm, xóa, và cập nhật thông tin mặt hàng."
                  }
                ],
                "relatedJobTitles": [
                  "Nhà khoa học dữ liệu (Data Scientist)",
                  "Kỹ sư dữ liệu (Data Engineer)",
                  "Nhà phân tích dữ liệu (Data Analyst)",
                  "Chuyên viên BI (BI Developer)",
                  "Kỹ sư học máy (Machine Learning Engineer)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Python",
                  "R",
                  "SQL",
                  "Pandas",
                  "NumPy"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "15-25 giờ",
                "importanceScore": 10,
                "requiredProficiency": "Conceptual",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Kỹ sư học máy (Machine Learning Engineer)",
                    "score": 9
                  },
                  {
                    "path": "Data Engineer",
                    "score": 10
                  }
                ],
                "skillRelations": [
                  {
                    "type": "unlocks",
                    "skillId": "data-manipulation-and-cleaning",
                    "description": "Nền tảng vững chắc cho việc học các kỹ thuật thao tác và làm sạch dữ liệu hiệu quả."
                  },
                  {
                    "type": "unlocks",
                    "skillId": "database-fundamentals",
                    "description": "Hiểu rõ các loại và cấu trúc dữ liệu giúp dễ dàng tiếp thu kiến thức về mô hình hóa và truy vấn cơ sở dữ liệu."
                  },
                  {
                    "type": "unlocks",
                    "skillId": "basic-statistical-analysis",
                    "description": "Khả năng phân loại và hiểu dữ liệu là cần thiết để áp dụng các phương pháp thống kê chính xác."
                  }
                ]
              },
              {
                "id": "KU-DS-BI-DashboardInteractivity",
                "name": "Thiết kế Tính tương tác và Điều khiển trên Dashboard",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào việc thiết kế và triển khai các yếu tố tương tác như bộ lọc, bộ chọn, hành động (actions), và các điều khiển khác trên dashboard. Mục tiêu là cho phép người dùng khám phá dữ liệu một cách linh hoạt, cá nhân hóa trải nghiệm và tự trả lời các câu hỏi cụ thể, từ đó nâng cao khả năng ra quyết định dựa trên dữ liệu hiệu quả hơn.",
                "keywords": [
                  "Dashboard tương tác",
                  "Bộ lọc dữ liệu",
                  "Điều khiển dashboard",
                  "Hành động dashboard",
                  "Drill-down",
                  "Drill-through",
                  "Trải nghiệm người dùng (UX)",
                  "Tương tác dữ liệu",
                  "Tham số dashboard",
                  "Khám phá dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "Tableau: Create Dashboard Actions",
                    "url": "https://help.tableau.com/current/pro/desktop/en-us/actions.htm"
                  },
                  {
                    "name": "Power BI: Interact with reports in Power BI",
                    "url": "https://learn.microsoft.com/en-us/power-bi/consumer/end-user-interactions"
                  },
                  {
                    "name": "10 Tips for Designing Interactive Dashboards",
                    "url": "https://towardsdatascience.com/10-tips-for-designing-interactive-dashboards-3e6f9a7d3c5f"
                  }
                ],
                "prerequisites": [
                  "KU-DS-BI-VisualizationBasicPrinciples",
                  "KU-DS-BI-DashboardDesignPrinciples",
                  "KU-DS-BI-BI_Tool_Fundamentals"
                ],
                "projectIdeas": [
                  "Xây dựng dashboard phân tích doanh số bán hàng tương tác cho một công ty bán lẻ, cho phép người dùng lọc theo khu vực, danh mục sản phẩm và khoảng thời gian. Dashboard cần có chức năng drill-down để xem chi tiết doanh số theo từng cửa hàng hoặc sản phẩm.",
                  "Thiết kế dashboard theo dõi hiệu suất chiến dịch marketing, tích hợp các bộ lọc và điều khiển để so sánh các chiến dịch, xem xét hiệu quả theo kênh và thời gian. Dashboard cần có khả năng hiển thị thay đổi theo thời gian thực (nếu dữ liệu cho phép) và các nút để chuyển đổi giữa các KPI."
                ],
                "relatedJobTitles": [
                  "Chuyên viên Phân tích dữ liệu (Data Analyst)",
                  "Nhà phát triển Business Intelligence (BI Developer)",
                  "Thiết kế Dashboard (Dashboard Designer)",
                  "Chuyên gia Trực quan hóa dữ liệu (Data Visualization Specialist)",
                  "Tư vấn BI (BI Consultant)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Looker Studio (Google Data Studio)",
                  "Qlik Sense",
                  "Python (thư viện Plotly Dash, Streamlit)",
                  "R (thư viện Shiny)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-25 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Chuyên viên Phân tích dữ liệu",
                    "score": 10
                  },
                  {
                    "path": "Nhà phát triển Business Intelligence",
                    "score": 10
                  },
                  {
                    "path": "Kỹ sư Dữ liệu",
                    "score": 7
                  },
                  {
                    "path": "Nhà khoa học Dữ liệu",
                    "score": 8
                  },
                  {
                    "path": "Chuyên gia UX cho sản phẩm dữ liệu",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "KU-DS-BI-VisualizationBasicPrinciples",
                    "type": "prerequisite",
                    "description": "Hiểu biết về các loại biểu đồ và nguyên tắc trực quan hóa dữ liệu cơ bản là cần thiết để thiết kế các phần tử tương tác hiệu quả và trực quan."
                  },
                  {
                    "id": "KU-DS-BI-DashboardDesignPrinciples",
                    "type": "prerequisite",
                    "description": "Nắm vững các nguyên tắc thiết kế dashboard chung giúp đặt các điều khiển tương tác một cách hợp lý, tối ưu hóa bố cục và luồng người dùng."
                  },
                  {
                    "id": "KU-DS-BI-BI_Tool_Fundamentals",
                    "type": "prerequisite",
                    "description": "Kiến thức cơ bản về ít nhất một công cụ BI (như Tableau, Power BI) là nền tảng để triển khai và cấu hình các tính năng tương tác."
                  },
                  {
                    "id": "KU-DS-BI-DataStorytelling",
                    "type": "complementary",
                    "description": "Thiết kế tương tác mạnh mẽ giúp tăng cường khả năng kể chuyện bằng dữ liệu, cho phép người dùng tự khám phá và hiểu sâu hơn về thông điệp từ dữ liệu."
                  },
                  {
                    "id": "KU-DS-BI-AdvancedDashboardOptimization",
                    "type": "unlocks",
                    "description": "Kỹ năng này là tiền đề để tối ưu hóa hiệu suất và trải nghiệm người dùng của các dashboard phức tạp, tận dụng tối đa các tính năng tương tác nâng cao và xử lý lượng lớn dữ liệu."
                  }
                ]
              }
            ],
            "skillName": "Thiết kế và phát triển Dashboard/Báo cáo tương tác",
            "competency": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
            "description": "Kỹ năng này trang bị cho bạn khả năng thiết kế và xây dựng các dashboard và báo cáo tương tác, biến dữ liệu thô thành những thông tin chi tiết dễ hiểu và có giá trị. Bạn sẽ học cách áp dụng các nguyên tắc trực quan hóa dữ liệu, UI/UX và kể chuyện bằng dữ liệu để truyền tải thông điệp một cách hiệu quả và thúc đẩy việc ra quyết định dựa trên dữ liệu.",
            "learningResources": [
              {
                "name": "The Big Book of Dashboards: Visualizing Your Data Using Real-World Data",
                "type": "Book",
                "url": "https://www.amazon.com/Big-Book-Dashboards-Visualizing-Real-World/dp/1119282725"
              },
              {
                "name": "Mastering Tableau/Power BI for Data Visualization - Comprehensive Course",
                "type": "Video Playlist/Online Course",
                "url": "https://www.udemy.com/course/tableau-a-z/"
              }
            ],
            "projectIdeas": [
              {
                "name": "Dashboard phân tích hiệu suất chiến dịch Marketing",
                "description": "Thiết kế và phát triển một dashboard tương tác cho phép phân tích hiệu suất của các chiến dịch marketing. Dashboard cần hiển thị các chỉ số như chi phí quảng cáo, lượt tiếp cận, lượt click, tỷ lệ chuyển đổi, và doanh thu. Người dùng có thể lọc dữ liệu theo kênh marketing, thời gian, và loại chiến dịch. Đảm bảo dashboard có giao diện thân thiện, dễ hiểu và kể được câu chuyện về hiệu quả đầu tư marketing (ROI)."
              }
            ],
            "tools": [
              "Tableau",
              "Power BI",
              "Looker Studio (Google Data Studio)",
              "Microsoft Excel",
              "Python (Matplotlib, Seaborn, Plotly, Dash)",
              "R (ggplot2, Shiny)"
            ],
            "difficultyLevel": "Intermediate",
            "estimatedTimeToComplete": "40-60 hours",
            "importanceScore": 8
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5_3",
            "name": "Thành thạo các công cụ trực quan hóa dữ liệu (ví dụ: Tableau, Power BI, Python/R libraries)",
            "type": "skill",
            "children": [
              {
                "id": "skill-unit-data-viz-principles",
                "name": "Nguyên tắc thiết kế trực quan hóa dữ liệu hiệu quả",
                "type": "knowledge",
                "children": [],
                "description": "Đơn vị kiến thức này tập trung vào các nguyên tắc nền tảng để tạo ra các biểu đồ và dashboard trực quan, rõ ràng, chính xác và có tác động. Nó bao gồm việc lựa chọn loại biểu đồ phù hợp, sử dụng màu sắc hiệu quả, và áp dụng các kỹ thuật để tránh gây hiểu lầm hoặc sai lệch dữ liệu, đảm bảo thông điệp được truyền tải một cách hiệu quả và đáng tin cậy.",
                "keywords": [
                  "lựa chọn biểu đồ",
                  "lý thuyết màu sắc",
                  "tỷ lệ mực dữ liệu",
                  "thuộc tính tiền nhận thức",
                  "sai lệch trực quan",
                  "kể chuyện dữ liệu",
                  "thiết kế dashboard",
                  "nguyên tắc Tufte",
                  "mã hóa thị giác",
                  "hiểu biết dữ liệu"
                ],
                "learningResources": [
                  {
                    "name": "Storytelling with Data: A Data Visualization Guide for Business Professionals",
                    "url": "https://www.storytellingwithdata.com/"
                  },
                  {
                    "name": "Fundamentals of Data Visualization by Claus O. Wilke",
                    "url": "https://clauswilke.com/dataviz/"
                  },
                  {
                    "name": "Tableau's Best Practices for Data Visualization",
                    "url": "https://www.tableau.com/learn/articles/best-practices-data-visualization"
                  }
                ],
                "prerequisites": [
                  "skill-unit-data-fundamentals",
                  "skill-unit-basic-statistical-concepts"
                ],
                "projectIdeas": [
                  "Chọn một biểu đồ kém hiệu quả từ một báo cáo hoặc tin tức và thiết kế lại nó, áp dụng ít nhất ba nguyên tắc thiết kế trực quan hóa dữ liệu hiệu quả để cải thiện sự rõ ràng và tác động. Giải thích các thay đổi bạn đã thực hiện.",
                  "Sử dụng một bộ dữ liệu nhỏ (ví dụ: dữ liệu bán hàng hoặc khảo sát) để tạo một chuỗi 3-4 biểu đồ nhằm kể một câu chuyện cụ thể. Tập trung vào việc lựa chọn biểu đồ, sử dụng màu sắc, và thêm chú thích để truyền tải thông điệp chính xác và không gây sai lệch."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Scientist",
                  "Data Visualization Specialist",
                  "UX/UI Designer (Data-Focused)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "D3.js"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-30 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Data Engineer",
                    "score": 6
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "skill-unit-understanding-data-types-and-structures"
                  },
                  {
                    "type": "complementary",
                    "id": "skill-unit-mastering-tableau-fundamentals"
                  },
                  {
                    "type": "complementary",
                    "id": "skill-unit-python-data-visualization-libraries"
                  },
                  {
                    "type": "unlocks",
                    "id": "skill-unit-designing-interactive-dashboards"
                  },
                  {
                    "type": "unlocks",
                    "id": "skill-unit-advanced-data-storytelling"
                  }
                ]
              },
              {
                "id": "DS_DV_CommonChartTypes",
                "name": "Các loại biểu đồ phổ biến (ví dụ: biểu đồ cột, đường, tròn, phân tán) và ứng dụng phù hợp của chúng",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị khả năng nhận diện và hiểu các loại biểu đồ cơ bản như biểu đồ cột, đường, tròn, phân tán, và biểu đồ hộp. Việc nắm vững cách chọn biểu đồ phù hợp với loại dữ liệu và mục tiêu phân tích là tối quan trọng để trực quan hóa dữ liệu một cách hiệu quả, giúp truyền tải thông điệp rõ ràng và hỗ trợ ra quyết định dựa trên dữ liệu.",
                "keywords": [
                  "biểu đồ cột",
                  "biểu đồ đường",
                  "biểu đồ tròn",
                  "biểu đồ phân tán",
                  "trực quan hóa dữ liệu",
                  "phân tích dữ liệu",
                  "lựa chọn biểu đồ",
                  "ứng dụng biểu đồ",
                  "biểu đồ hộp",
                  "thống kê mô tả"
                ],
                "learningResources": [
                  {
                    "name": "Chart Types for Data Visualization",
                    "url": "https://help.tableau.com/current/pro/desktop/en-us/build_charts.htm"
                  },
                  {
                    "name": "From Data to Viz: Find the graphic you need",
                    "url": "https://www.data-to-viz.com/"
                  },
                  {
                    "name": "Data Visualization: A Guide to Best Practices",
                    "url": "https://www.sas.com/en_us/insights/analytics/data-visualization.html"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích dữ liệu bán hàng: Sử dụng một tập dữ liệu bán hàng nhỏ, tạo các biểu đồ cột để so sánh doanh thu giữa các khu vực, biểu đồ đường để theo dõi xu hướng bán hàng theo thời gian, và biểu đồ tròn để hiển thị tỷ lệ đóng góp của từng sản phẩm hoặc khách hàng.",
                  "Trực quan hóa khảo sát: Thu thập hoặc sử dụng dữ liệu từ một khảo sát nhỏ. Áp dụng biểu đồ cột để thể hiện tần suất câu trả lời, biểu đồ tròn cho các tỷ lệ phần trăm, biểu đồ phân tán để khám phá mối quan hệ giữa hai biến định lượng, và biểu đồ hộp để so sánh phân phối giữa các nhóm."
                ],
                "relatedJobTitles": [
                  "Nhà phân tích dữ liệu (Data Analyst)",
                  "Chuyên viên Khoa học dữ liệu (Data Scientist)",
                  "Chuyên viên Phân tích kinh doanh (Business Intelligence Analyst)",
                  "Chuyên viên báo cáo (Reporting Specialist)",
                  "Nghiên cứu viên (Researcher)"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "Microsoft Excel",
                  "Google Sheets"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "4-8 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Khoa học dữ liệu",
                    "score": 9
                  },
                  {
                    "path": "Phân tích dữ liệu",
                    "score": 10
                  },
                  {
                    "path": "Kỹ sư dữ liệu",
                    "score": 7
                  },
                  {
                    "path": "Phân tích kinh doanh",
                    "score": 9
                  },
                  {
                    "path": "Nghiên cứu",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "complementary",
                    "targetSkillId": "DS_DA_StatisticalAnalysis"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS_DV_DataStorytelling"
                  },
                  {
                    "type": "unlocks",
                    "targetSkillId": "DS_DV_DashboardDesign"
                  }
                ]
              },
              {
                "id": "data_types_and_structures_fundamentals",
                "name": "Hiểu biết về các loại dữ liệu (định lượng, định tính, thời gian) và cấu trúc dữ liệu cơ bản (ví dụ: bảng, cột, hàng)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị nền tảng về các loại dữ liệu khác nhau (định lượng, định tính, thời gian) và cách chúng được tổ chức trong các cấu trúc cơ bản như bảng, cột, và hàng. Việc hiểu rõ bản chất và cách tổ chức dữ liệu là cực kỳ quan trọng để có thể phân tích, xử lý và trực quan hóa dữ liệu một cách chính xác và hiệu quả.",
                "keywords": [
                  "loại dữ liệu",
                  "dữ liệu định lượng",
                  "dữ liệu định tính",
                  "dữ liệu thời gian",
                  "cấu trúc dữ liệu",
                  "bảng dữ liệu",
                  "hàng",
                  "cột",
                  "biến",
                  "thang đo dữ liệu"
                ],
                "learningResources": [
                  {
                    "title": "What are Data Types in Data Science?",
                    "url": "https://www.coursera.org/articles/data-types-in-data-science"
                  },
                  {
                    "title": "Introduction to Tabular Data and DataFrames in Pandas",
                    "url": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html"
                  },
                  {
                    "title": "Data Structures: A Beginner's Guide",
                    "url": "https://www.ibm.com/topics/data-structures"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Chọn một tập dữ liệu CSV nhỏ (ví dụ: về dân số, doanh số bán hàng), xác định từng cột là loại dữ liệu nào (định lượng, định tính, thời gian) và giải thích tại sao. Mô tả số hàng và cột của tập dữ liệu.",
                  "Tạo một bảng dữ liệu đơn giản trên Excel hoặc bằng thư viện Pandas (Python) với ít nhất 5 cột thuộc các loại dữ liệu khác nhau và 10 hàng. Viết mô tả về cấu trúc và ý nghĩa của từng cột."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Junior Data Scientist",
                  "Data Engineer",
                  "Market Research Analyst"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Microsoft Excel",
                  "Google Sheets",
                  "Pandas (Python)",
                  "R Data Frames",
                  "SQL (Databases)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 10
                  },
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Data Engineer",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "type": "unlocks",
                    "targetId": "data_visualization_mastery",
                    "description": "Hiểu biết sâu sắc về các loại dữ liệu và cấu trúc dữ liệu là nền tảng thiết yếu để lựa chọn biểu đồ, công cụ trực quan hóa phù hợp và diễn giải dữ liệu một cách chính xác."
                  },
                  {
                    "type": "complementary",
                    "targetId": "data_cleaning_and_preprocessing_techniques",
                    "description": "Kiến thức về loại và cấu trúc dữ liệu giúp ích rất nhiều trong quá trình phát hiện lỗi, làm sạch và chuẩn bị dữ liệu cho phân tích."
                  }
                ]
              },
              {
                "id": "data-viz-workflow-basics",
                "name": "Quy trình làm việc cơ bản trong trực quan hóa dữ liệu (từ chuẩn bị dữ liệu đến xây dựng và trình bày trực quan)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này bao gồm các bước tuần tự và có hệ thống để chuyển đổi dữ liệu thô thành các biểu đồ và báo cáo trực quan có ý nghĩa. Việc nắm vững quy trình này đảm bảo rằng các trực quan hóa không chỉ chính xác mà còn hiệu quả trong việc truyền tải thông điệp, hỗ trợ đưa ra quyết định dựa trên dữ liệu.",
                "keywords": [
                  "chuẩn bị dữ liệu",
                  "làm sạch dữ liệu",
                  "chuyển đổi dữ liệu",
                  "phân tích dữ liệu thăm dò (EDA)",
                  "thiết kế trực quan",
                  "kể chuyện bằng dữ liệu",
                  "xây dựng bảng điều khiển",
                  "trình bày dữ liệu",
                  "trực quan hóa tương tác",
                  "data pipeline"
                ],
                "learningResources": [
                  {
                    "name": "Getting Started with Tableau",
                    "url": "https://www.tableau.com/learn/tutorials/on-demand/getting-started"
                  },
                  {
                    "name": "Get started with Power BI",
                    "url": "https://learn.microsoft.com/en-us/training/modules/get-started-power-bi/"
                  },
                  {
                    "name": "Python Data Visualization Basics with Matplotlib and Seaborn",
                    "url": "https://realpython.com/python-data-visualization-basics/"
                  }
                ],
                "prerequisites": [
                  "data-cleaning-transformation-basics",
                  "basic-statistical-concepts-for-data",
                  "intro-to-data-types-and-structures"
                ],
                "projectIdeas": [
                  "Phân tích bộ dữ liệu công khai (ví dụ: từ Kaggle) về xu hướng bán hàng của một chuỗi bán lẻ, từ làm sạch dữ liệu đến xây dựng bảng điều khiển tương tác mô tả doanh thu theo khu vực, sản phẩm và thời gian.",
                  "Trực quan hóa dữ liệu khảo sát khách hàng để xác định các yếu tố ảnh hưởng đến sự hài lòng, bao gồm chuẩn bị dữ liệu, chọn biểu đồ phù hợp và trình bày các phát hiện chính trong một báo cáo trực quan."
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "Data Scientist",
                  "BI Developer",
                  "Data Visualization Specialist"
                ],
                "marketDemand": "Extremely High",
                "tools": [
                  "Tableau",
                  "Microsoft Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "SQL",
                  "Excel"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "20-40 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 6
                  },
                  {
                    "path": "Product Manager (Data-Driven)",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "id": "data-cleaning-transformation-basics",
                    "type": "prerequisite"
                  },
                  {
                    "id": "basic-statistical-concepts-for-data",
                    "type": "prerequisite"
                  },
                  {
                    "id": "intro-to-data-types-and-structures",
                    "type": "prerequisite"
                  },
                  {
                    "id": "effective-data-storytelling",
                    "type": "complementary"
                  },
                  {
                    "id": "advanced-visualization-techniques",
                    "type": "complementary"
                  },
                  {
                    "id": "advanced-dashboard-design",
                    "type": "unlocks"
                  },
                  {
                    "id": "interactive-data-storytelling",
                    "type": "unlocks"
                  }
                ]
              }
            ],
            "skillName": "Thành thạo các công cụ trực quan hóa dữ liệu (ví dụ: Tableau, Power BI, Python/R libraries)",
            "competency": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
            "description": "Kỹ năng này trang bị cho người học khả năng biến đổi dữ liệu thô thành các biểu đồ và dashboard trực quan, có ý nghĩa, sử dụng các công cụ chuyên dụng. Người học sẽ biết cách áp dụng các nguyên tắc thiết kế hiệu quả và lựa chọn loại biểu đồ phù hợp để truyền đạt thông điệp một cách rõ ràng và thuyết phục.",
            "learningResources": [
              {
                "title": "Chương 1-3: Giới thiệu về Trực quan hóa dữ liệu & Nguyên tắc thiết kế",
                "type": "Chương sách",
                "source": "Sách 'The Big Book of Dashboards: Visualizing Your Data Using Real-World Business Cases' của Steve Wexler, Jeffrey Shaffer, và Andy Cotgreave"
              },
              {
                "title": "Playlist: Hướng dẫn cơ bản về Tableau/Power BI cho người mới bắt đầu",
                "type": "Video playlist",
                "source": "Kênh YouTube 'freeCodeCamp.org' hoặc 'DataCamp' (tìm kiếm 'Tableau Tutorial' hoặc 'Power BI Tutorial')"
              }
            ],
            "projectIdeas": [
              {
                "title": "Xây dựng Dashboard Phân tích Doanh số bán hàng",
                "description": "Sử dụng một bộ dữ liệu doanh số bán hàng (ví dụ: Superstore Dataset), xây dựng một dashboard tương tác bằng Tableau hoặc Power BI. Dashboard cần hiển thị các chỉ số như tổng doanh thu, lợi nhuận, doanh số theo sản phẩm/danh mục, theo khu vực và theo thời gian. Dự án yêu cầu áp dụng các nguyên tắc thiết kế hiệu quả, chọn loại biểu đồ phù hợp cho từng loại dữ liệu và trích xuất ít nhất 3 insight kinh doanh từ dữ liệu trực quan hóa."
              }
            ],
            "tools": [
              "Tableau Desktop",
              "Microsoft Power BI Desktop",
              "Python (thư viện Matplotlib, Seaborn, Plotly)",
              "R (thư viện ggplot2, Plotly)"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "40-60 giờ (bao gồm lý thuyết và thực hành với 1-2 công cụ)",
            "importanceScore": 8
          },
          {
            "id": "skill_khoa_học_dữ_liệu_(data_science)_và_dữ_liệu_lớn_(big_data)_5_4",
            "name": "Áp dụng nguyên tắc thiết kế trực quan và tâm lý học nhận thức",
            "type": "skill",
            "children": [
              {
                "id": "visual_design_principles_basic",
                "name": "Nguyên tắc thiết kế trực quan cơ bản (như căn chỉnh, tương phản, lặp lại, gần gũi)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này tập trung vào các nguyên tắc thiết kế cốt lõi như Căn chỉnh (Alignment), Tương phản (Contrast), Lặp lại (Repetition), và Gần gũi (Proximity – ACRP). Nắm vững chúng giúp tạo ra các biểu đồ, đồ thị và bảng điều khiển dữ liệu rõ ràng, dễ hiểu và hấp dẫn, từ đó nâng cao hiệu quả truyền đạt thông tin và đưa ra quyết định dựa trên dữ liệu.",
                "keywords": [
                  "Căn chỉnh",
                  "Tương phản",
                  "Lặp lại",
                  "Gần gũi",
                  "Thiết kế trực quan",
                  "Trực quan hóa dữ liệu",
                  "UI/UX",
                  "Phân cấp thông tin",
                  "Truyền thông thị giác",
                  "Nguyên tắc thiết kế"
                ],
                "learningResources": [
                  {
                    "title": "The CRAP Principles: A Guide to Visual Design for Data Scientists",
                    "url": "https://towardsdatascience.com/the-crap-principles-a-guide-to-visual-design-for-data-scientists-e2743ce2f13f",
                    "type": "Article"
                  },
                  {
                    "title": "Visual Design Principles: Alignment, Proximity, Repetition, Contrast",
                    "url": "https://uxdesign.cc/visual-design-principles-alignment-proximity-repetition-contrast-d4850c950c41",
                    "type": "Article"
                  },
                  {
                    "title": "Data Visualization Best Practices: CRAP Principles for Better Charts",
                    "url": "https://www.datacamp.com/blog/data-visualization-best-practices-crap-principles-for-better-charts",
                    "type": "Article"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  {
                    "title": "Tái thiết kế biểu đồ kém hiệu quả",
                    "description": "Chọn một biểu đồ dữ liệu bất kỳ được thiết kế kém (ví dụ từ báo cáo hoặc trang web), sau đó áp dụng các nguyên tắc ACRP để tái thiết kế nó, giải thích những cải thiện đã thực hiện và lý do."
                  },
                  {
                    "title": "Thiết kế bảng điều khiển đơn giản",
                    "description": "Sử dụng một công cụ như Tableau Public, Power BI hoặc thư viện Python/R để tạo một bảng điều khiển (dashboard) dữ liệu đơn giản. Đảm bảo tất cả các thành phần trực quan (biểu đồ, văn bản, tiêu đề) tuân thủ chặt chẽ các nguyên tắc căn chỉnh, tương phản, lặp lại và gần gũi."
                  }
                ],
                "relatedJobTitles": [
                  "Data Analyst",
                  "Data Scientist",
                  "Business Intelligence Developer",
                  "Data Visualization Specialist",
                  "UX Designer (Data Products)",
                  "Product Analyst"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Python (Matplotlib, Seaborn, Plotly, Altair)",
                  "R (ggplot2)",
                  "D3.js",
                  "Figma (cho thiết kế wireframe)"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "6-10 giờ",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Analyst",
                    "score": 10
                  },
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "Data Visualization Specialist",
                    "score": 10
                  }
                ],
                "skillRelations": [
                  {
                    "targetSkillId": "cognitive_psychology_ui_design",
                    "type": "complementary",
                    "description": "Hiểu tâm lý học nhận thức giúp áp dụng các nguyên tắc thiết kế trực quan hiệu quả hơn, phù hợp với cách não bộ xử lý thông tin."
                  },
                  {
                    "targetSkillId": "data_storytelling",
                    "type": "complementary",
                    "description": "Các nguyên tắc thiết kế trực quan là nền tảng để tạo ra các câu chuyện dữ liệu hấp dẫn và dễ hiểu."
                  },
                  {
                    "targetSkillId": "interactive_dashboard_design",
                    "type": "unlocks",
                    "description": "Nắm vững ACRP là điều kiện tiên quyết để thiết kế các bảng điều khiển tương tác hiệu quả và trực quan."
                  },
                  {
                    "targetSkillId": "advanced_chart_design",
                    "type": "unlocks",
                    "description": "Kiến thức về các nguyên tắc cơ bản là nền tảng để học và áp dụng các kỹ thuật thiết kế biểu đồ nâng cao."
                  }
                ]
              },
              {
                "id": "ds_color_theory_data_vis",
                "name": "Lý thuyết màu sắc và ứng dụng trong trực quan hóa dữ liệu (như bảng màu tuần tự, phân kỳ, phân loại, khả năng tiếp cận)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này trang bị hiểu biết sâu sắc về các nguyên tắc của lý thuyết màu sắc, bao gồm các loại bảng màu (tuần tự, phân kỳ, phân loại) và cách áp dụng chúng hiệu quả trong trực quan hóa dữ liệu. Nó tập trung vào việc lựa chọn màu sắc để truyền tải thông điệp rõ ràng, tăng cường khả năng nhận thức và đảm bảo tính khả dụng, đặc biệt đối với người dùng mắc chứng mù màu.",
                "keywords": [
                  "Lý thuyết màu sắc",
                  "Trực quan hóa dữ liệu",
                  "Bảng màu tuần tự",
                  "Bảng màu phân kỳ",
                  "Bảng màu phân loại",
                  "Khả năng tiếp cận màu sắc",
                  "Mù màu",
                  "Thẩm mỹ dữ liệu",
                  "Tâm lý học màu sắc",
                  "Thiết kế nhận thức"
                ],
                "learningResources": [
                  {
                    "name": "ColorBrewer 2.0 - Color Advice for Cartography",
                    "url": "https://colorbrewer2.org/",
                    "type": "Web Tool & Guide"
                  },
                  {
                    "name": "Data Visualization: A Practical Introduction (Chapter on Color)",
                    "url": "https://socviz.co/makeover.html#color",
                    "type": "Online Book Chapter"
                  },
                  {
                    "name": "Understanding Color in Data Visualization (Datawrapper Blog)",
                    "url": "https://blog.datawrapper.de/understanding-color-in-data-visualization/",
                    "type": "Blog Post/Guide"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích và cải thiện bảng màu: Chọn một biểu đồ bất kỳ từ một nguồn công khai (ví dụ: báo cáo tin tức, trang web chính phủ) có cách sử dụng màu sắc kém hiệu quả hoặc không rõ ràng. Thiết kế lại biểu đồ đó với các bảng màu được cải thiện (tuần tự, phân kỳ, hoặc phân loại) để truyền tải thông điệp tốt hơn và tăng cường khả năng tiếp cận, giải thích lý do cho các lựa chọn của bạn.",
                  "Xây dựng bảng màu tùy chỉnh và đánh giá khả năng tiếp cận: Sử dụng một tập dữ liệu nhỏ của riêng bạn, tạo ra ba loại biểu đồ khác nhau (ví dụ: biểu đồ cột, bản đồ nhiệt, biểu đồ phân tán). Phát triển các bảng màu tùy chỉnh cho mỗi biểu đồ bằng cách sử dụng công cụ như ColorBrewer hoặc Adobe Color, sau đó sử dụng một công cụ mô phỏng mù màu (ví dụ: Coblis) để đánh giá và điều chỉnh chúng nhằm đảm bảo tính khả dụng."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Developer",
                  "Data Visualization Specialist",
                  "UX/UI Designer (Data Focus)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Python (Matplotlib, Seaborn)",
                  "R (ggplot2)",
                  "D3.js",
                  "ColorBrewer",
                  "Adobe Color",
                  "Coolors.co",
                  "Viz Palette"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "6-10 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Data Visualization Specialist",
                    "score": 10
                  },
                  {
                    "path": "Product Manager (Data Products)",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "skillId": "ds_visual_design_principles",
                    "type": "complementary",
                    "description": "Kiến thức cốt lõi để áp dụng các nguyên tắc thiết kế thị giác hiệu quả trong trực quan hóa dữ liệu."
                  },
                  {
                    "skillId": "ds_cognitive_psychology_for_vis",
                    "type": "complementary",
                    "description": "Bổ trợ cho việc hiểu cách tâm lý nhận thức ảnh hưởng đến cách người xem giải thích màu sắc và thông tin."
                  },
                  {
                    "skillId": "ds_advanced_visualization_techniques",
                    "type": "unlocks",
                    "description": "Nền tảng vững chắc để thực hiện các kỹ thuật trực quan hóa dữ liệu phức tạp và hiệu quả hơn, đảm bảo tính thẩm mỹ và khả năng truyền đạt thông tin."
                  }
                ]
              },
              {
                "id": "gestalt_principles_visual_perception",
                "name": "Các nguyên tắc Gestalt trong nhận thức thị giác (như gần gũi, tương đồng, liên tục, khép kín)",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này đi sâu vào các nguyên tắc Gestalt, mô tả cách bộ não con người tự động tổ chức và giải thích thông tin thị giác thành các mẫu có ý nghĩa (ví dụ: gần gũi, tương đồng, liên tục, khép kín, hình nền). Trong Khoa học Dữ liệu, việc hiểu và áp dụng các nguyên tắc này là cực kỳ quan trọng để thiết kế các biểu đồ, dashboard và giao diện người dùng dữ liệu hiệu quả, giúp người xem dễ dàng hiểu được các insight phức tạp và giảm tải nhận thức.",
                "keywords": [
                  "Gestalt Principles",
                  "Nhận thức thị giác",
                  "Tâm lý học nhận thức",
                  "Trực quan hóa dữ liệu",
                  "Thiết kế UI/UX",
                  "Gần gũi (Proximity)",
                  "Tương đồng (Similarity)",
                  "Liên tục (Continuity)",
                  "Khép kín (Closure)",
                  "Tổ chức thị giác"
                ],
                "learningResources": [
                  {
                    "name": "Gestalt Principles of Perception",
                    "url": "https://www.nngroup.com/articles/gestalt-principles/"
                  },
                  {
                    "name": "The Gestalt Principles in UI Design",
                    "url": "https://www.interaction-design.org/literature/article/the-gestalt-principles-in-ui-design"
                  },
                  {
                    "name": "Gestalt Principles in Data Visualization",
                    "url": "https://towardsdatascience.com/gestalt-principles-in-data-visualization-1a357f848f72"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích một dashboard hoặc biểu đồ dữ liệu hiện có và đề xuất các cải tiến dựa trên ít nhất ba nguyên tắc Gestalt. Vẽ lại phiên bản cải tiến.",
                  "Chọn một tập dữ liệu công khai và tạo hai phiên bản trực quan hóa khác nhau (ví dụ: biểu đồ phân tán, biểu đồ thanh). Một phiên bản cố tình bỏ qua các nguyên tắc Gestalt và một phiên bản áp dụng chúng để minh họa sự khác biệt về khả năng đọc và hiểu."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Developer",
                  "Data Visualization Engineer",
                  "UX/UI Designer (Data Focus)",
                  "Product Manager (Data-driven Products)"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "D3.js",
                  "Matplotlib",
                  "Seaborn",
                  "Plotly",
                  "ggplot2 (R)",
                  "Figma",
                  "Adobe XD"
                ],
                "difficultyLevel": "Beginner",
                "estimatedTimeToComplete": "4-8 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "Data Visualization Specialist",
                    "score": 10
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  },
                  {
                    "path": "UX/UI Designer",
                    "score": 8
                  }
                ],
                "skillRelations": [
                  {
                    "targetId": "color_theory_data_viz",
                    "type": "complementary",
                    "description": "Kết hợp với lý thuyết màu sắc để tăng cường sự rõ ràng và phân biệt trong trực quan hóa dữ liệu."
                  },
                  {
                    "targetId": "information_hierarchy_principles",
                    "type": "complementary",
                    "description": "Các nguyên tắc Gestalt hỗ trợ việc thiết lập hệ thống phân cấp thông tin hiệu quả trong các thiết kế trực quan."
                  },
                  {
                    "targetId": "data_storytelling_foundations",
                    "type": "unlocks",
                    "description": "Hiểu Gestalt là nền tảng để xây dựng các câu chuyện dữ liệu thuyết phục thông qua trực quan hóa."
                  },
                  {
                    "targetId": "designing_effective_dashboards",
                    "type": "unlocks",
                    "description": "Áp dụng các nguyên tắc này để thiết kế các bảng điều khiển trực quan và dễ sử dụng."
                  }
                ]
              },
              {
                "id": "DS-AP-PAA-VP",
                "name": "Các thuộc tính tiền chú ý (preattentive attributes) và cơ chế xử lý thông tin thị giác của não bộ",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này khám phá các thuộc tính tiền chú ý (preattentive attributes) như màu sắc, hình dạng, kích thước, định hướng, và vị trí, cùng với cách não bộ xử lý thông tin thị giác một cách vô thức và nhanh chóng. Hiểu rõ các thuộc tính này là nền tảng để thiết kế các biểu đồ và bảng điều khiển trực quan hiệu quả, giúp người xem nhanh chóng nhận diện mẫu, ngoại lệ và mối quan hệ trong dữ liệu mà không cần tập trung cao độ, từ đó cải thiện đáng kể khả năng truyền đạt thông tin.",
                "keywords": [
                  "Preattentive Attributes",
                  "Visual Perception",
                  "Cognitive Psychology",
                  "Data Visualization",
                  "Information Processing",
                  "Gestalt Principles",
                  "Visual Encoding",
                  "Reaction Time",
                  "Cognitive Load",
                  "Dashboard Design"
                ],
                "learningResources": [
                  {
                    "name": "Data Visualization: Preattentive Attributes",
                    "url": "https://www.tableau.com/learn/articles/data-visualization-preattentive-attributes"
                  },
                  {
                    "name": "Preattentive Processing by Robert Kosara",
                    "url": "https://eagereyes.org/basics/preattentive-processing"
                  },
                  {
                    "name": "Cognitive Science in Data Visualization",
                    "url": "https://towardsdatascience.com/cognitive-science-in-data-visualization-a32df17c1626"
                  }
                ],
                "prerequisites": [
                  "DS-AP-INTRO-VISUALIZATION"
                ],
                "projectIdeas": [
                  "Thiết kế lại một bảng điều khiển dữ liệu hiện có để tối ưu hóa việc sử dụng các thuộc tính tiền chú ý, nhằm cải thiện tốc độ và độ chính xác khi người dùng trích xuất thông tin quan trọng (Ví dụ: Biểu đồ doanh số, KPI).",
                  "Thực hiện một thử nghiệm nhỏ (user study) với các phiên bản trực quan khác nhau của cùng một tập dữ liệu, một phiên bản sử dụng các thuộc tính tiền chú ý hiệu quả và một phiên bản không, để đo lường sự khác biệt về thời gian phản ứng và khả năng nhận diện mẫu của người dùng."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Developer",
                  "UX Designer (Data-focused)",
                  "Visualization Engineer"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "D3.js"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "8-12 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 9
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 9
                  },
                  {
                    "path": "UX/UI Designer (Data Products)",
                    "score": 8
                  },
                  {
                    "path": "Machine Learning Engineer",
                    "score": 7
                  }
                ],
                "skillRelations": [
                  {
                    "type": "prerequisite",
                    "id": "DS-AP-INTRO-VISUALIZATION"
                  },
                  {
                    "type": "complementary",
                    "id": "DS-AP-GESTALT-PRINCIPLES"
                  },
                  {
                    "type": "complementary",
                    "id": "DS-AP-VISUAL-ENCODING-CHANNELS"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS-AP-OPTIMIZE-VISUAL-PERFORMANCE"
                  },
                  {
                    "type": "unlocks",
                    "id": "DS-AP-DASHBOARD-DESIGN"
                  }
                ]
              },
              {
                "id": "DS_ADV_CognitiveBiases_DataViz",
                "name": "Các thiên kiến nhận thức (cognitive biases) và ảnh hưởng của chúng đến việc diễn giải dữ liệu, cách giảm thiểu trong trực quan hóa",
                "type": "knowledge",
                "children": [],
                "description": "Kiến thức này khám phá các lỗi hệ thống trong tư duy con người (cognitive biases) có thể bóp méo cách chúng ta thu thập, phân tích và diễn giải dữ liệu, đặc biệt trong bối cảnh trực quan hóa. Việc hiểu và nhận diện các thiên kiến này là rất quan trọng để tạo ra các biểu đồ, dashboard trung thực, khách quan, và giúp người dùng đưa ra quyết định chính xác dựa trên dữ liệu, đồng thời giảm thiểu rủi ro truyền đạt thông tin sai lệch.",
                "keywords": [
                  "Cognitive Biases",
                  "Data Interpretation",
                  "Data Visualization Ethics",
                  "Bias Mitigation",
                  "Perception Bias",
                  "Confirmation Bias",
                  "Anchoring Bias",
                  "Framing Effect",
                  "Visual Deception",
                  "Debiasing Techniques"
                ],
                "learningResources": [
                  {
                    "name": "Cognitive Biases in Data Visualization",
                    "url": "https://towardsdatascience.com/cognitive-biases-in-data-visualization-12e0380e210b"
                  },
                  {
                    "name": "How to Spot (and Avoid) Cognitive Biases in Data Analysis",
                    "url": "https://hbr.org/2017/07/how-to-spot-and-avoid-cognitive-biases-in-data-analysis"
                  },
                  {
                    "name": "The Importance of Understanding Cognitive Biases in Data Visualization",
                    "url": "https://www.tableau.com/blog/cognitive-biases-data-visualization"
                  }
                ],
                "prerequisites": [],
                "projectIdeas": [
                  "Phân tích một dashboard hoặc biểu đồ công khai (ví dụ: từ báo chí, trang tin tức) để xác định các thiên kiến nhận thức tiềm ẩn trong cách trình bày dữ liệu. Đề xuất và thiết kế lại một phiên bản cải tiến để giảm thiểu những thiên kiến đó.",
                  "Phát triển một bộ hướng dẫn (guideline) về thiết kế trực quan hóa dữ liệu 'chống thiên kiến' (bias-proof data visualization) cho một tổ chức cụ thể, bao gồm các ví dụ minh họa cho từng nguyên tắc và loại thiên kiến."
                ],
                "relatedJobTitles": [
                  "Data Scientist",
                  "Data Analyst",
                  "Business Intelligence Analyst",
                  "UX Researcher",
                  "Data Visualization Specialist",
                  "Machine Learning Engineer (responsible for model interpretability)",
                  "Product Manager"
                ],
                "marketDemand": "High",
                "tools": [
                  "Tableau",
                  "Power BI",
                  "Python (Matplotlib, Seaborn, Plotly)",
                  "R (ggplot2)",
                  "D3.js",
                  "Figma (cho thiết kế nguyên mẫu trực quan)",
                  "Human-Centered Design Principles (phương pháp luận)"
                ],
                "difficultyLevel": "Intermediate",
                "estimatedTimeToComplete": "15-20 hours",
                "importanceScore": 9,
                "requiredProficiency": "Practical",
                "careerPathRelevance": [
                  {
                    "path": "Data Scientist",
                    "score": 9
                  },
                  {
                    "path": "Data Analyst",
                    "score": 8
                  },
                  {
                    "path": "Business Intelligence Developer",
                    "score": 8
                  },
                  {
                    "path": "UX Designer/Researcher",
                    "score": 7
                  },
                  {
                    "path": "Data Visualization Specialist",
                    "score": 9
                  }
                ],
                "skillRelations": [
                  {
                    "id": "DS_DV_FundamentalVizPrinciples",
                    "type": "complementary",
                    "description": "Kiến thức về các nguyên tắc trực quan hóa dữ liệu cơ bản là cần thiết để có thể áp dụng các kỹ thuật giảm thiểu thiên kiến một cách hiệu quả trong thực tế."
                  },
                  {
                    "id": "DS_HUMAN_PerceptionAndCognition",
                    "type": "complementary",
                    "description": "Hiểu biết về tâm lý học nhận thức và cách con người xử lý thông tin thị giác là nền tảng để nắm bắt sâu sắc các loại thiên kiến và nguyên nhân của chúng."
                  },
                  {
                    "id": "DS_DV_EthicalDataPractices",
                    "type": "unlocks",
                    "description": "Nắm vững các thiên kiến nhận thức là yếu tố cốt lõi để phát triển và áp dụng các nguyên tắc đạo đức trong phân tích và trực quan hóa dữ liệu."
                  }
                ]
              }
            ],
            "skillName": "Áp dụng nguyên tắc thiết kế trực quan và tâm lý học nhận thức",
            "competency": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
            "description": "Kỹ năng này cho phép bạn thiết kế các biểu đồ và dashboard không chỉ đẹp mắt mà còn hiệu quả trong việc truyền tải thông điệp. Bạn sẽ biết cách tận dụng các nguyên tắc thiết kế thị giác và hiểu biết về tâm lý học nhận thức để tạo ra các trực quan hóa dữ liệu dễ hiểu, dễ tiếp cận và tránh gây ra các thiên kiến hiểu sai thông tin cho người xem.",
            "learningResources": [
              {
                "title": "Storytelling with Data: A Data Visualization Guide for Business Professionals",
                "author": "Cole Nussbaumer Knaflic",
                "type": "Sách",
                "url": "https://www.storytellingwithdata.com/book"
              },
              {
                "title": "The Gestalt Principles of Perception",
                "author": "Interaction Design Foundation",
                "type": "Khóa học/Bài viết",
                "url": "https://www.interaction-design.org/literature/article/the-gestalt-principles-of-visual-perception"
              }
            ],
            "projectIdeas": [
              {
                "title": "Thiết kế Dashboard tối ưu hóa trải nghiệm người dùng cho dữ liệu bán hàng",
                "description": "Sử dụng một bộ dữ liệu bán hàng bất kỳ (ví dụ: doanh số theo khu vực, thời gian, sản phẩm), hãy thiết kế một dashboard tương tác. Áp dụng các nguyên tắc thiết kế trực quan (căn chỉnh, tương phản), lý thuyết màu sắc (chọn bảng màu phù hợp), nguyên tắc Gestalt để nhóm và sắp xếp thông tin, và sử dụng các thuộc tính tiền chú ý để làm nổi bật các điểm dữ liệu quan trọng. Đồng thời, trình bày các lựa chọn thiết kế của bạn để giảm thiểu các thiên kiến nhận thức có thể phát sinh khi người dùng diễn giải dữ liệu."
              }
            ],
            "tools": [
              "Tableau",
              "Power BI",
              "Python (Matplotlib, Seaborn, Plotly)",
              "R (ggplot2)",
              "D3.js",
              "Figma (để lên ý tưởng thiết kế)"
            ],
            "difficultyLevel": "Trung bình",
            "estimatedTimeToComplete": "20-30 giờ",
            "importanceScore": 8.5
          }
        ],
        "abilityName": "Trực quan hóa và Truyền đạt Dữ liệu (Data Visualization & Communication)",
        "specialization": "Khoa học dữ liệu (Data Science) và Dữ liệu lớn (Big Data)",
        "description": "Năng lực này tập trung vào việc biến dữ liệu phức tạp thành thông tin dễ hiểu và có giá trị thông qua hình ảnh trực quan và câu chuyện lôi cuốn. Nó không chỉ đòi hỏi kỹ năng kỹ thuật trong việc sử dụng công cụ mà còn cần khả năng tư duy chiến lược để truyền đạt thông điệp chính xác và thuyết phục. Việc thành thạo năng lực này giúp các nhà khoa học dữ liệu trình bày kết quả phân tích một cách hiệu quả, hỗ trợ quá trình ra quyết định dựa trên dữ liệu.",
        "learningResources": [
          {
            "type": "Sách",
            "title": "Storytelling with Data: A Data Visualization Guide for Business Professionals",
            "author": "Cole Nussbaumer Knaflic"
          },
          {
            "type": "Khóa học trực tuyến",
            "title": "Data Visualization and Communication with Tableau/Power BI Specialization",
            "platform": "Coursera/edX (ví dụ)"
          }
        ],
        "projectIdeas": [
          {
            "title": "Phân tích và Trực quan hóa Dữ liệu toàn diện cho Doanh nghiệp",
            "description": "Phát triển một bộ Dashboard tương tác và Báo cáo thông tin chi tiết (insights report) dựa trên dữ liệu thực tế (ví dụ: dữ liệu bán hàng, vận hành, khách hàng) của một doanh nghiệp. Dự án yêu cầu kể một câu chuyện dữ liệu mạch lạc, trình bày các phát hiện quan trọng, xu hướng và đề xuất hành động cụ thể cho các cấp quản lý, áp dụng các nguyên tắc thiết kế trực quan hiệu quả và kỹ thuật kể chuyện bằng dữ liệu."
          }
        ],
        "tools": [
          "Tableau",
          "Microsoft Power BI",
          "Python (Matplotlib, Seaborn, Plotly, Altair)",
          "R (ggplot2)",
          "D3.js (cho web)",
          "Google Data Studio"
        ],
        "difficultyLevel": "Trung bình đến Nâng cao",
        "estimatedTimeToComplete": "3-6 tháng (để đạt trình độ thành thạo cơ bản)",
        "importanceScore": 9
      }
    ]
  }
]